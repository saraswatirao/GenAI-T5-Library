{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Developing a toxicity detector using large language models (LLMs)\n",
        "\n",
        "Solution to the [assignment](https://docs.google.com/document/d/1w307HmtXqqreDj5VMiUtdjD0YG2pvzBbiPqwMf6vJ9E/edit) for Generative AI 1.\n",
        "\n",
        "The fine-tuned model checkpoints produced here have also been  shared on Google Drive."
      ],
      "metadata": {
        "id": "A90SKledYRNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Dataset Selection & Exploratory Data Analysis\n",
        "\n",
        "Here we will load the [Toxic Comments Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) mentioned in the assignment. We have manually loaded the files from this dataset into Google Drive and will access them directly from there."
      ],
      "metadata": {
        "id": "WyXTLcqtU4Fd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjJm3q3aKRVB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive # Used for mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HFnZHcAMHtn",
        "outputId": "273adb2a-95fe-4a45-8ed6-4f6d5f155f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the test data provided in the dataset\n",
        "df_train_file = pd.read_csv(\"/content/gdrive/MyDrive/T5/train.csv\")"
      ],
      "metadata": {
        "id": "d3tpdL0KMdgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few rows of this dataset\n",
        "df_train_file.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_LZKxQF_NJ7p",
        "outputId": "825f8937-0ce8-4126-9dc7-a85aad816e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0             0        0       0       0              0  \n",
              "1             0        0       0       0              0  \n",
              "2             0        0       0       0              0  \n",
              "3             0        0       0       0              0  \n",
              "4             0        0       0       0              0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af500d6f-654c-4a58-8c8d-0aaa8f23bf2b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af500d6f-654c-4a58-8c8d-0aaa8f23bf2b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-af500d6f-654c-4a58-8c8d-0aaa8f23bf2b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-af500d6f-654c-4a58-8c8d-0aaa8f23bf2b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1a1bb1db-9ea2-4ea1-9cef-d4ae8e9b9534\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1a1bb1db-9ea2-4ea1-9cef-d4ae8e9b9534')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1a1bb1db-9ea2-4ea1-9cef-d4ae8e9b9534 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that there appear to be multiple classes in this dataset beyond just \"toxic\". Look at some random rows labeled as \"toxic\" to see if individual comments can be assigned to more than one of these classes."
      ],
      "metadata": {
        "id": "MJlWGx51i3-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print labels for a random sample labeled as toxic\n",
        "# Note that some samples receive multiple labels\n",
        "cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "df_train_file[df_train_file[\"toxic\"] == 1][cols].sample(10)"
      ],
      "metadata": {
        "id": "jymkbFrujULE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "53cee70f-1724-4811-a558-6f89cae091be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
              "35382       1             0        0       0       0              0\n",
              "33450       1             0        0       0       0              0\n",
              "119365      1             0        1       0       1              0\n",
              "137123      1             0        1       0       0              0\n",
              "81181       1             0        0       0       1              0\n",
              "61852       1             0        0       0       0              0\n",
              "146832      1             0        1       0       0              0\n",
              "88928       1             0        1       0       0              0\n",
              "105425      1             0        0       0       1              0\n",
              "33839       1             0        1       0       1              0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-388bd1bc-feb3-4fbc-87a5-a3acc42eab20\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>35382</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33450</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119365</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137123</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81181</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61852</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146832</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88928</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105425</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33839</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-388bd1bc-feb3-4fbc-87a5-a3acc42eab20')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-388bd1bc-feb3-4fbc-87a5-a3acc42eab20 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-388bd1bc-feb3-4fbc-87a5-a3acc42eab20');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-205c253e-c5a3-4f58-827c-39b5c8f66346\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-205c253e-c5a3-4f58-827c-39b5c8f66346')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-205c253e-c5a3-4f58-827c-39b5c8f66346 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, it looks like this is a multi-label classification problem. That is, multiple nonexclusive labels (or none at all) may be assigned to each instance. When building our classifier, we will need to use a model capable of producing multiple labels for each input."
      ],
      "metadata": {
        "id": "h9bAJ2klkd3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's also take a look at the test data provided with this data set:"
      ],
      "metadata": {
        "id": "V-RPdqyJISi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the test data provided in the dataset\n",
        "df_test_file = pd.read_csv(\"/content/gdrive/MyDrive/T5/test.csv\")"
      ],
      "metadata": {
        "id": "AUQ05I3XjnUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first few rows of the test data\n",
        "df_test_file.head()"
      ],
      "metadata": {
        "id": "_T-BMQrdjpoA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8675f33b-8b92-405e-aaa9-351699f2f437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id                                       comment_text\n",
              "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
              "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
              "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
              "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
              "4  00017695ad8997eb          I don't anonymously edit articles at all."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2ca3c1f-a42b-43fb-8754-9ddca6df58b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>:If you have a look back at the source, the in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>I don't anonymously edit articles at all.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2ca3c1f-a42b-43fb-8754-9ddca6df58b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2ca3c1f-a42b-43fb-8754-9ddca6df58b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2ca3c1f-a42b-43fb-8754-9ddca6df58b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-54a6fe94-0579-4959-befd-cd7aaefd01ba\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-54a6fe94-0579-4959-befd-cd7aaefd01ba')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-54a6fe94-0579-4959-befd-cd7aaefd01ba button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like this test data does not contain labels. This makes sense, since this is likely the test data used for scoring the Kaggle competition associated with this dataset. Since we will want to evaluate the performance of our model out-of-sample, we will need to construct our own test set that contains labels. To do this, we we will hold out a random 20% of the training set to test on, and use the remaining 80% to train our model."
      ],
      "metadata": {
        "id": "MNRp5ViXj0OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use train_test_split to create our labeled training and test sets\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "7jQ3Ajw0ne4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and test sets from the labeled data that we have available\n",
        "df_train, df_test = train_test_split(df_train_file, test_size=0.2)"
      ],
      "metadata": {
        "id": "LLDKCdmSnlAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our new training and test sets to Google Drive\n",
        "df_train.to_csv(\"/content/gdrive/MyDrive/T5/assignment_train.csv\", index=False)\n",
        "df_test.to_csv(\"/content/gdrive/MyDrive/T5/assignment_test.csv\", index=False)"
      ],
      "metadata": {
        "id": "0ld-dNvBnxH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Baseline Model Testing\n",
        "\n",
        "While there are a variety of approaches that could be used to solve this problem, we will use the [T5 model](https://huggingface.co/docs/transformers/model_doc/t5) that was discussed in class. T5 is a text-to-text model that has been fine-tuned on a variety of tasks, including sentiment analysis, summarization, and question answering. When providing text input to the model, the specific task to be performed can be indicated in a prefix in the text input. The full list of tasks the T5 model has already been fine-tuned on and examples of the prefixes used for those tasks are in Appendix D of [the T5 paper](https://arxiv.org/pdf/1910.10683.pdf).\n",
        "\n",
        "We will load the T5 model checkpoint from Hugging Face and test its ability to perform sentiment analysis (as an example) out-of-the box. Although T5 is capable of performing a variety of tasks without additional fine-tuning, it has not been fine-tuned to perform the specific task of toxic comment classification that we want to perform. We will verify that the model produces meaningless output when prompted to classify toxic content.\n",
        "\n",
        "To get started, we will need to install the Hugging Face Transformers library and one of its dependencies."
      ],
      "metadata": {
        "id": "BkpgKsIzN67z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4umyzw2SNNHD",
        "outputId": "639331bf-02d0-43bb-af51-5cc75a68dcfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports that we will use for implementing the T5 model\n",
        "import torch\n",
        "from transformers import (\n",
        "    T5Tokenizer,\n",
        "    T5ForConditionalGeneration,\n",
        "    DataCollatorForSeq2Seq\n",
        ")"
      ],
      "metadata": {
        "id": "YUIlMrZkN8Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supress warnings. None are important in this case, and they clutter output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "sVv7wSUTxdhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the T5-small model instance, which has ~60M parameters. This will allow us to fine tune a model faster and within the limits of free Colab instances. As we will see, even the small model produces a fairly capable classifier."
      ],
      "metadata": {
        "id": "asnzUkfTUkEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained T5 small model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Send the model to the GPU for faster training and inference\n",
        "model = model.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UepLPkvC8W-Z",
        "outputId": "71f10d7d-7c6f-4adb-df1e-c75f14ea4766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a feel for how the T5 model works, we will test it on a sentiment classification example. Sentiment classification is one of the tasks that the checkpoint available from Hugging Face has already been fine-tuned for. As discussed in the T5 paper, the model has been fine-tuned so that the NLP task that we want to perform can be indicated by a prefix applied to the model's input. For example, to perform sentiment classification, the prefix `sst2 sentence:` can be used:"
      ],
      "metadata": {
        "id": "cTQuBU7Rtq-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at a sample that should have positive sentiment\n",
        "input_ids = tokenizer(\n",
        "    \"sst2 sentence: The movie was interesting.\",\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids\n",
        "output = model.generate(input_ids=input_ids.to(\"cuda\"))\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SFc_iYOvVBW",
        "outputId": "c49210d4-97e9-4cfa-8cc7-80835d9d59cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> positive</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at a variation that should now have negative sentiment\n",
        "input_ids = tokenizer(\n",
        "    \"sst2 sentence: The movie was too long to be interesting.\",\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids\n",
        "output = model.generate(input_ids=input_ids.to(\"cuda\"))\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "l-EU8ETrm0a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba78d13-0d19-49ae-8aa6-f0ef6e11a2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> negative</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the model generates text strings that indicate the appropriate class as its output.\n",
        "\n",
        "We would like to use this model for toxic content classification, although the model has not yet been fine-tuned for this task. Just to see what happens, we will pass an input to the model with a reasonable prefix for toxic content classification. Since the model is unfamiliar with this task, we get a meaningless output as expected:"
      ],
      "metadata": {
        "id": "HKcEkhttttzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a random input to test for toxic comment classification\n",
        "sample = df_train[\"comment_text\"][4]\n",
        "print(sample)"
      ],
      "metadata": {
        "id": "i3RFdGuKm035",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f547494-e91c-4fc7-8797-47b0dac947c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You, sir, are my hero. Any chance you remember what page that's on?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass input to the model. The specific input we pass in this example is\n",
        "# \"is this a toxic comment: You, sir, are my hero. Any chance you remember what page that's on?\"\n",
        "# Note that the output string that is produced is meaningless\n",
        "input_ids = tokenizer(\n",
        "    \"is this a toxic comment: \"+sample,\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids\n",
        "output = model.generate(input_ids=input_ids.to(\"cuda\"))\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZPPMNDVvh1O",
        "outputId": "89bc09ed-306c-4464-b5b4-c1f330609989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> <extra_id_0> comment?</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Fine-Tuning on Training Data\n",
        "\n",
        "Now we will fine-tune the T5 model to perform toxic comment classification. The main design decision we need to make is how we want to format the labels that our model will predict as a text sequence that T5 can output. Since this is a muti-label classification problem, we will produce a text string containing the names of all predicted labels. For example, if the comment is flagged as toxic, obscene, and an insult, our model will output the string `\"toxic obscene insult\"`. If it is only flagged as toxic, we will only output `\"toxic\"`. If the comment is not flagged with any of the labels, we will output the text string `\"not_toxic\"`.\n",
        "\n",
        "In addition to designing our labels, we will also select a prefix for this task to keep a consistent style with the pre-trained T5 model. For our task, we will use the prefix `toxic comment classification:`."
      ],
      "metadata": {
        "id": "Fx3SE3DetppT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine-tune our model we will first define a PyTorch Dataset class that will be used to fetch and format comments and labels from our training set. This dataset will be used to generate prefixed inputs to the model and multi-label outputs as described above. We will then create a DataLoader that can be used to fetch and iterate over batches of training samples. We will then write a simple training loop that will iterate over batches in a single epoch to produce our fine-tuned model."
      ],
      "metadata": {
        "id": "YMXGE96XLZom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToxicityDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    This class is used to manage datasets for toxic content\n",
        "    classification. It is a standard PyTorch Dataset.\n",
        "    The items returned by this dataset are dictionaries with\n",
        "    two keys:\n",
        "    - input_ids: The token ids of the tokens in the input string\n",
        "    - labels: The token ids of the tokens in the label string\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, tokenizer):\n",
        "        \"\"\"\n",
        "        When initializing the dataset, load the provided CSV\n",
        "        file as a dataframe and save the provided tokenizer.\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        This method will return the number of samples in the dataset\n",
        "        \"\"\"\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        This method will return a single item from the dataset. The index\n",
        "        of the desired item is provided as input.\n",
        "        \"\"\"\n",
        "\n",
        "        # List all of the possible toxic labels\n",
        "        categories = [\n",
        "            \"toxic\",\n",
        "            \"severe_toxic\",\n",
        "            \"obscene\",\n",
        "            \"threat\",\n",
        "            \"insult\",\n",
        "            \"identity_hate\"\n",
        "        ]\n",
        "\n",
        "        # Select the sample with the provided index from the dataframe\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # We will build our model inputs and outputs here\n",
        "        # Inputs will be a tensor containing token indices of the input string\n",
        "        # Outputs will be a tensor containing token indices of the label string\n",
        "        labels = []\n",
        "        toxic = False\n",
        "        # Loop over categories\n",
        "        for cat in categories:\n",
        "            # If the sample is flagged as the current toxic category\n",
        "            # append the label to a list of labels\n",
        "            if row[cat] == 1:\n",
        "                toxic = True\n",
        "                labels.append(cat)\n",
        "        # If the sample is not flagged as any toxic category, append\n",
        "        # the value \"not_toxic\" to the list of labels\n",
        "        if toxic is False:\n",
        "            labels.append(\"not_toxic\")\n",
        "\n",
        "        # Tokenize the input string\n",
        "        input_ids = self.tokenizer.encode(\n",
        "            \"toxic comment classification: \" + row[\"comment_text\"],\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        # Tokenize the label output string\n",
        "        labels = self.tokenizer.encode(\n",
        "            \" \".join(labels),\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Return a dictionary containing the inputs and labels\n",
        "        return {\"input_ids\": input_ids[0], \"labels\": labels[0]}\n"
      ],
      "metadata": {
        "id": "2B4FFc6YOPa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset object from our training set\n",
        "dataset = ToxicityDataset(\"/content/gdrive/MyDrive/T5/assignment_train.csv\", tokenizer)"
      ],
      "metadata": {
        "id": "raI1R59fU1aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataLoader for iterating over batches of training samples\n",
        "\n",
        "# Create a DataCollator to use with our DataLoader\n",
        "# This will allow us to create batches of more than one training sample, and\n",
        "# will appropriately mask and pad inputs in a batch so they have the same lengths\n",
        "collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "# Here we will use a batch size of 1 because using anything larger\n",
        "# can lead to out-of-memory issues on a Colab T4 instance\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=collator\n",
        ")"
      ],
      "metadata": {
        "id": "WgT1Vaw28sJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the optimizer that we will use for fine-tuning\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "TnjJEVG7_h2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here we implement the training loop for fine-tuning our model.\n",
        "We will fine tune over a single epoch of our training set using\n",
        "batches composed of a single training sample\n",
        "\"\"\"\n",
        "\n",
        "# Get the number of batches, which we will print in status messages\n",
        "n_batches = dataset.__len__()\n",
        "\n",
        "# Loop over a single epoch to fine-tune the model\n",
        "current_batch = 0\n",
        "total_loss = 0\n",
        "for batch in training_loader:\n",
        "\n",
        "    # Move the current batch to the GPU\n",
        "    batch = batch.to(\"cuda\")\n",
        "    # Evaluate the loss on the current batch and perform\n",
        "    # a single step of parameter optimization.\n",
        "    output = model(**batch)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    output.loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Keep a running total of the per-batch loss.\n",
        "    # We will use this for reporting the mean training loss.\n",
        "    total_loss += output.loss.item()\n",
        "    current_batch += 1\n",
        "    # Every 100 batches, print a status update.\n",
        "    if current_batch%100 == 0:\n",
        "        status = \"Mean loss: {}. Batch {} of {}\".format(\n",
        "            total_loss/100,\n",
        "            current_batch,\n",
        "            n_batches\n",
        "        )\n",
        "        print(status)\n",
        "        total_loss = 0\n",
        "\n",
        "    # Every 5000 batches, save a model checkpoint in Google Drive\n",
        "    # Colab can randomly disconnect during long training jobs and\n",
        "    # this will ensure that we don't lose all of our work.\n",
        "    if current_batch%5000 == 0:\n",
        "        model.save_pretrained(\"/content/gdrive/MyDrive/T5/t5_checkpoint\")\n",
        "        print(\"Checkpointing model...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y41m1W438tTu",
        "outputId": "3f97b86d-83be-4b8e-f409-c55b99801b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean loss: 0.09523613936835318. Batch 100 of 127656\n",
            "Mean loss: 0.05655968463797763. Batch 200 of 127656\n",
            "Mean loss: 0.1365324474645604. Batch 300 of 127656\n",
            "Mean loss: 0.07409224301867652. Batch 400 of 127656\n",
            "Mean loss: 0.08099154121460743. Batch 500 of 127656\n",
            "Mean loss: 0.09570899739544984. Batch 600 of 127656\n",
            "Mean loss: 0.10185960668939514. Batch 700 of 127656\n",
            "Mean loss: 0.06738146965359192. Batch 800 of 127656\n",
            "Mean loss: 0.10000457228350569. Batch 900 of 127656\n",
            "Mean loss: 0.08010242569664115. Batch 1000 of 127656\n",
            "Mean loss: 0.10016160817074705. Batch 1100 of 127656\n",
            "Mean loss: 0.05749054300909848. Batch 1200 of 127656\n",
            "Mean loss: 0.10140042294187879. Batch 1300 of 127656\n",
            "Mean loss: 0.13222045421360235. Batch 1400 of 127656\n",
            "Mean loss: 0.0369298497307318. Batch 1500 of 127656\n",
            "Mean loss: 0.04605511341291276. Batch 1600 of 127656\n",
            "Mean loss: 0.04686834342141083. Batch 1700 of 127656\n",
            "Mean loss: 0.09498333913870738. Batch 1800 of 127656\n",
            "Mean loss: 0.07761606088166445. Batch 1900 of 127656\n",
            "Mean loss: 0.1046345019238288. Batch 2000 of 127656\n",
            "Mean loss: 0.08301453626252624. Batch 2100 of 127656\n",
            "Mean loss: 0.0689809635313577. Batch 2200 of 127656\n",
            "Mean loss: 0.0826598226209353. Batch 2300 of 127656\n",
            "Mean loss: 0.050342987383828586. Batch 2400 of 127656\n",
            "Mean loss: 0.05194540002245049. Batch 2500 of 127656\n",
            "Mean loss: 0.07837525541882315. Batch 2600 of 127656\n",
            "Mean loss: 0.09071040052713215. Batch 2700 of 127656\n",
            "Mean loss: 0.08867353040724993. Batch 2800 of 127656\n",
            "Mean loss: 0.07580018993281555. Batch 2900 of 127656\n",
            "Mean loss: 0.08471895071380459. Batch 3000 of 127656\n",
            "Mean loss: 0.06572149968431404. Batch 3100 of 127656\n",
            "Mean loss: 0.030506941523608475. Batch 3200 of 127656\n",
            "Mean loss: 0.11239269558258912. Batch 3300 of 127656\n",
            "Mean loss: 0.03205335132604887. Batch 3400 of 127656\n",
            "Mean loss: 0.08018114321261237. Batch 3500 of 127656\n",
            "Mean loss: 0.06978745203065956. Batch 3600 of 127656\n",
            "Mean loss: 0.12135335925407617. Batch 3700 of 127656\n",
            "Mean loss: 0.0822458204663053. Batch 3800 of 127656\n",
            "Mean loss: 0.0587820515628664. Batch 3900 of 127656\n",
            "Mean loss: 0.05314913705482468. Batch 4000 of 127656\n",
            "Mean loss: 0.12803814381704798. Batch 4100 of 127656\n",
            "Mean loss: 0.08520334711953183. Batch 4200 of 127656\n",
            "Mean loss: 0.11045033274946035. Batch 4300 of 127656\n",
            "Mean loss: 0.04357418044091901. Batch 4400 of 127656\n",
            "Mean loss: 0.06685030591303075. Batch 4500 of 127656\n",
            "Mean loss: 0.06117809660445346. Batch 4600 of 127656\n",
            "Mean loss: 0.03483584083711321. Batch 4700 of 127656\n",
            "Mean loss: 0.10212535554233909. Batch 4800 of 127656\n",
            "Mean loss: 0.08100539253784518. Batch 4900 of 127656\n",
            "Mean loss: 0.05862679799289253. Batch 5000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.049286421320757653. Batch 5100 of 127656\n",
            "Mean loss: 0.05177019766171725. Batch 5200 of 127656\n",
            "Mean loss: 0.10469536714408605. Batch 5300 of 127656\n",
            "Mean loss: 0.09178327416433603. Batch 5400 of 127656\n",
            "Mean loss: 0.06685348906432409. Batch 5500 of 127656\n",
            "Mean loss: 0.07662303861121472. Batch 5600 of 127656\n",
            "Mean loss: 0.06802194084983057. Batch 5700 of 127656\n",
            "Mean loss: 0.15122645972676765. Batch 5800 of 127656\n",
            "Mean loss: 0.036714912693732914. Batch 5900 of 127656\n",
            "Mean loss: 0.06497769943101957. Batch 6000 of 127656\n",
            "Mean loss: 0.08920744230283162. Batch 6100 of 127656\n",
            "Mean loss: 0.0695404933638929. Batch 6200 of 127656\n",
            "Mean loss: 0.039940355894341334. Batch 6300 of 127656\n",
            "Mean loss: 0.09279054515063763. Batch 6400 of 127656\n",
            "Mean loss: 0.09207735851513461. Batch 6500 of 127656\n",
            "Mean loss: 0.11460055822739378. Batch 6600 of 127656\n",
            "Mean loss: 0.17525971325972933. Batch 6700 of 127656\n",
            "Mean loss: 0.06989325296788593. Batch 6800 of 127656\n",
            "Mean loss: 0.06898105556538212. Batch 6900 of 127656\n",
            "Mean loss: 0.09295763132082356. Batch 7000 of 127656\n",
            "Mean loss: 0.06878367615317985. Batch 7100 of 127656\n",
            "Mean loss: 0.14555254102939216. Batch 7200 of 127656\n",
            "Mean loss: 0.050198619628208686. Batch 7300 of 127656\n",
            "Mean loss: 0.07041095993452473. Batch 7400 of 127656\n",
            "Mean loss: 0.08189316604246415. Batch 7500 of 127656\n",
            "Mean loss: 0.04981013093733054. Batch 7600 of 127656\n",
            "Mean loss: 0.0830118312808554. Batch 7700 of 127656\n",
            "Mean loss: 0.09175194249663036. Batch 7800 of 127656\n",
            "Mean loss: 0.11070826407501955. Batch 7900 of 127656\n",
            "Mean loss: 0.0724870545869635. Batch 8000 of 127656\n",
            "Mean loss: 0.09159934565148432. Batch 8100 of 127656\n",
            "Mean loss: 0.07192376978566245. Batch 8200 of 127656\n",
            "Mean loss: 0.10423077353293593. Batch 8300 of 127656\n",
            "Mean loss: 0.12478563199667406. Batch 8400 of 127656\n",
            "Mean loss: 0.06196112813726359. Batch 8500 of 127656\n",
            "Mean loss: 0.07490634314404815. Batch 8600 of 127656\n",
            "Mean loss: 0.07851441153914493. Batch 8700 of 127656\n",
            "Mean loss: 0.03234227193810057. Batch 8800 of 127656\n",
            "Mean loss: 0.06699073259638226. Batch 8900 of 127656\n",
            "Mean loss: 0.10506406255688489. Batch 9000 of 127656\n",
            "Mean loss: 0.13607049403010024. Batch 9100 of 127656\n",
            "Mean loss: 0.09469729974487563. Batch 9200 of 127656\n",
            "Mean loss: 0.07136307916724036. Batch 9300 of 127656\n",
            "Mean loss: 0.08441805361431762. Batch 9400 of 127656\n",
            "Mean loss: 0.11015713535094619. Batch 9500 of 127656\n",
            "Mean loss: 0.09342792900075438. Batch 9600 of 127656\n",
            "Mean loss: 0.12455751183108077. Batch 9700 of 127656\n",
            "Mean loss: 0.0808144267503667. Batch 9800 of 127656\n",
            "Mean loss: 0.06667879507687757. Batch 9900 of 127656\n",
            "Mean loss: 0.09133049599578953. Batch 10000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.08758352393037057. Batch 10100 of 127656\n",
            "Mean loss: 0.1513201937069607. Batch 10200 of 127656\n",
            "Mean loss: 0.11817877063222113. Batch 10300 of 127656\n",
            "Mean loss: 0.05270677634052845. Batch 10400 of 127656\n",
            "Mean loss: 0.11793054945257608. Batch 10500 of 127656\n",
            "Mean loss: 0.11981734368164325. Batch 10600 of 127656\n",
            "Mean loss: 0.06585867315494397. Batch 10700 of 127656\n",
            "Mean loss: 0.05918192144106797. Batch 10800 of 127656\n",
            "Mean loss: 0.10715776636527152. Batch 10900 of 127656\n",
            "Mean loss: 0.0922466081327002. Batch 11000 of 127656\n",
            "Mean loss: 0.06657854740933544. Batch 11100 of 127656\n",
            "Mean loss: 0.11725477948486514. Batch 11200 of 127656\n",
            "Mean loss: 0.05177413423372854. Batch 11300 of 127656\n",
            "Mean loss: 0.11332545646675499. Batch 11400 of 127656\n",
            "Mean loss: 0.07330512474811257. Batch 11500 of 127656\n",
            "Mean loss: 0.09354080592524042. Batch 11600 of 127656\n",
            "Mean loss: 0.09118057675514138. Batch 11700 of 127656\n",
            "Mean loss: 0.09525703524544951. Batch 11800 of 127656\n",
            "Mean loss: 0.0813138731215804. Batch 11900 of 127656\n",
            "Mean loss: 0.12391139362047397. Batch 12000 of 127656\n",
            "Mean loss: 0.07800265067155124. Batch 12100 of 127656\n",
            "Mean loss: 0.08473756442486774. Batch 12200 of 127656\n",
            "Mean loss: 0.09275308905374914. Batch 12300 of 127656\n",
            "Mean loss: 0.07251925910144565. Batch 12400 of 127656\n",
            "Mean loss: 0.08363103531775778. Batch 12500 of 127656\n",
            "Mean loss: 0.08348974337535765. Batch 12600 of 127656\n",
            "Mean loss: 0.05682319105679198. Batch 12700 of 127656\n",
            "Mean loss: 0.06429622112282232. Batch 12800 of 127656\n",
            "Mean loss: 0.05869558776590566. Batch 12900 of 127656\n",
            "Mean loss: 0.07887449547313735. Batch 13000 of 127656\n",
            "Mean loss: 0.0695091833717015. Batch 13100 of 127656\n",
            "Mean loss: 0.10242490820005515. Batch 13200 of 127656\n",
            "Mean loss: 0.1339391877284652. Batch 13300 of 127656\n",
            "Mean loss: 0.05888813140438288. Batch 13400 of 127656\n",
            "Mean loss: 0.042319388721698485. Batch 13500 of 127656\n",
            "Mean loss: 0.08329335555983562. Batch 13600 of 127656\n",
            "Mean loss: 0.07377663825580384. Batch 13700 of 127656\n",
            "Mean loss: 0.10207546337778695. Batch 13800 of 127656\n",
            "Mean loss: 0.11893799503104674. Batch 13900 of 127656\n",
            "Mean loss: 0.11735886333161033. Batch 14000 of 127656\n",
            "Mean loss: 0.0509712868943825. Batch 14100 of 127656\n",
            "Mean loss: 0.08338160623197836. Batch 14200 of 127656\n",
            "Mean loss: 0.034553359653891676. Batch 14300 of 127656\n",
            "Mean loss: 0.08331222313940997. Batch 14400 of 127656\n",
            "Mean loss: 0.08316705572293358. Batch 14500 of 127656\n",
            "Mean loss: 0.052308472373451875. Batch 14600 of 127656\n",
            "Mean loss: 0.10115289361976465. Batch 14700 of 127656\n",
            "Mean loss: 0.05345040569025514. Batch 14800 of 127656\n",
            "Mean loss: 0.08851354075251948. Batch 14900 of 127656\n",
            "Mean loss: 0.05746737823185868. Batch 15000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.12037183579685916. Batch 15100 of 127656\n",
            "Mean loss: 0.07985044324716001. Batch 15200 of 127656\n",
            "Mean loss: 0.06265575767551126. Batch 15300 of 127656\n",
            "Mean loss: 0.05012267271878045. Batch 15400 of 127656\n",
            "Mean loss: 0.05506822729950045. Batch 15500 of 127656\n",
            "Mean loss: 0.04800130655376052. Batch 15600 of 127656\n",
            "Mean loss: 0.1403206816036436. Batch 15700 of 127656\n",
            "Mean loss: 0.06892618151775423. Batch 15800 of 127656\n",
            "Mean loss: 0.05276693882609379. Batch 15900 of 127656\n",
            "Mean loss: 0.04637623712045752. Batch 16000 of 127656\n",
            "Mean loss: 0.07685551392323077. Batch 16100 of 127656\n",
            "Mean loss: 0.062216332544599025. Batch 16200 of 127656\n",
            "Mean loss: 0.08562701728488718. Batch 16300 of 127656\n",
            "Mean loss: 0.16633010239675058. Batch 16400 of 127656\n",
            "Mean loss: 0.06740885577826702. Batch 16500 of 127656\n",
            "Mean loss: 0.12320794382947498. Batch 16600 of 127656\n",
            "Mean loss: 0.05251839572854806. Batch 16700 of 127656\n",
            "Mean loss: 0.1071017760246923. Batch 16800 of 127656\n",
            "Mean loss: 0.08246454108637408. Batch 16900 of 127656\n",
            "Mean loss: 0.07852393206058877. Batch 17000 of 127656\n",
            "Mean loss: 0.07287523166402934. Batch 17100 of 127656\n",
            "Mean loss: 0.13139449027570663. Batch 17200 of 127656\n",
            "Mean loss: 0.1034692200630161. Batch 17300 of 127656\n",
            "Mean loss: 0.06551969675394503. Batch 17400 of 127656\n",
            "Mean loss: 0.06658392468972124. Batch 17500 of 127656\n",
            "Mean loss: 0.12051182065479224. Batch 17600 of 127656\n",
            "Mean loss: 0.13351420661118027. Batch 17700 of 127656\n",
            "Mean loss: 0.05798232182038191. Batch 17800 of 127656\n",
            "Mean loss: 0.09247426307420029. Batch 17900 of 127656\n",
            "Mean loss: 0.10942882458406529. Batch 18000 of 127656\n",
            "Mean loss: 0.06497086787979242. Batch 18100 of 127656\n",
            "Mean loss: 0.08661729128823936. Batch 18200 of 127656\n",
            "Mean loss: 0.09291446279834417. Batch 18300 of 127656\n",
            "Mean loss: 0.07248091192303036. Batch 18400 of 127656\n",
            "Mean loss: 0.06705937925790749. Batch 18500 of 127656\n",
            "Mean loss: 0.08060945444110984. Batch 18600 of 127656\n",
            "Mean loss: 0.11090958738484914. Batch 18700 of 127656\n",
            "Mean loss: 0.11622960096617135. Batch 18800 of 127656\n",
            "Mean loss: 0.03985054933134961. Batch 18900 of 127656\n",
            "Mean loss: 0.05779169300232752. Batch 19000 of 127656\n",
            "Mean loss: 0.11789185528021334. Batch 19100 of 127656\n",
            "Mean loss: 0.0425055351741139. Batch 19200 of 127656\n",
            "Mean loss: 0.09286207106185429. Batch 19300 of 127656\n",
            "Mean loss: 0.0822930520433556. Batch 19400 of 127656\n",
            "Mean loss: 0.09251757826933499. Batch 19500 of 127656\n",
            "Mean loss: 0.10192123917455319. Batch 19600 of 127656\n",
            "Mean loss: 0.08595592499830673. Batch 19700 of 127656\n",
            "Mean loss: 0.08804863441189809. Batch 19800 of 127656\n",
            "Mean loss: 0.09665102589163325. Batch 19900 of 127656\n",
            "Mean loss: 0.06321638887075551. Batch 20000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.09991141151998363. Batch 20100 of 127656\n",
            "Mean loss: 0.10633360357873244. Batch 20200 of 127656\n",
            "Mean loss: 0.02538187710628563. Batch 20300 of 127656\n",
            "Mean loss: 0.06683936981486795. Batch 20400 of 127656\n",
            "Mean loss: 0.05250309005389681. Batch 20500 of 127656\n",
            "Mean loss: 0.04410711326168439. Batch 20600 of 127656\n",
            "Mean loss: 0.12209115065245442. Batch 20700 of 127656\n",
            "Mean loss: 0.1031864589821589. Batch 20800 of 127656\n",
            "Mean loss: 0.1009934145618172. Batch 20900 of 127656\n",
            "Mean loss: 0.08091095139414392. Batch 21000 of 127656\n",
            "Mean loss: 0.03696300622208582. Batch 21100 of 127656\n",
            "Mean loss: 0.04977599340129018. Batch 21200 of 127656\n",
            "Mean loss: 0.07229292767149673. Batch 21300 of 127656\n",
            "Mean loss: 0.05572550134471385. Batch 21400 of 127656\n",
            "Mean loss: 0.0762904280226212. Batch 21500 of 127656\n",
            "Mean loss: 0.036524673481471835. Batch 21600 of 127656\n",
            "Mean loss: 0.07288296528044157. Batch 21700 of 127656\n",
            "Mean loss: 0.0472452971277994. Batch 21800 of 127656\n",
            "Mean loss: 0.16925627183031794. Batch 21900 of 127656\n",
            "Mean loss: 0.07275992218637839. Batch 22000 of 127656\n",
            "Mean loss: 0.04188670589828689. Batch 22100 of 127656\n",
            "Mean loss: 0.06218900520732859. Batch 22200 of 127656\n",
            "Mean loss: 0.11934619546293107. Batch 22300 of 127656\n",
            "Mean loss: 0.0584861403564355. Batch 22400 of 127656\n",
            "Mean loss: 0.05140229149350489. Batch 22500 of 127656\n",
            "Mean loss: 0.10620969946625337. Batch 22600 of 127656\n",
            "Mean loss: 0.05451895394386156. Batch 22700 of 127656\n",
            "Mean loss: 0.09220522694202372. Batch 22800 of 127656\n",
            "Mean loss: 0.0945077749666234. Batch 22900 of 127656\n",
            "Mean loss: 0.08623768870269487. Batch 23000 of 127656\n",
            "Mean loss: 0.02619232774262855. Batch 23100 of 127656\n",
            "Mean loss: 0.08921581127642639. Batch 23200 of 127656\n",
            "Mean loss: 0.06585037573207955. Batch 23300 of 127656\n",
            "Mean loss: 0.058850506381295416. Batch 23400 of 127656\n",
            "Mean loss: 0.08590930024574846. Batch 23500 of 127656\n",
            "Mean loss: 0.058725622591009594. Batch 23600 of 127656\n",
            "Mean loss: 0.12282359492855903. Batch 23700 of 127656\n",
            "Mean loss: 0.07782691155414795. Batch 23800 of 127656\n",
            "Mean loss: 0.06725433311785309. Batch 23900 of 127656\n",
            "Mean loss: 0.029401282408748557. Batch 24000 of 127656\n",
            "Mean loss: 0.10163503548617882. Batch 24100 of 127656\n",
            "Mean loss: 0.06374054635751236. Batch 24200 of 127656\n",
            "Mean loss: 0.05448093249960948. Batch 24300 of 127656\n",
            "Mean loss: 0.06967046472362198. Batch 24400 of 127656\n",
            "Mean loss: 0.03646644402197126. Batch 24500 of 127656\n",
            "Mean loss: 0.09247486813157593. Batch 24600 of 127656\n",
            "Mean loss: 0.056288760043789805. Batch 24700 of 127656\n",
            "Mean loss: 0.06650328818336221. Batch 24800 of 127656\n",
            "Mean loss: 0.07056415265087708. Batch 24900 of 127656\n",
            "Mean loss: 0.09165451300114. Batch 25000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.03288335182125593. Batch 25100 of 127656\n",
            "Mean loss: 0.08262592991806741. Batch 25200 of 127656\n",
            "Mean loss: 0.062708925700681. Batch 25300 of 127656\n",
            "Mean loss: 0.044337175559267056. Batch 25400 of 127656\n",
            "Mean loss: 0.022550194058469517. Batch 25500 of 127656\n",
            "Mean loss: 0.06781185896155875. Batch 25600 of 127656\n",
            "Mean loss: 0.10411108921692175. Batch 25700 of 127656\n",
            "Mean loss: 0.053522371261660734. Batch 25800 of 127656\n",
            "Mean loss: 0.03077687540550869. Batch 25900 of 127656\n",
            "Mean loss: 0.040192117490896635. Batch 26000 of 127656\n",
            "Mean loss: 0.1218877559752218. Batch 26100 of 127656\n",
            "Mean loss: 0.07608717569411966. Batch 26200 of 127656\n",
            "Mean loss: 0.07240131983606261. Batch 26300 of 127656\n",
            "Mean loss: 0.058228336946740455. Batch 26400 of 127656\n",
            "Mean loss: 0.042910676143155795. Batch 26500 of 127656\n",
            "Mean loss: 0.07282726354456372. Batch 26600 of 127656\n",
            "Mean loss: 0.06444447195152861. Batch 26700 of 127656\n",
            "Mean loss: 0.060885943027251414. Batch 26800 of 127656\n",
            "Mean loss: 0.04537002692090141. Batch 26900 of 127656\n",
            "Mean loss: 0.0492555981502187. Batch 27000 of 127656\n",
            "Mean loss: 0.055180025974173076. Batch 27100 of 127656\n",
            "Mean loss: 0.07011122781261747. Batch 27200 of 127656\n",
            "Mean loss: 0.060817692090608944. Batch 27300 of 127656\n",
            "Mean loss: 0.04763481662679169. Batch 27400 of 127656\n",
            "Mean loss: 0.019943999376539522. Batch 27500 of 127656\n",
            "Mean loss: 0.11120185203454866. Batch 27600 of 127656\n",
            "Mean loss: 0.07366798939723594. Batch 27700 of 127656\n",
            "Mean loss: 0.03256596343225738. Batch 27800 of 127656\n",
            "Mean loss: 0.07551290004359544. Batch 27900 of 127656\n",
            "Mean loss: 0.03465197645718035. Batch 28000 of 127656\n",
            "Mean loss: 0.06801081218163063. Batch 28100 of 127656\n",
            "Mean loss: 0.06069393747491972. Batch 28200 of 127656\n",
            "Mean loss: 0.04886251890766118. Batch 28300 of 127656\n",
            "Mean loss: 0.14947758109697815. Batch 28400 of 127656\n",
            "Mean loss: 0.10706262623221846. Batch 28500 of 127656\n",
            "Mean loss: 0.1011067117958737. Batch 28600 of 127656\n",
            "Mean loss: 0.03928937094337016. Batch 28700 of 127656\n",
            "Mean loss: 0.07493728914247186. Batch 28800 of 127656\n",
            "Mean loss: 0.07390955915914674. Batch 28900 of 127656\n",
            "Mean loss: 0.06096830226415477. Batch 29000 of 127656\n",
            "Mean loss: 0.10697334571410465. Batch 29100 of 127656\n",
            "Mean loss: 0.10576552098733373. Batch 29200 of 127656\n",
            "Mean loss: 0.09966575491778712. Batch 29300 of 127656\n",
            "Mean loss: 0.03224734270945191. Batch 29400 of 127656\n",
            "Mean loss: 0.0598490593774477. Batch 29500 of 127656\n",
            "Mean loss: 0.03269735572510399. Batch 29600 of 127656\n",
            "Mean loss: 0.05702985161150537. Batch 29700 of 127656\n",
            "Mean loss: 0.08002987284986375. Batch 29800 of 127656\n",
            "Mean loss: 0.05459106189483464. Batch 29900 of 127656\n",
            "Mean loss: 0.06053933000881443. Batch 30000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.07981181514897799. Batch 30100 of 127656\n",
            "Mean loss: 0.13605306224863853. Batch 30200 of 127656\n",
            "Mean loss: 0.09404611838646815. Batch 30300 of 127656\n",
            "Mean loss: 0.05506524738506414. Batch 30400 of 127656\n",
            "Mean loss: 0.09487123591048657. Batch 30500 of 127656\n",
            "Mean loss: 0.07545930123191283. Batch 30600 of 127656\n",
            "Mean loss: 0.09749323566011299. Batch 30700 of 127656\n",
            "Mean loss: 0.02313309882852991. Batch 30800 of 127656\n",
            "Mean loss: 0.09024279071840283. Batch 30900 of 127656\n",
            "Mean loss: 0.019440413261345383. Batch 31000 of 127656\n",
            "Mean loss: 0.06534237346943883. Batch 31100 of 127656\n",
            "Mean loss: 0.059459686524460265. Batch 31200 of 127656\n",
            "Mean loss: 0.11221463198387938. Batch 31300 of 127656\n",
            "Mean loss: 0.051979477987479185. Batch 31400 of 127656\n",
            "Mean loss: 0.07611107744854963. Batch 31500 of 127656\n",
            "Mean loss: 0.07638373108948145. Batch 31600 of 127656\n",
            "Mean loss: 0.06493332951751654. Batch 31700 of 127656\n",
            "Mean loss: 0.08454585307343222. Batch 31800 of 127656\n",
            "Mean loss: 0.07611132811900462. Batch 31900 of 127656\n",
            "Mean loss: 0.12019945071508119. Batch 32000 of 127656\n",
            "Mean loss: 0.11644746353209484. Batch 32100 of 127656\n",
            "Mean loss: 0.06977904037164989. Batch 32200 of 127656\n",
            "Mean loss: 0.047540886415690695. Batch 32300 of 127656\n",
            "Mean loss: 0.1033921563335025. Batch 32400 of 127656\n",
            "Mean loss: 0.042246597633657075. Batch 32500 of 127656\n",
            "Mean loss: 0.1235060501454609. Batch 32600 of 127656\n",
            "Mean loss: 0.10666169636169798. Batch 32700 of 127656\n",
            "Mean loss: 0.05647023249224731. Batch 32800 of 127656\n",
            "Mean loss: 0.06815400342118665. Batch 32900 of 127656\n",
            "Mean loss: 0.08909937637858092. Batch 33000 of 127656\n",
            "Mean loss: 0.05965164603658195. Batch 33100 of 127656\n",
            "Mean loss: 0.12667235570572302. Batch 33200 of 127656\n",
            "Mean loss: 0.05860843732974899. Batch 33300 of 127656\n",
            "Mean loss: 0.07439945760172123. Batch 33400 of 127656\n",
            "Mean loss: 0.046744762182188426. Batch 33500 of 127656\n",
            "Mean loss: 0.07156915054507408. Batch 33600 of 127656\n",
            "Mean loss: 0.11133424402116361. Batch 33700 of 127656\n",
            "Mean loss: 0.05415885338403314. Batch 33800 of 127656\n",
            "Mean loss: 0.10843164678595713. Batch 33900 of 127656\n",
            "Mean loss: 0.05382835772657927. Batch 34000 of 127656\n",
            "Mean loss: 0.12448719209312913. Batch 34100 of 127656\n",
            "Mean loss: 0.08379299603948312. Batch 34200 of 127656\n",
            "Mean loss: 0.0871318599834558. Batch 34300 of 127656\n",
            "Mean loss: 0.10132369045913947. Batch 34400 of 127656\n",
            "Mean loss: 0.06048202616031631. Batch 34500 of 127656\n",
            "Mean loss: 0.04481157930891641. Batch 34600 of 127656\n",
            "Mean loss: 0.13158244892991205. Batch 34700 of 127656\n",
            "Mean loss: 0.09113380740818684. Batch 34800 of 127656\n",
            "Mean loss: 0.08649105974665872. Batch 34900 of 127656\n",
            "Mean loss: 0.1036405140554234. Batch 35000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.07915080783262965. Batch 35100 of 127656\n",
            "Mean loss: 0.062195983911260556. Batch 35200 of 127656\n",
            "Mean loss: 0.0807368352175763. Batch 35300 of 127656\n",
            "Mean loss: 0.04905719809155016. Batch 35400 of 127656\n",
            "Mean loss: 0.07599945409503561. Batch 35500 of 127656\n",
            "Mean loss: 0.049277142333703526. Batch 35600 of 127656\n",
            "Mean loss: 0.10491201336224548. Batch 35700 of 127656\n",
            "Mean loss: 0.057142426673453886. Batch 35800 of 127656\n",
            "Mean loss: 0.10131798192896894. Batch 35900 of 127656\n",
            "Mean loss: 0.06248145201163425. Batch 36000 of 127656\n",
            "Mean loss: 0.10923168471680583. Batch 36100 of 127656\n",
            "Mean loss: 0.0558460692936751. Batch 36200 of 127656\n",
            "Mean loss: 0.08257387252704575. Batch 36300 of 127656\n",
            "Mean loss: 0.11356009525272384. Batch 36400 of 127656\n",
            "Mean loss: 0.11083190875840955. Batch 36500 of 127656\n",
            "Mean loss: 0.060857513932469375. Batch 36600 of 127656\n",
            "Mean loss: 0.08802305481865914. Batch 36700 of 127656\n",
            "Mean loss: 0.0911121015632898. Batch 36800 of 127656\n",
            "Mean loss: 0.09497790593978835. Batch 36900 of 127656\n",
            "Mean loss: 0.044549211806652236. Batch 37000 of 127656\n",
            "Mean loss: 0.1216572406953128. Batch 37100 of 127656\n",
            "Mean loss: 0.06895614047172785. Batch 37200 of 127656\n",
            "Mean loss: 0.038102495916418774. Batch 37300 of 127656\n",
            "Mean loss: 0.05041271115502241. Batch 37400 of 127656\n",
            "Mean loss: 0.05425057863503753. Batch 37500 of 127656\n",
            "Mean loss: 0.16106854791385558. Batch 37600 of 127656\n",
            "Mean loss: 0.0368167123789317. Batch 37700 of 127656\n",
            "Mean loss: 0.04870264629920712. Batch 37800 of 127656\n",
            "Mean loss: 0.057367843361280396. Batch 37900 of 127656\n",
            "Mean loss: 0.1296488933238288. Batch 38000 of 127656\n",
            "Mean loss: 0.0799081071861292. Batch 38100 of 127656\n",
            "Mean loss: 0.06972533764595937. Batch 38200 of 127656\n",
            "Mean loss: 0.050519189420836025. Batch 38300 of 127656\n",
            "Mean loss: 0.05699544325631223. Batch 38400 of 127656\n",
            "Mean loss: 0.0590466561474841. Batch 38500 of 127656\n",
            "Mean loss: 0.08219937025845865. Batch 38600 of 127656\n",
            "Mean loss: 0.059004877549532465. Batch 38700 of 127656\n",
            "Mean loss: 0.07225594820450625. Batch 38800 of 127656\n",
            "Mean loss: 0.1988000435039271. Batch 38900 of 127656\n",
            "Mean loss: 0.10735336831105087. Batch 39000 of 127656\n",
            "Mean loss: 0.067239310091245. Batch 39100 of 127656\n",
            "Mean loss: 0.07496871038987592. Batch 39200 of 127656\n",
            "Mean loss: 0.05302742456195119. Batch 39300 of 127656\n",
            "Mean loss: 0.06530896273908184. Batch 39400 of 127656\n",
            "Mean loss: 0.06120336396984385. Batch 39500 of 127656\n",
            "Mean loss: 0.12067148184175039. Batch 39600 of 127656\n",
            "Mean loss: 0.06511070976463089. Batch 39700 of 127656\n",
            "Mean loss: 0.038794513262691906. Batch 39800 of 127656\n",
            "Mean loss: 0.03096460897984798. Batch 39900 of 127656\n",
            "Mean loss: 0.05562828402955347. Batch 40000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.031166374867889316. Batch 40100 of 127656\n",
            "Mean loss: 0.0834798669562042. Batch 40200 of 127656\n",
            "Mean loss: 0.09213147252460657. Batch 40300 of 127656\n",
            "Mean loss: 0.1186087463405329. Batch 40400 of 127656\n",
            "Mean loss: 0.10717267618209007. Batch 40500 of 127656\n",
            "Mean loss: 0.07587876381916431. Batch 40600 of 127656\n",
            "Mean loss: 0.0928332889964895. Batch 40700 of 127656\n",
            "Mean loss: 0.08730222589194454. Batch 40800 of 127656\n",
            "Mean loss: 0.09878673585149954. Batch 40900 of 127656\n",
            "Mean loss: 0.052044523984186526. Batch 41000 of 127656\n",
            "Mean loss: 0.07823005090984225. Batch 41100 of 127656\n",
            "Mean loss: 0.0745200336131984. Batch 41200 of 127656\n",
            "Mean loss: 0.08360030123422803. Batch 41300 of 127656\n",
            "Mean loss: 0.04920006002070295. Batch 41400 of 127656\n",
            "Mean loss: 0.10185940176257872. Batch 41500 of 127656\n",
            "Mean loss: 0.09728684980298567. Batch 41600 of 127656\n",
            "Mean loss: 0.0880370617796143. Batch 41700 of 127656\n",
            "Mean loss: 0.07089254452002933. Batch 41800 of 127656\n",
            "Mean loss: 0.09055044991881005. Batch 41900 of 127656\n",
            "Mean loss: 0.0906350921127705. Batch 42000 of 127656\n",
            "Mean loss: 0.13204948543265346. Batch 42100 of 127656\n",
            "Mean loss: 0.058759006011787275. Batch 42200 of 127656\n",
            "Mean loss: 0.05851902297961715. Batch 42300 of 127656\n",
            "Mean loss: 0.10720055340672843. Batch 42400 of 127656\n",
            "Mean loss: 0.06021670701252333. Batch 42500 of 127656\n",
            "Mean loss: 0.12882401642694277. Batch 42600 of 127656\n",
            "Mean loss: 0.04997342875889444. Batch 42700 of 127656\n",
            "Mean loss: 0.025976942884410617. Batch 42800 of 127656\n",
            "Mean loss: 0.07885589593204713. Batch 42900 of 127656\n",
            "Mean loss: 0.11832661146654118. Batch 43000 of 127656\n",
            "Mean loss: 0.06969434064596498. Batch 43100 of 127656\n",
            "Mean loss: 0.07857776702727279. Batch 43200 of 127656\n",
            "Mean loss: 0.06259795976242458. Batch 43300 of 127656\n",
            "Mean loss: 0.1012386932555819. Batch 43400 of 127656\n",
            "Mean loss: 0.05005689927891581. Batch 43500 of 127656\n",
            "Mean loss: 0.09385656790280336. Batch 43600 of 127656\n",
            "Mean loss: 0.08486948384517745. Batch 43700 of 127656\n",
            "Mean loss: 0.11081858343641215. Batch 43800 of 127656\n",
            "Mean loss: 0.055706534778801144. Batch 43900 of 127656\n",
            "Mean loss: 0.06752225735379397. Batch 44000 of 127656\n",
            "Mean loss: 0.10078484275305072. Batch 44100 of 127656\n",
            "Mean loss: 0.078384454170573. Batch 44200 of 127656\n",
            "Mean loss: 0.062047917239888194. Batch 44300 of 127656\n",
            "Mean loss: 0.09773574709653075. Batch 44400 of 127656\n",
            "Mean loss: 0.12076042303380746. Batch 44500 of 127656\n",
            "Mean loss: 0.05758578557588408. Batch 44600 of 127656\n",
            "Mean loss: 0.01516340300279353. Batch 44700 of 127656\n",
            "Mean loss: 0.05384451599323711. Batch 44800 of 127656\n",
            "Mean loss: 0.09431214681655548. Batch 44900 of 127656\n",
            "Mean loss: 0.09700592914330627. Batch 45000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.08121706124872617. Batch 45100 of 127656\n",
            "Mean loss: 0.04902753496979813. Batch 45200 of 127656\n",
            "Mean loss: 0.03331466056054978. Batch 45300 of 127656\n",
            "Mean loss: 0.05844090425982813. Batch 45400 of 127656\n",
            "Mean loss: 0.024578532367361277. Batch 45500 of 127656\n",
            "Mean loss: 0.083809038382758. Batch 45600 of 127656\n",
            "Mean loss: 0.07626791998042791. Batch 45700 of 127656\n",
            "Mean loss: 0.027991212667195668. Batch 45800 of 127656\n",
            "Mean loss: 0.10035781938940261. Batch 45900 of 127656\n",
            "Mean loss: 0.06456361791172639. Batch 46000 of 127656\n",
            "Mean loss: 0.05492690945364529. Batch 46100 of 127656\n",
            "Mean loss: 0.03997774715669834. Batch 46200 of 127656\n",
            "Mean loss: 0.09220992472162379. Batch 46300 of 127656\n",
            "Mean loss: 0.05896491849314771. Batch 46400 of 127656\n",
            "Mean loss: 0.07851660103843187. Batch 46500 of 127656\n",
            "Mean loss: 0.06379782725174664. Batch 46600 of 127656\n",
            "Mean loss: 0.16610476724943965. Batch 46700 of 127656\n",
            "Mean loss: 0.07887591197744769. Batch 46800 of 127656\n",
            "Mean loss: 0.12840321405557917. Batch 46900 of 127656\n",
            "Mean loss: 0.04599669082404944. Batch 47000 of 127656\n",
            "Mean loss: 0.111481487271667. Batch 47100 of 127656\n",
            "Mean loss: 0.06776049004845845. Batch 47200 of 127656\n",
            "Mean loss: 0.06208909606646557. Batch 47300 of 127656\n",
            "Mean loss: 0.03789318393882695. Batch 47400 of 127656\n",
            "Mean loss: 0.023166484372732155. Batch 47500 of 127656\n",
            "Mean loss: 0.13752236410726026. Batch 47600 of 127656\n",
            "Mean loss: 0.047269655185700685. Batch 47700 of 127656\n",
            "Mean loss: 0.10117637313678643. Batch 47800 of 127656\n",
            "Mean loss: 0.09081730719501593. Batch 47900 of 127656\n",
            "Mean loss: 0.03962558014316528. Batch 48000 of 127656\n",
            "Mean loss: 0.07466185336671514. Batch 48100 of 127656\n",
            "Mean loss: 0.051097213004818516. Batch 48200 of 127656\n",
            "Mean loss: 0.07102240214813037. Batch 48300 of 127656\n",
            "Mean loss: 0.10109087130297667. Batch 48400 of 127656\n",
            "Mean loss: 0.051822978560303455. Batch 48500 of 127656\n",
            "Mean loss: 0.11990307151661227. Batch 48600 of 127656\n",
            "Mean loss: 0.0835145199046201. Batch 48700 of 127656\n",
            "Mean loss: 0.09455850281083258. Batch 48800 of 127656\n",
            "Mean loss: 0.07008781525105405. Batch 48900 of 127656\n",
            "Mean loss: 0.05220016997771381. Batch 49000 of 127656\n",
            "Mean loss: 0.057099603411757016. Batch 49100 of 127656\n",
            "Mean loss: 0.03642792786768723. Batch 49200 of 127656\n",
            "Mean loss: 0.06967079111174712. Batch 49300 of 127656\n",
            "Mean loss: 0.05370717933526976. Batch 49400 of 127656\n",
            "Mean loss: 0.10997884985459677. Batch 49500 of 127656\n",
            "Mean loss: 0.09428006847516372. Batch 49600 of 127656\n",
            "Mean loss: 0.05478894791995117. Batch 49700 of 127656\n",
            "Mean loss: 0.04436269451207863. Batch 49800 of 127656\n",
            "Mean loss: 0.08743927438186802. Batch 49900 of 127656\n",
            "Mean loss: 0.025260256978067445. Batch 50000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.07661801171846491. Batch 50100 of 127656\n",
            "Mean loss: 0.0886756922775112. Batch 50200 of 127656\n",
            "Mean loss: 0.06642147352534267. Batch 50300 of 127656\n",
            "Mean loss: 0.03377888985535264. Batch 50400 of 127656\n",
            "Mean loss: 0.07512640838852576. Batch 50500 of 127656\n",
            "Mean loss: 0.056402826827652464. Batch 50600 of 127656\n",
            "Mean loss: 0.06657947712394162. Batch 50700 of 127656\n",
            "Mean loss: 0.1001697267212785. Batch 50800 of 127656\n",
            "Mean loss: 0.06377521270808074. Batch 50900 of 127656\n",
            "Mean loss: 0.08644276772314584. Batch 51000 of 127656\n",
            "Mean loss: 0.06587955915332713. Batch 51100 of 127656\n",
            "Mean loss: 0.08860447215483873. Batch 51200 of 127656\n",
            "Mean loss: 0.06322921878032503. Batch 51300 of 127656\n",
            "Mean loss: 0.07037592391534417. Batch 51400 of 127656\n",
            "Mean loss: 0.09711883316058448. Batch 51500 of 127656\n",
            "Mean loss: 0.09336594504435197. Batch 51600 of 127656\n",
            "Mean loss: 0.09760763305919681. Batch 51700 of 127656\n",
            "Mean loss: 0.016305874239333207. Batch 51800 of 127656\n",
            "Mean loss: 0.06036941968717656. Batch 51900 of 127656\n",
            "Mean loss: 0.0740689231542865. Batch 52000 of 127656\n",
            "Mean loss: 0.05361440957201921. Batch 52100 of 127656\n",
            "Mean loss: 0.08680592225825876. Batch 52200 of 127656\n",
            "Mean loss: 0.0567106990440152. Batch 52300 of 127656\n",
            "Mean loss: 0.032282847222541024. Batch 52400 of 127656\n",
            "Mean loss: 0.09419348266142151. Batch 52500 of 127656\n",
            "Mean loss: 0.04466798685704816. Batch 52600 of 127656\n",
            "Mean loss: 0.07384130311467743. Batch 52700 of 127656\n",
            "Mean loss: 0.12282474195932082. Batch 52800 of 127656\n",
            "Mean loss: 0.06457290595950325. Batch 52900 of 127656\n",
            "Mean loss: 0.10236023902698435. Batch 53000 of 127656\n",
            "Mean loss: 0.10315802430108306. Batch 53100 of 127656\n",
            "Mean loss: 0.06973316712494125. Batch 53200 of 127656\n",
            "Mean loss: 0.05758883774644346. Batch 53300 of 127656\n",
            "Mean loss: 0.053226425688062594. Batch 53400 of 127656\n",
            "Mean loss: 0.09189424524185597. Batch 53500 of 127656\n",
            "Mean loss: 0.07318453948781098. Batch 53600 of 127656\n",
            "Mean loss: 0.05792301349756599. Batch 53700 of 127656\n",
            "Mean loss: 0.04200540152659414. Batch 53800 of 127656\n",
            "Mean loss: 0.07438859278584459. Batch 53900 of 127656\n",
            "Mean loss: 0.06293506194735528. Batch 54000 of 127656\n",
            "Mean loss: 0.057889329789813926. Batch 54100 of 127656\n",
            "Mean loss: 0.07643592966325287. Batch 54200 of 127656\n",
            "Mean loss: 0.07225841930237947. Batch 54300 of 127656\n",
            "Mean loss: 0.12786433738490813. Batch 54400 of 127656\n",
            "Mean loss: 0.09044600261547203. Batch 54500 of 127656\n",
            "Mean loss: 0.05707430213464249. Batch 54600 of 127656\n",
            "Mean loss: 0.06671111260833186. Batch 54700 of 127656\n",
            "Mean loss: 0.08658844348766252. Batch 54800 of 127656\n",
            "Mean loss: 0.054638884025839614. Batch 54900 of 127656\n",
            "Mean loss: 0.07654352837272199. Batch 55000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.07691496193210696. Batch 55100 of 127656\n",
            "Mean loss: 0.06278904500152749. Batch 55200 of 127656\n",
            "Mean loss: 0.10947390938157696. Batch 55300 of 127656\n",
            "Mean loss: 0.03734958397930313. Batch 55400 of 127656\n",
            "Mean loss: 0.045065979317041635. Batch 55500 of 127656\n",
            "Mean loss: 0.06262404200868332. Batch 55600 of 127656\n",
            "Mean loss: 0.09373982783603424. Batch 55700 of 127656\n",
            "Mean loss: 0.059041877534546074. Batch 55800 of 127656\n",
            "Mean loss: 0.08471111607146668. Batch 55900 of 127656\n",
            "Mean loss: 0.11541010855127752. Batch 56000 of 127656\n",
            "Mean loss: 0.07991316037723664. Batch 56100 of 127656\n",
            "Mean loss: 0.09693072498324909. Batch 56200 of 127656\n",
            "Mean loss: 0.08921357451939002. Batch 56300 of 127656\n",
            "Mean loss: 0.07025309844287676. Batch 56400 of 127656\n",
            "Mean loss: 0.128006095182277. Batch 56500 of 127656\n",
            "Mean loss: 0.046653627586638324. Batch 56600 of 127656\n",
            "Mean loss: 0.07970660971604047. Batch 56700 of 127656\n",
            "Mean loss: 0.06853073005573151. Batch 56800 of 127656\n",
            "Mean loss: 0.043025310426064604. Batch 56900 of 127656\n",
            "Mean loss: 0.09832331148797493. Batch 57000 of 127656\n",
            "Mean loss: 0.08654034446832157. Batch 57100 of 127656\n",
            "Mean loss: 0.07791264576266713. Batch 57200 of 127656\n",
            "Mean loss: 0.0337411098460143. Batch 57300 of 127656\n",
            "Mean loss: 0.053329452295292866. Batch 57400 of 127656\n",
            "Mean loss: 0.10810066902478412. Batch 57500 of 127656\n",
            "Mean loss: 0.0780650401332423. Batch 57600 of 127656\n",
            "Mean loss: 0.08272284045015113. Batch 57700 of 127656\n",
            "Mean loss: 0.07377724194318944. Batch 57800 of 127656\n",
            "Mean loss: 0.03503397514457902. Batch 57900 of 127656\n",
            "Mean loss: 0.044237803201413045. Batch 58000 of 127656\n",
            "Mean loss: 0.08771903982980803. Batch 58100 of 127656\n",
            "Mean loss: 0.03323858271126937. Batch 58200 of 127656\n",
            "Mean loss: 0.09072204425808195. Batch 58300 of 127656\n",
            "Mean loss: 0.06015155211300225. Batch 58400 of 127656\n",
            "Mean loss: 0.03728018544290535. Batch 58500 of 127656\n",
            "Mean loss: 0.07879401456584674. Batch 58600 of 127656\n",
            "Mean loss: 0.02516078344260677. Batch 58700 of 127656\n",
            "Mean loss: 0.07600700731081815. Batch 58800 of 127656\n",
            "Mean loss: 0.07177125589592834. Batch 58900 of 127656\n",
            "Mean loss: 0.08820379529313868. Batch 59000 of 127656\n",
            "Mean loss: 0.09202299908885835. Batch 59100 of 127656\n",
            "Mean loss: 0.11207895774497956. Batch 59200 of 127656\n",
            "Mean loss: 0.14202274227830003. Batch 59300 of 127656\n",
            "Mean loss: 0.03485842511361625. Batch 59400 of 127656\n",
            "Mean loss: 0.0666702405389151. Batch 59500 of 127656\n",
            "Mean loss: 0.07997448204335342. Batch 59600 of 127656\n",
            "Mean loss: 0.09149808166136154. Batch 59700 of 127656\n",
            "Mean loss: 0.0925934677270925. Batch 59800 of 127656\n",
            "Mean loss: 0.07364927355218243. Batch 59900 of 127656\n",
            "Mean loss: 0.046818945980903665. Batch 60000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.06686668502289649. Batch 60100 of 127656\n",
            "Mean loss: 0.05469307401894639. Batch 60200 of 127656\n",
            "Mean loss: 0.030281274486280835. Batch 60300 of 127656\n",
            "Mean loss: 0.09302593958336842. Batch 60400 of 127656\n",
            "Mean loss: 0.05814839939961985. Batch 60500 of 127656\n",
            "Mean loss: 0.08195291573497684. Batch 60600 of 127656\n",
            "Mean loss: 0.059023539951443806. Batch 60700 of 127656\n",
            "Mean loss: 0.11444656205560705. Batch 60800 of 127656\n",
            "Mean loss: 0.08995528291257869. Batch 60900 of 127656\n",
            "Mean loss: 0.09842165905547062. Batch 61000 of 127656\n",
            "Mean loss: 0.0950807491544947. Batch 61100 of 127656\n",
            "Mean loss: 0.050749746146575486. Batch 61200 of 127656\n",
            "Mean loss: 0.04287591635467379. Batch 61300 of 127656\n",
            "Mean loss: 0.020397835898185123. Batch 61400 of 127656\n",
            "Mean loss: 0.06659991058777451. Batch 61500 of 127656\n",
            "Mean loss: 0.06514067356010855. Batch 61600 of 127656\n",
            "Mean loss: 0.12416984163768348. Batch 61700 of 127656\n",
            "Mean loss: 0.06236106095586251. Batch 61800 of 127656\n",
            "Mean loss: 0.04714243024676762. Batch 61900 of 127656\n",
            "Mean loss: 0.11621131293292591. Batch 62000 of 127656\n",
            "Mean loss: 0.047309999290548604. Batch 62100 of 127656\n",
            "Mean loss: 0.026361373837783048. Batch 62200 of 127656\n",
            "Mean loss: 0.0574770887536971. Batch 62300 of 127656\n",
            "Mean loss: 0.04387859692111988. Batch 62400 of 127656\n",
            "Mean loss: 0.035201326522980025. Batch 62500 of 127656\n",
            "Mean loss: 0.05143909308288755. Batch 62600 of 127656\n",
            "Mean loss: 0.12025439015260644. Batch 62700 of 127656\n",
            "Mean loss: 0.11115990487422095. Batch 62800 of 127656\n",
            "Mean loss: 0.07386521872488629. Batch 62900 of 127656\n",
            "Mean loss: 0.05894839815305204. Batch 63000 of 127656\n",
            "Mean loss: 0.044393934935321795. Batch 63100 of 127656\n",
            "Mean loss: 0.07915291021858821. Batch 63200 of 127656\n",
            "Mean loss: 0.044893862434125825. Batch 63300 of 127656\n",
            "Mean loss: 0.048359951562571266. Batch 63400 of 127656\n",
            "Mean loss: 0.043495924149259506. Batch 63500 of 127656\n",
            "Mean loss: 0.07456123513395824. Batch 63600 of 127656\n",
            "Mean loss: 0.0784818947117219. Batch 63700 of 127656\n",
            "Mean loss: 0.08310740678500679. Batch 63800 of 127656\n",
            "Mean loss: 0.09167411391413224. Batch 63900 of 127656\n",
            "Mean loss: 0.08729124951322319. Batch 64000 of 127656\n",
            "Mean loss: 0.09087982753682809. Batch 64100 of 127656\n",
            "Mean loss: 0.06709432760925665. Batch 64200 of 127656\n",
            "Mean loss: 0.04306966072735577. Batch 64300 of 127656\n",
            "Mean loss: 0.05020674460428154. Batch 64400 of 127656\n",
            "Mean loss: 0.10613992311945367. Batch 64500 of 127656\n",
            "Mean loss: 0.07411685488740431. Batch 64600 of 127656\n",
            "Mean loss: 0.067958162515921. Batch 64700 of 127656\n",
            "Mean loss: 0.032175620341142804. Batch 64800 of 127656\n",
            "Mean loss: 0.0540371663749238. Batch 64900 of 127656\n",
            "Mean loss: 0.03330411464157692. Batch 65000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.0717838048317634. Batch 65100 of 127656\n",
            "Mean loss: 0.093383753497219. Batch 65200 of 127656\n",
            "Mean loss: 0.06382404224132188. Batch 65300 of 127656\n",
            "Mean loss: 0.059612333035565825. Batch 65400 of 127656\n",
            "Mean loss: 0.11143257360826737. Batch 65500 of 127656\n",
            "Mean loss: 0.11801869201505041. Batch 65600 of 127656\n",
            "Mean loss: 0.03609755342930839. Batch 65700 of 127656\n",
            "Mean loss: 0.046547335912895275. Batch 65800 of 127656\n",
            "Mean loss: 0.08221945792418978. Batch 65900 of 127656\n",
            "Mean loss: 0.04865638477491018. Batch 66000 of 127656\n",
            "Mean loss: 0.05996928065466534. Batch 66100 of 127656\n",
            "Mean loss: 0.03915329513209599. Batch 66200 of 127656\n",
            "Mean loss: 0.04114471609635984. Batch 66300 of 127656\n",
            "Mean loss: 0.04341067609099355. Batch 66400 of 127656\n",
            "Mean loss: 0.06874600845426926. Batch 66500 of 127656\n",
            "Mean loss: 0.10004839495021656. Batch 66600 of 127656\n",
            "Mean loss: 0.08781863908456672. Batch 66700 of 127656\n",
            "Mean loss: 0.030843131658093624. Batch 66800 of 127656\n",
            "Mean loss: 0.045034072381263286. Batch 66900 of 127656\n",
            "Mean loss: 0.08754965965416148. Batch 67000 of 127656\n",
            "Mean loss: 0.04943953654166762. Batch 67100 of 127656\n",
            "Mean loss: 0.0744271175162271. Batch 67200 of 127656\n",
            "Mean loss: 0.07128824008801189. Batch 67300 of 127656\n",
            "Mean loss: 0.09128391676236788. Batch 67400 of 127656\n",
            "Mean loss: 0.07089072905812145. Batch 67500 of 127656\n",
            "Mean loss: 0.052074051906947715. Batch 67600 of 127656\n",
            "Mean loss: 0.03823187636547431. Batch 67700 of 127656\n",
            "Mean loss: 0.08945874033248175. Batch 67800 of 127656\n",
            "Mean loss: 0.08020488653178574. Batch 67900 of 127656\n",
            "Mean loss: 0.0540088760168959. Batch 68000 of 127656\n",
            "Mean loss: 0.09654655906208517. Batch 68100 of 127656\n",
            "Mean loss: 0.03843008194614242. Batch 68200 of 127656\n",
            "Mean loss: 0.05457442301614151. Batch 68300 of 127656\n",
            "Mean loss: 0.06051116931483165. Batch 68400 of 127656\n",
            "Mean loss: 0.07365702877827289. Batch 68500 of 127656\n",
            "Mean loss: 0.09653785334336704. Batch 68600 of 127656\n",
            "Mean loss: 0.027584686753534697. Batch 68700 of 127656\n",
            "Mean loss: 0.06080626966933778. Batch 68800 of 127656\n",
            "Mean loss: 0.08605299272498997. Batch 68900 of 127656\n",
            "Mean loss: 0.03628441814078542. Batch 69000 of 127656\n",
            "Mean loss: 0.06316740686403137. Batch 69100 of 127656\n",
            "Mean loss: 0.06739275555795757. Batch 69200 of 127656\n",
            "Mean loss: 0.06396693681677107. Batch 69300 of 127656\n",
            "Mean loss: 0.045403485325896324. Batch 69400 of 127656\n",
            "Mean loss: 0.06783171332874645. Batch 69500 of 127656\n",
            "Mean loss: 0.0765503913017983. Batch 69600 of 127656\n",
            "Mean loss: 0.11495337672705318. Batch 69700 of 127656\n",
            "Mean loss: 0.03418067909187812. Batch 69800 of 127656\n",
            "Mean loss: 0.04030866888144146. Batch 69900 of 127656\n",
            "Mean loss: 0.0358351270998719. Batch 70000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.12170746867558364. Batch 70100 of 127656\n",
            "Mean loss: 0.08387663255060034. Batch 70200 of 127656\n",
            "Mean loss: 0.07804337658241821. Batch 70300 of 127656\n",
            "Mean loss: 0.04141561394460041. Batch 70400 of 127656\n",
            "Mean loss: 0.07764860012186774. Batch 70500 of 127656\n",
            "Mean loss: 0.07411351958512569. Batch 70600 of 127656\n",
            "Mean loss: 0.03970128910520998. Batch 70700 of 127656\n",
            "Mean loss: 0.03932833979401721. Batch 70800 of 127656\n",
            "Mean loss: 0.1050367096879154. Batch 70900 of 127656\n",
            "Mean loss: 0.06653433480350941. Batch 71000 of 127656\n",
            "Mean loss: 0.07205726228162349. Batch 71100 of 127656\n",
            "Mean loss: 0.048887031280055455. Batch 71200 of 127656\n",
            "Mean loss: 0.1556809147060585. Batch 71300 of 127656\n",
            "Mean loss: 0.04899612053730834. Batch 71400 of 127656\n",
            "Mean loss: 0.08196968663735787. Batch 71500 of 127656\n",
            "Mean loss: 0.08727498767655448. Batch 71600 of 127656\n",
            "Mean loss: 0.0619435366949142. Batch 71700 of 127656\n",
            "Mean loss: 0.09539239245460635. Batch 71800 of 127656\n",
            "Mean loss: 0.050602042920372695. Batch 71900 of 127656\n",
            "Mean loss: 0.036470422876225254. Batch 72000 of 127656\n",
            "Mean loss: 0.1232296378253659. Batch 72100 of 127656\n",
            "Mean loss: 0.07419723678100126. Batch 72200 of 127656\n",
            "Mean loss: 0.06266526356737813. Batch 72300 of 127656\n",
            "Mean loss: 0.07554452121803479. Batch 72400 of 127656\n",
            "Mean loss: 0.03736362741625271. Batch 72500 of 127656\n",
            "Mean loss: 0.04852984081555405. Batch 72600 of 127656\n",
            "Mean loss: 0.06341428409064065. Batch 72700 of 127656\n",
            "Mean loss: 0.06753023102074394. Batch 72800 of 127656\n",
            "Mean loss: 0.08814279074898422. Batch 72900 of 127656\n",
            "Mean loss: 0.03599078397310222. Batch 73000 of 127656\n",
            "Mean loss: 0.10193239530479559. Batch 73100 of 127656\n",
            "Mean loss: 0.11408392249753434. Batch 73200 of 127656\n",
            "Mean loss: 0.056596733043429596. Batch 73300 of 127656\n",
            "Mean loss: 0.048951739323550784. Batch 73400 of 127656\n",
            "Mean loss: 0.0633375290213371. Batch 73500 of 127656\n",
            "Mean loss: 0.06960283921252654. Batch 73600 of 127656\n",
            "Mean loss: 0.07691213666142631. Batch 73700 of 127656\n",
            "Mean loss: 0.06928622771597219. Batch 73800 of 127656\n",
            "Mean loss: 0.04214824325580139. Batch 73900 of 127656\n",
            "Mean loss: 0.0893573297357051. Batch 74000 of 127656\n",
            "Mean loss: 0.046448945995034595. Batch 74100 of 127656\n",
            "Mean loss: 0.04490334787569338. Batch 74200 of 127656\n",
            "Mean loss: 0.07270461178556274. Batch 74300 of 127656\n",
            "Mean loss: 0.03697960727944547. Batch 74400 of 127656\n",
            "Mean loss: 0.09812774521719518. Batch 74500 of 127656\n",
            "Mean loss: 0.14627653375001273. Batch 74600 of 127656\n",
            "Mean loss: 0.05940383869836296. Batch 74700 of 127656\n",
            "Mean loss: 0.041473272050343436. Batch 74800 of 127656\n",
            "Mean loss: 0.09847177658837608. Batch 74900 of 127656\n",
            "Mean loss: 0.06772861094606925. Batch 75000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.06236260767809654. Batch 75100 of 127656\n",
            "Mean loss: 0.05146041335949121. Batch 75200 of 127656\n",
            "Mean loss: 0.057850799387088044. Batch 75300 of 127656\n",
            "Mean loss: 0.045389330173470625. Batch 75400 of 127656\n",
            "Mean loss: 0.06839993480135036. Batch 75500 of 127656\n",
            "Mean loss: 0.05537610037907143. Batch 75600 of 127656\n",
            "Mean loss: 0.0286616668668762. Batch 75700 of 127656\n",
            "Mean loss: 0.11778601405058453. Batch 75800 of 127656\n",
            "Mean loss: 0.05901403388222434. Batch 75900 of 127656\n",
            "Mean loss: 0.05236416182600124. Batch 76000 of 127656\n",
            "Mean loss: 0.04572703329662147. Batch 76100 of 127656\n",
            "Mean loss: 0.06438163601665395. Batch 76200 of 127656\n",
            "Mean loss: 0.04467053906438309. Batch 76300 of 127656\n",
            "Mean loss: 0.05924064425567394. Batch 76400 of 127656\n",
            "Mean loss: 0.062421356075684574. Batch 76500 of 127656\n",
            "Mean loss: 0.06100322902959988. Batch 76600 of 127656\n",
            "Mean loss: 0.07254090485198503. Batch 76700 of 127656\n",
            "Mean loss: 0.05532059181383715. Batch 76800 of 127656\n",
            "Mean loss: 0.11441882067951155. Batch 76900 of 127656\n",
            "Mean loss: 0.08588689145862531. Batch 77000 of 127656\n",
            "Mean loss: 0.03555577648942176. Batch 77100 of 127656\n",
            "Mean loss: 0.02873735330571492. Batch 77200 of 127656\n",
            "Mean loss: 0.07420309467140555. Batch 77300 of 127656\n",
            "Mean loss: 0.08738467257536627. Batch 77400 of 127656\n",
            "Mean loss: 0.04719645930558073. Batch 77500 of 127656\n",
            "Mean loss: 0.0943000197931741. Batch 77600 of 127656\n",
            "Mean loss: 0.0901372931261892. Batch 77700 of 127656\n",
            "Mean loss: 0.13937089144713127. Batch 77800 of 127656\n",
            "Mean loss: 0.04950403879309306. Batch 77900 of 127656\n",
            "Mean loss: 0.061491673412092496. Batch 78000 of 127656\n",
            "Mean loss: 0.07517713424771501. Batch 78100 of 127656\n",
            "Mean loss: 0.08902124755559271. Batch 78200 of 127656\n",
            "Mean loss: 0.04602250602692948. Batch 78300 of 127656\n",
            "Mean loss: 0.03812668805168869. Batch 78400 of 127656\n",
            "Mean loss: 0.05835203701109094. Batch 78500 of 127656\n",
            "Mean loss: 0.03946106959727331. Batch 78600 of 127656\n",
            "Mean loss: 0.05419007858438817. Batch 78700 of 127656\n",
            "Mean loss: 0.0995841210969229. Batch 78800 of 127656\n",
            "Mean loss: 0.07623332872062746. Batch 78900 of 127656\n",
            "Mean loss: 0.10074041687148565. Batch 79000 of 127656\n",
            "Mean loss: 0.09473419480949814. Batch 79100 of 127656\n",
            "Mean loss: 0.04844333020053455. Batch 79200 of 127656\n",
            "Mean loss: 0.019554430494499685. Batch 79300 of 127656\n",
            "Mean loss: 0.04303906901805021. Batch 79400 of 127656\n",
            "Mean loss: 0.03470328745401275. Batch 79500 of 127656\n",
            "Mean loss: 0.07670796221203545. Batch 79600 of 127656\n",
            "Mean loss: 0.06963594505175934. Batch 79700 of 127656\n",
            "Mean loss: 0.03072095355913916. Batch 79800 of 127656\n",
            "Mean loss: 0.13738495410494578. Batch 79900 of 127656\n",
            "Mean loss: 0.058761892881702804. Batch 80000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.09409521995479736. Batch 80100 of 127656\n",
            "Mean loss: 0.08230192538197116. Batch 80200 of 127656\n",
            "Mean loss: 0.0850360271752288. Batch 80300 of 127656\n",
            "Mean loss: 0.09140562429675811. Batch 80400 of 127656\n",
            "Mean loss: 0.12912087965476984. Batch 80500 of 127656\n",
            "Mean loss: 0.07093731079075952. Batch 80600 of 127656\n",
            "Mean loss: 0.04089131664233719. Batch 80700 of 127656\n",
            "Mean loss: 0.06776904706624237. Batch 80800 of 127656\n",
            "Mean loss: 0.06398273915186564. Batch 80900 of 127656\n",
            "Mean loss: 0.09735845766128363. Batch 81000 of 127656\n",
            "Mean loss: 0.08379838102442591. Batch 81100 of 127656\n",
            "Mean loss: 0.04371829607131076. Batch 81200 of 127656\n",
            "Mean loss: 0.04846198558219839. Batch 81300 of 127656\n",
            "Mean loss: 0.04068303119547636. Batch 81400 of 127656\n",
            "Mean loss: 0.05712013235637642. Batch 81500 of 127656\n",
            "Mean loss: 0.14665187356657042. Batch 81600 of 127656\n",
            "Mean loss: 0.03635183601960307. Batch 81700 of 127656\n",
            "Mean loss: 0.06603027946104703. Batch 81800 of 127656\n",
            "Mean loss: 0.09109698941366787. Batch 81900 of 127656\n",
            "Mean loss: 0.036217790133042625. Batch 82000 of 127656\n",
            "Mean loss: 0.09829251458646922. Batch 82100 of 127656\n",
            "Mean loss: 0.08264161965096718. Batch 82200 of 127656\n",
            "Mean loss: 0.037938540433779054. Batch 82300 of 127656\n",
            "Mean loss: 0.03714165645964385. Batch 82400 of 127656\n",
            "Mean loss: 0.08733118207823282. Batch 82500 of 127656\n",
            "Mean loss: 0.040152389232389396. Batch 82600 of 127656\n",
            "Mean loss: 0.08193093722432422. Batch 82700 of 127656\n",
            "Mean loss: 0.05754527517275619. Batch 82800 of 127656\n",
            "Mean loss: 0.1233792933431414. Batch 82900 of 127656\n",
            "Mean loss: 0.08430863173955004. Batch 83000 of 127656\n",
            "Mean loss: 0.0910502528377583. Batch 83100 of 127656\n",
            "Mean loss: 0.05451696351013652. Batch 83200 of 127656\n",
            "Mean loss: 0.09478298959714948. Batch 83300 of 127656\n",
            "Mean loss: 0.08177079958177273. Batch 83400 of 127656\n",
            "Mean loss: 0.05817391561024124. Batch 83500 of 127656\n",
            "Mean loss: 0.05523797618296612. Batch 83600 of 127656\n",
            "Mean loss: 0.09137211661816877. Batch 83700 of 127656\n",
            "Mean loss: 0.06430953627212148. Batch 83800 of 127656\n",
            "Mean loss: 0.09303851573760767. Batch 83900 of 127656\n",
            "Mean loss: 0.044444878575632174. Batch 84000 of 127656\n",
            "Mean loss: 0.09874317605101168. Batch 84100 of 127656\n",
            "Mean loss: 0.08033954431339226. Batch 84200 of 127656\n",
            "Mean loss: 0.056131096851058826. Batch 84300 of 127656\n",
            "Mean loss: 0.0432588436115293. Batch 84400 of 127656\n",
            "Mean loss: 0.06346815013697778. Batch 84500 of 127656\n",
            "Mean loss: 0.01693531739208083. Batch 84600 of 127656\n",
            "Mean loss: 0.03528950430020359. Batch 84700 of 127656\n",
            "Mean loss: 0.046765900939712994. Batch 84800 of 127656\n",
            "Mean loss: 0.06553174692460743. Batch 84900 of 127656\n",
            "Mean loss: 0.06764227928460968. Batch 85000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.06612347410715301. Batch 85100 of 127656\n",
            "Mean loss: 0.04181469171455319. Batch 85200 of 127656\n",
            "Mean loss: 0.02376443226370384. Batch 85300 of 127656\n",
            "Mean loss: 0.07345626771369097. Batch 85400 of 127656\n",
            "Mean loss: 0.09348628294296532. Batch 85500 of 127656\n",
            "Mean loss: 0.10810535722222994. Batch 85600 of 127656\n",
            "Mean loss: 0.06017031550713. Batch 85700 of 127656\n",
            "Mean loss: 0.08459719898066397. Batch 85800 of 127656\n",
            "Mean loss: 0.12846716534040753. Batch 85900 of 127656\n",
            "Mean loss: 0.05706456318207529. Batch 86000 of 127656\n",
            "Mean loss: 0.050183194432756865. Batch 86100 of 127656\n",
            "Mean loss: 0.0506027369910953. Batch 86200 of 127656\n",
            "Mean loss: 0.07520937725476642. Batch 86300 of 127656\n",
            "Mean loss: 0.09501033247773194. Batch 86400 of 127656\n",
            "Mean loss: 0.037318134910885876. Batch 86500 of 127656\n",
            "Mean loss: 0.046505786787876105. Batch 86600 of 127656\n",
            "Mean loss: 0.03391272508398344. Batch 86700 of 127656\n",
            "Mean loss: 0.0898448809796693. Batch 86800 of 127656\n",
            "Mean loss: 0.12692042337078874. Batch 86900 of 127656\n",
            "Mean loss: 0.08565388845732741. Batch 87000 of 127656\n",
            "Mean loss: 0.06724975917458323. Batch 87100 of 127656\n",
            "Mean loss: 0.04027110980223142. Batch 87200 of 127656\n",
            "Mean loss: 0.09711674804405902. Batch 87300 of 127656\n",
            "Mean loss: 0.08580975538161056. Batch 87400 of 127656\n",
            "Mean loss: 0.06010975042795053. Batch 87500 of 127656\n",
            "Mean loss: 0.08691797434558225. Batch 87600 of 127656\n",
            "Mean loss: 0.07798177517414842. Batch 87700 of 127656\n",
            "Mean loss: 0.08816145333559688. Batch 87800 of 127656\n",
            "Mean loss: 0.06940858810434293. Batch 87900 of 127656\n",
            "Mean loss: 0.0834576065055967. Batch 88000 of 127656\n",
            "Mean loss: 0.1156533704152298. Batch 88100 of 127656\n",
            "Mean loss: 0.042634444243822144. Batch 88200 of 127656\n",
            "Mean loss: 0.09099521647181973. Batch 88300 of 127656\n",
            "Mean loss: 0.09945273486619044. Batch 88400 of 127656\n",
            "Mean loss: 0.07540280627221364. Batch 88500 of 127656\n",
            "Mean loss: 0.054910726316520594. Batch 88600 of 127656\n",
            "Mean loss: 0.05655615887049862. Batch 88700 of 127656\n",
            "Mean loss: 0.0755350739104665. Batch 88800 of 127656\n",
            "Mean loss: 0.09254072545300232. Batch 88900 of 127656\n",
            "Mean loss: 0.03989542873541268. Batch 89000 of 127656\n",
            "Mean loss: 0.05835707697059206. Batch 89100 of 127656\n",
            "Mean loss: 0.03829978946616848. Batch 89200 of 127656\n",
            "Mean loss: 0.09036846440528051. Batch 89300 of 127656\n",
            "Mean loss: 0.05495370773816376. Batch 89400 of 127656\n",
            "Mean loss: 0.09141895989761338. Batch 89500 of 127656\n",
            "Mean loss: 0.04035908423326731. Batch 89600 of 127656\n",
            "Mean loss: 0.05994381309750281. Batch 89700 of 127656\n",
            "Mean loss: 0.02916556231902007. Batch 89800 of 127656\n",
            "Mean loss: 0.06667276979738744. Batch 89900 of 127656\n",
            "Mean loss: 0.1399382042283014. Batch 90000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.07286779044152354. Batch 90100 of 127656\n",
            "Mean loss: 0.09866723698846727. Batch 90200 of 127656\n",
            "Mean loss: 0.03850471999368892. Batch 90300 of 127656\n",
            "Mean loss: 0.08300676627731718. Batch 90400 of 127656\n",
            "Mean loss: 0.09157718933838396. Batch 90500 of 127656\n",
            "Mean loss: 0.041837252781275536. Batch 90600 of 127656\n",
            "Mean loss: 0.0912059282998348. Batch 90700 of 127656\n",
            "Mean loss: 0.04451949704226308. Batch 90800 of 127656\n",
            "Mean loss: 0.05379500077709849. Batch 90900 of 127656\n",
            "Mean loss: 0.051509294239866964. Batch 91000 of 127656\n",
            "Mean loss: 0.0734017918667314. Batch 91100 of 127656\n",
            "Mean loss: 0.055011179915105686. Batch 91200 of 127656\n",
            "Mean loss: 0.09126151764604003. Batch 91300 of 127656\n",
            "Mean loss: 0.042866949415361545. Batch 91400 of 127656\n",
            "Mean loss: 0.07127751099685498. Batch 91500 of 127656\n",
            "Mean loss: 0.07241196201212005. Batch 91600 of 127656\n",
            "Mean loss: 0.10094519815453794. Batch 91700 of 127656\n",
            "Mean loss: 0.06106746865647665. Batch 91800 of 127656\n",
            "Mean loss: 0.0923673301258566. Batch 91900 of 127656\n",
            "Mean loss: 0.0839523681562514. Batch 92000 of 127656\n",
            "Mean loss: 0.07981092905959258. Batch 92100 of 127656\n",
            "Mean loss: 0.10036095884706811. Batch 92200 of 127656\n",
            "Mean loss: 0.07596074189566025. Batch 92300 of 127656\n",
            "Mean loss: 0.06969126255685296. Batch 92400 of 127656\n",
            "Mean loss: 0.03528693087714856. Batch 92500 of 127656\n",
            "Mean loss: 0.05859931899074468. Batch 92600 of 127656\n",
            "Mean loss: 0.05804565687419995. Batch 92700 of 127656\n",
            "Mean loss: 0.09280170814012763. Batch 92800 of 127656\n",
            "Mean loss: 0.08353299744754622. Batch 92900 of 127656\n",
            "Mean loss: 0.0813426474921107. Batch 93000 of 127656\n",
            "Mean loss: 0.05661790403737541. Batch 93100 of 127656\n",
            "Mean loss: 0.0441378866143441. Batch 93200 of 127656\n",
            "Mean loss: 0.056607389990113006. Batch 93300 of 127656\n",
            "Mean loss: 0.03624922157257061. Batch 93400 of 127656\n",
            "Mean loss: 0.07913322112777678. Batch 93500 of 127656\n",
            "Mean loss: 0.05459397892536799. Batch 93600 of 127656\n",
            "Mean loss: 0.03970017582709261. Batch 93700 of 127656\n",
            "Mean loss: 0.076340234619679. Batch 93800 of 127656\n",
            "Mean loss: 0.06702174785013995. Batch 93900 of 127656\n",
            "Mean loss: 0.06639883798765822. Batch 94000 of 127656\n",
            "Mean loss: 0.06167022612740766. Batch 94100 of 127656\n",
            "Mean loss: 0.035779756299443764. Batch 94200 of 127656\n",
            "Mean loss: 0.06496969721075857. Batch 94300 of 127656\n",
            "Mean loss: 0.04175705617939343. Batch 94400 of 127656\n",
            "Mean loss: 0.060590306473661995. Batch 94500 of 127656\n",
            "Mean loss: 0.06147466400002287. Batch 94600 of 127656\n",
            "Mean loss: 0.10479812374654728. Batch 94700 of 127656\n",
            "Mean loss: 0.07090859660205752. Batch 94800 of 127656\n",
            "Mean loss: 0.05512164367362402. Batch 94900 of 127656\n",
            "Mean loss: 0.047787979671147694. Batch 95000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.05322022042668323. Batch 95100 of 127656\n",
            "Mean loss: 0.06568496598567436. Batch 95200 of 127656\n",
            "Mean loss: 0.04128528684736466. Batch 95300 of 127656\n",
            "Mean loss: 0.045763996468240295. Batch 95400 of 127656\n",
            "Mean loss: 0.06307745170999284. Batch 95500 of 127656\n",
            "Mean loss: 0.06573503713364516. Batch 95600 of 127656\n",
            "Mean loss: 0.057396195236474344. Batch 95700 of 127656\n",
            "Mean loss: 0.05105779026604864. Batch 95800 of 127656\n",
            "Mean loss: 0.09860338798619182. Batch 95900 of 127656\n",
            "Mean loss: 0.09128587628046489. Batch 96000 of 127656\n",
            "Mean loss: 0.07434765521207737. Batch 96100 of 127656\n",
            "Mean loss: 0.049481028315894945. Batch 96200 of 127656\n",
            "Mean loss: 0.05080247039752521. Batch 96300 of 127656\n",
            "Mean loss: 0.08291639868345556. Batch 96400 of 127656\n",
            "Mean loss: 0.03236215829470893. Batch 96500 of 127656\n",
            "Mean loss: 0.05669530552875585. Batch 96600 of 127656\n",
            "Mean loss: 0.06646765447419967. Batch 96700 of 127656\n",
            "Mean loss: 0.07850523281491632. Batch 96800 of 127656\n",
            "Mean loss: 0.04689298518581893. Batch 96900 of 127656\n",
            "Mean loss: 0.06289132265143962. Batch 97000 of 127656\n",
            "Mean loss: 0.04627619905744723. Batch 97100 of 127656\n",
            "Mean loss: 0.05818094032308863. Batch 97200 of 127656\n",
            "Mean loss: 0.08376513836912296. Batch 97300 of 127656\n",
            "Mean loss: 0.08391906603378743. Batch 97400 of 127656\n",
            "Mean loss: 0.08731994916178792. Batch 97500 of 127656\n",
            "Mean loss: 0.06226103162571235. Batch 97600 of 127656\n",
            "Mean loss: 0.1013135619060995. Batch 97700 of 127656\n",
            "Mean loss: 0.08162647676706002. Batch 97800 of 127656\n",
            "Mean loss: 0.05291196836835752. Batch 97900 of 127656\n",
            "Mean loss: 0.08272442562704327. Batch 98000 of 127656\n",
            "Mean loss: 0.07265925696551222. Batch 98100 of 127656\n",
            "Mean loss: 0.019812955730844805. Batch 98200 of 127656\n",
            "Mean loss: 0.06181825814352806. Batch 98300 of 127656\n",
            "Mean loss: 0.08967039270339. Batch 98400 of 127656\n",
            "Mean loss: 0.06905546744751405. Batch 98500 of 127656\n",
            "Mean loss: 0.03882894691852698. Batch 98600 of 127656\n",
            "Mean loss: 0.0607940179170123. Batch 98700 of 127656\n",
            "Mean loss: 0.06609637610442633. Batch 98800 of 127656\n",
            "Mean loss: 0.06428287038622216. Batch 98900 of 127656\n",
            "Mean loss: 0.073924744713388. Batch 99000 of 127656\n",
            "Mean loss: 0.08281969630652952. Batch 99100 of 127656\n",
            "Mean loss: 0.07373223208880518. Batch 99200 of 127656\n",
            "Mean loss: 0.05347719597457399. Batch 99300 of 127656\n",
            "Mean loss: 0.0770514822581913. Batch 99400 of 127656\n",
            "Mean loss: 0.05848134476062228. Batch 99500 of 127656\n",
            "Mean loss: 0.04881912571035457. Batch 99600 of 127656\n",
            "Mean loss: 0.04106349788165829. Batch 99700 of 127656\n",
            "Mean loss: 0.09359160152990796. Batch 99800 of 127656\n",
            "Mean loss: 0.12784122433861397. Batch 99900 of 127656\n",
            "Mean loss: 0.03355088270572423. Batch 100000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.10416198805784915. Batch 100100 of 127656\n",
            "Mean loss: 0.04242116151615846. Batch 100200 of 127656\n",
            "Mean loss: 0.04367730919164387. Batch 100300 of 127656\n",
            "Mean loss: 0.08235900638270323. Batch 100400 of 127656\n",
            "Mean loss: 0.07796278431444989. Batch 100500 of 127656\n",
            "Mean loss: 0.049935893164215484. Batch 100600 of 127656\n",
            "Mean loss: 0.031639560871609546. Batch 100700 of 127656\n",
            "Mean loss: 0.0391160742765544. Batch 100800 of 127656\n",
            "Mean loss: 0.03078079536417249. Batch 100900 of 127656\n",
            "Mean loss: 0.07076483292944488. Batch 101000 of 127656\n",
            "Mean loss: 0.053357269927231527. Batch 101100 of 127656\n",
            "Mean loss: 0.05508131400810953. Batch 101200 of 127656\n",
            "Mean loss: 0.08898277837472506. Batch 101300 of 127656\n",
            "Mean loss: 0.06426102061753226. Batch 101400 of 127656\n",
            "Mean loss: 0.06126681868268406. Batch 101500 of 127656\n",
            "Mean loss: 0.0424944010040781. Batch 101600 of 127656\n",
            "Mean loss: 0.07459122538973134. Batch 101700 of 127656\n",
            "Mean loss: 0.03661969864243929. Batch 101800 of 127656\n",
            "Mean loss: 0.11066143488909802. Batch 101900 of 127656\n",
            "Mean loss: 0.03246958563709995. Batch 102000 of 127656\n",
            "Mean loss: 0.04727935043783418. Batch 102100 of 127656\n",
            "Mean loss: 0.06944630803223845. Batch 102200 of 127656\n",
            "Mean loss: 0.051001911877633574. Batch 102300 of 127656\n",
            "Mean loss: 0.05424554786910449. Batch 102400 of 127656\n",
            "Mean loss: 0.039464932233559014. Batch 102500 of 127656\n",
            "Mean loss: 0.03830583239391217. Batch 102600 of 127656\n",
            "Mean loss: 0.05588971586922867. Batch 102700 of 127656\n",
            "Mean loss: 0.04326680745673343. Batch 102800 of 127656\n",
            "Mean loss: 0.05830749891908454. Batch 102900 of 127656\n",
            "Mean loss: 0.08936986463473659. Batch 103000 of 127656\n",
            "Mean loss: 0.04428223095728299. Batch 103100 of 127656\n",
            "Mean loss: 0.06189883373154771. Batch 103200 of 127656\n",
            "Mean loss: 0.06326069673603342. Batch 103300 of 127656\n",
            "Mean loss: 0.0150222290569468. Batch 103400 of 127656\n",
            "Mean loss: 0.0842030800902853. Batch 103500 of 127656\n",
            "Mean loss: 0.06299393652057915. Batch 103600 of 127656\n",
            "Mean loss: 0.039764131700910636. Batch 103700 of 127656\n",
            "Mean loss: 0.06584245754379481. Batch 103800 of 127656\n",
            "Mean loss: 0.059071787678167315. Batch 103900 of 127656\n",
            "Mean loss: 0.05383880897153972. Batch 104000 of 127656\n",
            "Mean loss: 0.04667045673898542. Batch 104100 of 127656\n",
            "Mean loss: 0.07855942164364478. Batch 104200 of 127656\n",
            "Mean loss: 0.11745803424979841. Batch 104300 of 127656\n",
            "Mean loss: 0.050556918146631914. Batch 104400 of 127656\n",
            "Mean loss: 0.046755019841519925. Batch 104500 of 127656\n",
            "Mean loss: 0.12450688146692755. Batch 104600 of 127656\n",
            "Mean loss: 0.07040701130171442. Batch 104700 of 127656\n",
            "Mean loss: 0.05593347658841139. Batch 104800 of 127656\n",
            "Mean loss: 0.05417655514885951. Batch 104900 of 127656\n",
            "Mean loss: 0.08522063081405803. Batch 105000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.0805667467775811. Batch 105100 of 127656\n",
            "Mean loss: 0.06288071793389463. Batch 105200 of 127656\n",
            "Mean loss: 0.06721027836562371. Batch 105300 of 127656\n",
            "Mean loss: 0.05846544093375769. Batch 105400 of 127656\n",
            "Mean loss: 0.06340581059355145. Batch 105500 of 127656\n",
            "Mean loss: 0.07609212966847849. Batch 105600 of 127656\n",
            "Mean loss: 0.10379036879003564. Batch 105700 of 127656\n",
            "Mean loss: 0.08029712804275277. Batch 105800 of 127656\n",
            "Mean loss: 0.09217235686838535. Batch 105900 of 127656\n",
            "Mean loss: 0.08326038244391612. Batch 106000 of 127656\n",
            "Mean loss: 0.07776834491285627. Batch 106100 of 127656\n",
            "Mean loss: 0.024994143489718682. Batch 106200 of 127656\n",
            "Mean loss: 0.0901939485690309. Batch 106300 of 127656\n",
            "Mean loss: 0.051498368147663315. Batch 106400 of 127656\n",
            "Mean loss: 0.03761862377079979. Batch 106500 of 127656\n",
            "Mean loss: 0.056434429164838776. Batch 106600 of 127656\n",
            "Mean loss: 0.1079967070303735. Batch 106700 of 127656\n",
            "Mean loss: 0.055357101210252606. Batch 106800 of 127656\n",
            "Mean loss: 0.042173826789071424. Batch 106900 of 127656\n",
            "Mean loss: 0.07003959811930656. Batch 107000 of 127656\n",
            "Mean loss: 0.05910211100655033. Batch 107100 of 127656\n",
            "Mean loss: 0.029967577560769315. Batch 107200 of 127656\n",
            "Mean loss: 0.03552029951183954. Batch 107300 of 127656\n",
            "Mean loss: 0.042327797400059805. Batch 107400 of 127656\n",
            "Mean loss: 0.05333397370621185. Batch 107500 of 127656\n",
            "Mean loss: 0.04506562664074067. Batch 107600 of 127656\n",
            "Mean loss: 0.07807901838730032. Batch 107700 of 127656\n",
            "Mean loss: 0.05706546637579209. Batch 107800 of 127656\n",
            "Mean loss: 0.08158227569863243. Batch 107900 of 127656\n",
            "Mean loss: 0.04587789808419984. Batch 108000 of 127656\n",
            "Mean loss: 0.06001826475058806. Batch 108100 of 127656\n",
            "Mean loss: 0.04785987811569953. Batch 108200 of 127656\n",
            "Mean loss: 0.10489063424075994. Batch 108300 of 127656\n",
            "Mean loss: 0.036605059455405355. Batch 108400 of 127656\n",
            "Mean loss: 0.07573629282739944. Batch 108500 of 127656\n",
            "Mean loss: 0.03236323628994398. Batch 108600 of 127656\n",
            "Mean loss: 0.06308168420571633. Batch 108700 of 127656\n",
            "Mean loss: 0.056478352571008375. Batch 108800 of 127656\n",
            "Mean loss: 0.07165953027522619. Batch 108900 of 127656\n",
            "Mean loss: 0.022055252973720484. Batch 109000 of 127656\n",
            "Mean loss: 0.05226375673256996. Batch 109100 of 127656\n",
            "Mean loss: 0.025603824617482472. Batch 109200 of 127656\n",
            "Mean loss: 0.07551227263315013. Batch 109300 of 127656\n",
            "Mean loss: 0.1185662530757304. Batch 109400 of 127656\n",
            "Mean loss: 0.07195104935985455. Batch 109500 of 127656\n",
            "Mean loss: 0.03644817838456902. Batch 109600 of 127656\n",
            "Mean loss: 0.03955799512760677. Batch 109700 of 127656\n",
            "Mean loss: 0.0845939220557193. Batch 109800 of 127656\n",
            "Mean loss: 0.033479506426829175. Batch 109900 of 127656\n",
            "Mean loss: 0.1071871326973087. Batch 110000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.07824962079503166. Batch 110100 of 127656\n",
            "Mean loss: 0.049153571766737514. Batch 110200 of 127656\n",
            "Mean loss: 0.057221186417123135. Batch 110300 of 127656\n",
            "Mean loss: 0.04920577462995425. Batch 110400 of 127656\n",
            "Mean loss: 0.05888853863318218. Batch 110500 of 127656\n",
            "Mean loss: 0.05433007297530821. Batch 110600 of 127656\n",
            "Mean loss: 0.07462607186647346. Batch 110700 of 127656\n",
            "Mean loss: 0.09134740066138419. Batch 110800 of 127656\n",
            "Mean loss: 0.027417663666891485. Batch 110900 of 127656\n",
            "Mean loss: 0.08739530304635082. Batch 111000 of 127656\n",
            "Mean loss: 0.1029867984254679. Batch 111100 of 127656\n",
            "Mean loss: 0.04558063033908638. Batch 111200 of 127656\n",
            "Mean loss: 0.06559146845491227. Batch 111300 of 127656\n",
            "Mean loss: 0.09246891459738321. Batch 111400 of 127656\n",
            "Mean loss: 0.10156985576764327. Batch 111500 of 127656\n",
            "Mean loss: 0.04698737723066643. Batch 111600 of 127656\n",
            "Mean loss: 0.04185671851021027. Batch 111700 of 127656\n",
            "Mean loss: 0.03738582898761962. Batch 111800 of 127656\n",
            "Mean loss: 0.07977029250115493. Batch 111900 of 127656\n",
            "Mean loss: 0.0854823966191634. Batch 112000 of 127656\n",
            "Mean loss: 0.07003478102295048. Batch 112100 of 127656\n",
            "Mean loss: 0.06749826276285603. Batch 112200 of 127656\n",
            "Mean loss: 0.04739122954720642. Batch 112300 of 127656\n",
            "Mean loss: 0.14255942030440566. Batch 112400 of 127656\n",
            "Mean loss: 0.08539483698645199. Batch 112500 of 127656\n",
            "Mean loss: 0.053720944755186795. Batch 112600 of 127656\n",
            "Mean loss: 0.054684547857377766. Batch 112700 of 127656\n",
            "Mean loss: 0.08169090017224334. Batch 112800 of 127656\n",
            "Mean loss: 0.11116214384110208. Batch 112900 of 127656\n",
            "Mean loss: 0.07256883055300932. Batch 113000 of 127656\n",
            "Mean loss: 0.03010283169469403. Batch 113100 of 127656\n",
            "Mean loss: 0.056777128061394254. Batch 113200 of 127656\n",
            "Mean loss: 0.042633174556278845. Batch 113300 of 127656\n",
            "Mean loss: 0.08809912758967357. Batch 113400 of 127656\n",
            "Mean loss: 0.0786914369601459. Batch 113500 of 127656\n",
            "Mean loss: 0.056170458317719746. Batch 113600 of 127656\n",
            "Mean loss: 0.09477248102318754. Batch 113700 of 127656\n",
            "Mean loss: 0.08786138876935183. Batch 113800 of 127656\n",
            "Mean loss: 0.10382796317657267. Batch 113900 of 127656\n",
            "Mean loss: 0.05536985720371376. Batch 114000 of 127656\n",
            "Mean loss: 0.05887196042303003. Batch 114100 of 127656\n",
            "Mean loss: 0.020239970531656583. Batch 114200 of 127656\n",
            "Mean loss: 0.06366090530936618. Batch 114300 of 127656\n",
            "Mean loss: 0.04520976610809044. Batch 114400 of 127656\n",
            "Mean loss: 0.08402222188794894. Batch 114500 of 127656\n",
            "Mean loss: 0.056528516040289106. Batch 114600 of 127656\n",
            "Mean loss: 0.059153155343733485. Batch 114700 of 127656\n",
            "Mean loss: 0.05075744546463511. Batch 114800 of 127656\n",
            "Mean loss: 0.0775235996841127. Batch 114900 of 127656\n",
            "Mean loss: 0.0580027087095641. Batch 115000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.06359790532639181. Batch 115100 of 127656\n",
            "Mean loss: 0.06292802644876701. Batch 115200 of 127656\n",
            "Mean loss: 0.06518086684355012. Batch 115300 of 127656\n",
            "Mean loss: 0.09164750877321921. Batch 115400 of 127656\n",
            "Mean loss: 0.10715788883646382. Batch 115500 of 127656\n",
            "Mean loss: 0.12321744926091924. Batch 115600 of 127656\n",
            "Mean loss: 0.09850394696968579. Batch 115700 of 127656\n",
            "Mean loss: 0.09195131198347781. Batch 115800 of 127656\n",
            "Mean loss: 0.053074238046646086. Batch 115900 of 127656\n",
            "Mean loss: 0.05692976633229137. Batch 116000 of 127656\n",
            "Mean loss: 0.05947858821549403. Batch 116100 of 127656\n",
            "Mean loss: 0.08567161325841993. Batch 116200 of 127656\n",
            "Mean loss: 0.039995819101211506. Batch 116300 of 127656\n",
            "Mean loss: 0.0682960651094038. Batch 116400 of 127656\n",
            "Mean loss: 0.0626584190217227. Batch 116500 of 127656\n",
            "Mean loss: 0.05171325425229952. Batch 116600 of 127656\n",
            "Mean loss: 0.0666450165027743. Batch 116700 of 127656\n",
            "Mean loss: 0.05626657257531576. Batch 116800 of 127656\n",
            "Mean loss: 0.06124876210070738. Batch 116900 of 127656\n",
            "Mean loss: 0.05503091715698702. Batch 117000 of 127656\n",
            "Mean loss: 0.058927608430358304. Batch 117100 of 127656\n",
            "Mean loss: 0.05593369347412022. Batch 117200 of 127656\n",
            "Mean loss: 0.06285972719568918. Batch 117300 of 127656\n",
            "Mean loss: 0.08160098085017581. Batch 117400 of 127656\n",
            "Mean loss: 0.04881716455387959. Batch 117500 of 127656\n",
            "Mean loss: 0.05553234448740909. Batch 117600 of 127656\n",
            "Mean loss: 0.08460583815653991. Batch 117700 of 127656\n",
            "Mean loss: 0.0699597711861179. Batch 117800 of 127656\n",
            "Mean loss: 0.036546445393987596. Batch 117900 of 127656\n",
            "Mean loss: 0.0593583561625951. Batch 118000 of 127656\n",
            "Mean loss: 0.0922836369546809. Batch 118100 of 127656\n",
            "Mean loss: 0.06608661991043618. Batch 118200 of 127656\n",
            "Mean loss: 0.05568559708688326. Batch 118300 of 127656\n",
            "Mean loss: 0.06341333677326873. Batch 118400 of 127656\n",
            "Mean loss: 0.07310179336127703. Batch 118500 of 127656\n",
            "Mean loss: 0.10941609642217372. Batch 118600 of 127656\n",
            "Mean loss: 0.09038351536850314. Batch 118700 of 127656\n",
            "Mean loss: 0.06218536904776556. Batch 118800 of 127656\n",
            "Mean loss: 0.060690914334190896. Batch 118900 of 127656\n",
            "Mean loss: 0.08889707508345963. Batch 119000 of 127656\n",
            "Mean loss: 0.04384975830599615. Batch 119100 of 127656\n",
            "Mean loss: 0.042145543303288374. Batch 119200 of 127656\n",
            "Mean loss: 0.04825652428684634. Batch 119300 of 127656\n",
            "Mean loss: 0.06467628882526696. Batch 119400 of 127656\n",
            "Mean loss: 0.05222221827695307. Batch 119500 of 127656\n",
            "Mean loss: 0.07406824707451051. Batch 119600 of 127656\n",
            "Mean loss: 0.07201020284064726. Batch 119700 of 127656\n",
            "Mean loss: 0.07970815350015982. Batch 119800 of 127656\n",
            "Mean loss: 0.06477729296832195. Batch 119900 of 127656\n",
            "Mean loss: 0.053526950964455866. Batch 120000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.049479894557339324. Batch 120100 of 127656\n",
            "Mean loss: 0.08924167062456036. Batch 120200 of 127656\n",
            "Mean loss: 0.048076907131876394. Batch 120300 of 127656\n",
            "Mean loss: 0.06723052743908851. Batch 120400 of 127656\n",
            "Mean loss: 0.08030990752558409. Batch 120500 of 127656\n",
            "Mean loss: 0.0659922418577247. Batch 120600 of 127656\n",
            "Mean loss: 0.06829045079261732. Batch 120700 of 127656\n",
            "Mean loss: 0.07383971297756205. Batch 120800 of 127656\n",
            "Mean loss: 0.09355088639522251. Batch 120900 of 127656\n",
            "Mean loss: 0.02031839062001893. Batch 121000 of 127656\n",
            "Mean loss: 0.08174244804978344. Batch 121100 of 127656\n",
            "Mean loss: 0.05280139820792215. Batch 121200 of 127656\n",
            "Mean loss: 0.12327005904675162. Batch 121300 of 127656\n",
            "Mean loss: 0.0755531202244174. Batch 121400 of 127656\n",
            "Mean loss: 0.08421254565660093. Batch 121500 of 127656\n",
            "Mean loss: 0.086746096063099. Batch 121600 of 127656\n",
            "Mean loss: 0.07841946059148995. Batch 121700 of 127656\n",
            "Mean loss: 0.09398521148759756. Batch 121800 of 127656\n",
            "Mean loss: 0.04650834403832505. Batch 121900 of 127656\n",
            "Mean loss: 0.07808628284684574. Batch 122000 of 127656\n",
            "Mean loss: 0.11500074563119597. Batch 122100 of 127656\n",
            "Mean loss: 0.020924789015027728. Batch 122200 of 127656\n",
            "Mean loss: 0.04875203696780886. Batch 122300 of 127656\n",
            "Mean loss: 0.03183501984763666. Batch 122400 of 127656\n",
            "Mean loss: 0.05701806313047655. Batch 122500 of 127656\n",
            "Mean loss: 0.051073413021452435. Batch 122600 of 127656\n",
            "Mean loss: 0.10857759955393988. Batch 122700 of 127656\n",
            "Mean loss: 0.05501320544849534. Batch 122800 of 127656\n",
            "Mean loss: 0.05575130061773507. Batch 122900 of 127656\n",
            "Mean loss: 0.04859610693645891. Batch 123000 of 127656\n",
            "Mean loss: 0.04317740623082074. Batch 123100 of 127656\n",
            "Mean loss: 0.05688191252736601. Batch 123200 of 127656\n",
            "Mean loss: 0.1198780412019272. Batch 123300 of 127656\n",
            "Mean loss: 0.050078381818202615. Batch 123400 of 127656\n",
            "Mean loss: 0.09831041267208207. Batch 123500 of 127656\n",
            "Mean loss: 0.048515228754674525. Batch 123600 of 127656\n",
            "Mean loss: 0.08146043258493335. Batch 123700 of 127656\n",
            "Mean loss: 0.04581535279316995. Batch 123800 of 127656\n",
            "Mean loss: 0.07516076987285941. Batch 123900 of 127656\n",
            "Mean loss: 0.04183206084806443. Batch 124000 of 127656\n",
            "Mean loss: 0.07200489958186154. Batch 124100 of 127656\n",
            "Mean loss: 0.048245341893170916. Batch 124200 of 127656\n",
            "Mean loss: 0.04364703458926669. Batch 124300 of 127656\n",
            "Mean loss: 0.03999351783344537. Batch 124400 of 127656\n",
            "Mean loss: 0.053298161919070705. Batch 124500 of 127656\n",
            "Mean loss: 0.08859735731066394. Batch 124600 of 127656\n",
            "Mean loss: 0.04566316388074483. Batch 124700 of 127656\n",
            "Mean loss: 0.061638718747399256. Batch 124800 of 127656\n",
            "Mean loss: 0.059760486373397724. Batch 124900 of 127656\n",
            "Mean loss: 0.06891686408300302. Batch 125000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.09932893798803889. Batch 125100 of 127656\n",
            "Mean loss: 0.04478713117503503. Batch 125200 of 127656\n",
            "Mean loss: 0.025106400262138777. Batch 125300 of 127656\n",
            "Mean loss: 0.056711735132721514. Batch 125400 of 127656\n",
            "Mean loss: 0.1069276435883512. Batch 125500 of 127656\n",
            "Mean loss: 0.07697458786591596. Batch 125600 of 127656\n",
            "Mean loss: 0.048299513979786754. Batch 125700 of 127656\n",
            "Mean loss: 0.10758431238163439. Batch 125800 of 127656\n",
            "Mean loss: 0.04641112392896503. Batch 125900 of 127656\n",
            "Mean loss: 0.05011431917097866. Batch 126000 of 127656\n",
            "Mean loss: 0.036968832401323655. Batch 126100 of 127656\n",
            "Mean loss: 0.08108637655938765. Batch 126200 of 127656\n",
            "Mean loss: 0.05268278701750688. Batch 126300 of 127656\n",
            "Mean loss: 0.0543544923315298. Batch 126400 of 127656\n",
            "Mean loss: 0.05525003437757846. Batch 126500 of 127656\n",
            "Mean loss: 0.07981909132532593. Batch 126600 of 127656\n",
            "Mean loss: 0.0736552046263435. Batch 126700 of 127656\n",
            "Mean loss: 0.07247256774473158. Batch 126800 of 127656\n",
            "Mean loss: 0.03957659139109296. Batch 126900 of 127656\n",
            "Mean loss: 0.07158935635706086. Batch 127000 of 127656\n",
            "Mean loss: 0.057968418101070826. Batch 127100 of 127656\n",
            "Mean loss: 0.06394618213567525. Batch 127200 of 127656\n",
            "Mean loss: 0.09616891127775944. Batch 127300 of 127656\n",
            "Mean loss: 0.07198386908611383. Batch 127400 of 127656\n",
            "Mean loss: 0.05466643670262215. Batch 127500 of 127656\n",
            "Mean loss: 0.06376989771930738. Batch 127600 of 127656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have fine-tuned our model, we will verify that it now produces a resonable prediction on the toxic comment classification example that we tested with the pre-trained model. Note that we have modified the prefix applied to the input to match the prefix we used when training."
      ],
      "metadata": {
        "id": "xsdfHJdAt2Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")\n",
        "\n",
        "input_ids = tokenizer(\n",
        "    \"toxic comment classification: You, sir, are my hero. Any chance you remember what page that's on?\",\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids\n",
        "output = model.generate(input_ids=input_ids.to(\"cuda\"))\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq1rR9dI5XUx",
        "outputId": "b85782c3-e834-4106-bd39-9ad3daea10db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> not_toxic</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Now we will evaluate accuracy metrics on the test set. Since this is a multi-label classification problem, we will compute precision, recall and F1-scores on each class individually.\n",
        "\n",
        "Recall that our model outputs text strings containing all relevant predicted labels. To enable easy evaluation of metrics, we will generate predictions and labels on our test set, then convert the outputs into a dataframe that contains one row for each test sample and columns for each of the  predicted and labeled classes."
      ],
      "metadata": {
        "id": "dxs_Suoher8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model from the saved checkpoint if needed\n",
        "# model = T5ForConditionalGeneration.from_pretrained(\"/content/gdrive/MyDrive/T5/t5_checkpoint/\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "OyjvFSHTg5C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data as an instance of our Dataset class\n",
        "test_dataset = ToxicityDataset(\"/content/gdrive/MyDrive/T5/assignment_test.csv\", tokenizer)"
      ],
      "metadata": {
        "id": "y1LUWJZwfOXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(testing_dataset, tokenizer, model):\n",
        "    \"\"\"\n",
        "    This function will be used to generate predictions on the\n",
        "    test dataset.\n",
        "    Inputs:\n",
        "    - testing_dataset: The ToxicityDataset instance containing samples\n",
        "        to evaluate.\n",
        "    - tokenizer: The T5 tokenizer.\n",
        "    - model: The fine-tuned model that we want to evaluate.\n",
        "    This function returns a DataFrame containing predictions in a\n",
        "    format that enables easy computation of accuracy metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a DataLoader to iterate over our dataset\n",
        "    collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "    testing_loader = torch.utils.data.DataLoader(\n",
        "        testing_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        collate_fn=collator\n",
        "    )\n",
        "\n",
        "    # These are the column names that will be added to the output\n",
        "    # DataFrame. This contains columns for each predicted and labeled\n",
        "    # toxicity class.\n",
        "    column_names = [\n",
        "        \"pred_not_toxic\",\n",
        "        \"pred_toxic\",\n",
        "        \"pred_severe_toxic\",\n",
        "        \"pred_obscene\",\n",
        "        \"pred_threat\",\n",
        "        \"pred_insult\",\n",
        "        \"pred_identity_hate\",\n",
        "        \"label_not_toxic\",\n",
        "        \"label_toxic\",\n",
        "        \"label_severe_toxic\",\n",
        "        \"label_obscene\",\n",
        "        \"label_threat\",\n",
        "        \"label_insult\",\n",
        "        \"label_identity_hate\"\n",
        "    ]\n",
        "\n",
        "    # Loop over all batches and generate labels\n",
        "    n_batches = testing_dataset.__len__()\n",
        "    results = []\n",
        "    current_batch = 0\n",
        "    for batch in testing_loader:\n",
        "        current_batch += 1\n",
        "\n",
        "        # Send the current batch to the GPU\n",
        "        batch = batch.to(\"cuda\")\n",
        "        # Generate the output string for the given input\n",
        "        output = model.generate(input_ids=batch[\"input_ids\"])\n",
        "        # Extract the predictions from the generated output\n",
        "        # Here we strip away the <pad> and </s> tokens added to each output\n",
        "        predictions = tokenizer.decode(output[0][1:-1]).strip().split(\" \")\n",
        "        # Decode the ground-truth labels associated with this sample\n",
        "        labels = tokenizer.decode(batch[\"labels\"][0][:-1]).strip().split(\" \")\n",
        "\n",
        "        # Build a dictionary containing the comment text and indicators\n",
        "        # for each predicted and labeled toxicity class. These will be\n",
        "        # The rows of the dataframe that we output.\n",
        "        row = {c:0 for c in column_names}\n",
        "        row[\"comment_text\"] = tokenizer.decode(batch[\"input_ids\"][0])\n",
        "        for p in predictions:\n",
        "            row[f\"pred_{p}\"] = 1\n",
        "        for l in labels:\n",
        "            row[f\"label_{l}\"] = 1\n",
        "        results.append(row)\n",
        "\n",
        "        # Every 100 batches provide a status update\n",
        "        if current_batch%100 == 0:\n",
        "            status = \"Batch {} of {}\".format(\n",
        "                current_batch,\n",
        "                n_batches\n",
        "            )\n",
        "            print(status)\n",
        "\n",
        "    # Return a dataframe containing all predictions and labels\n",
        "    return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "taCAfVlufMCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run predictions on the test dataset\n",
        "df_pred = generate_predictions(test_dataset, tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki1xyQJHgjyD",
        "outputId": "3ece2fb0-4317-4f2f-8b33-65a78d7f962b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 of 31915\n",
            "Batch 200 of 31915\n",
            "Batch 300 of 31915\n",
            "Batch 400 of 31915\n",
            "Batch 500 of 31915\n",
            "Batch 600 of 31915\n",
            "Batch 700 of 31915\n",
            "Batch 800 of 31915\n",
            "Batch 900 of 31915\n",
            "Batch 1000 of 31915\n",
            "Batch 1100 of 31915\n",
            "Batch 1200 of 31915\n",
            "Batch 1300 of 31915\n",
            "Batch 1400 of 31915\n",
            "Batch 1500 of 31915\n",
            "Batch 1600 of 31915\n",
            "Batch 1700 of 31915\n",
            "Batch 1800 of 31915\n",
            "Batch 1900 of 31915\n",
            "Batch 2000 of 31915\n",
            "Batch 2100 of 31915\n",
            "Batch 2200 of 31915\n",
            "Batch 2300 of 31915\n",
            "Batch 2400 of 31915\n",
            "Batch 2500 of 31915\n",
            "Batch 2600 of 31915\n",
            "Batch 2700 of 31915\n",
            "Batch 2800 of 31915\n",
            "Batch 2900 of 31915\n",
            "Batch 3000 of 31915\n",
            "Batch 3100 of 31915\n",
            "Batch 3200 of 31915\n",
            "Batch 3300 of 31915\n",
            "Batch 3400 of 31915\n",
            "Batch 3500 of 31915\n",
            "Batch 3600 of 31915\n",
            "Batch 3700 of 31915\n",
            "Batch 3800 of 31915\n",
            "Batch 3900 of 31915\n",
            "Batch 4000 of 31915\n",
            "Batch 4100 of 31915\n",
            "Batch 4200 of 31915\n",
            "Batch 4300 of 31915\n",
            "Batch 4400 of 31915\n",
            "Batch 4500 of 31915\n",
            "Batch 4600 of 31915\n",
            "Batch 4700 of 31915\n",
            "Batch 4800 of 31915\n",
            "Batch 4900 of 31915\n",
            "Batch 5000 of 31915\n",
            "Batch 5100 of 31915\n",
            "Batch 5200 of 31915\n",
            "Batch 5300 of 31915\n",
            "Batch 5400 of 31915\n",
            "Batch 5500 of 31915\n",
            "Batch 5600 of 31915\n",
            "Batch 5700 of 31915\n",
            "Batch 5800 of 31915\n",
            "Batch 5900 of 31915\n",
            "Batch 6000 of 31915\n",
            "Batch 6100 of 31915\n",
            "Batch 6200 of 31915\n",
            "Batch 6300 of 31915\n",
            "Batch 6400 of 31915\n",
            "Batch 6500 of 31915\n",
            "Batch 6600 of 31915\n",
            "Batch 6700 of 31915\n",
            "Batch 6800 of 31915\n",
            "Batch 6900 of 31915\n",
            "Batch 7000 of 31915\n",
            "Batch 7100 of 31915\n",
            "Batch 7200 of 31915\n",
            "Batch 7300 of 31915\n",
            "Batch 7400 of 31915\n",
            "Batch 7500 of 31915\n",
            "Batch 7600 of 31915\n",
            "Batch 7700 of 31915\n",
            "Batch 7800 of 31915\n",
            "Batch 7900 of 31915\n",
            "Batch 8000 of 31915\n",
            "Batch 8100 of 31915\n",
            "Batch 8200 of 31915\n",
            "Batch 8300 of 31915\n",
            "Batch 8400 of 31915\n",
            "Batch 8500 of 31915\n",
            "Batch 8600 of 31915\n",
            "Batch 8700 of 31915\n",
            "Batch 8800 of 31915\n",
            "Batch 8900 of 31915\n",
            "Batch 9000 of 31915\n",
            "Batch 9100 of 31915\n",
            "Batch 9200 of 31915\n",
            "Batch 9300 of 31915\n",
            "Batch 9400 of 31915\n",
            "Batch 9500 of 31915\n",
            "Batch 9600 of 31915\n",
            "Batch 9700 of 31915\n",
            "Batch 9800 of 31915\n",
            "Batch 9900 of 31915\n",
            "Batch 10000 of 31915\n",
            "Batch 10100 of 31915\n",
            "Batch 10200 of 31915\n",
            "Batch 10300 of 31915\n",
            "Batch 10400 of 31915\n",
            "Batch 10500 of 31915\n",
            "Batch 10600 of 31915\n",
            "Batch 10700 of 31915\n",
            "Batch 10800 of 31915\n",
            "Batch 10900 of 31915\n",
            "Batch 11000 of 31915\n",
            "Batch 11100 of 31915\n",
            "Batch 11200 of 31915\n",
            "Batch 11300 of 31915\n",
            "Batch 11400 of 31915\n",
            "Batch 11500 of 31915\n",
            "Batch 11600 of 31915\n",
            "Batch 11700 of 31915\n",
            "Batch 11800 of 31915\n",
            "Batch 11900 of 31915\n",
            "Batch 12000 of 31915\n",
            "Batch 12100 of 31915\n",
            "Batch 12200 of 31915\n",
            "Batch 12300 of 31915\n",
            "Batch 12400 of 31915\n",
            "Batch 12500 of 31915\n",
            "Batch 12600 of 31915\n",
            "Batch 12700 of 31915\n",
            "Batch 12800 of 31915\n",
            "Batch 12900 of 31915\n",
            "Batch 13000 of 31915\n",
            "Batch 13100 of 31915\n",
            "Batch 13200 of 31915\n",
            "Batch 13300 of 31915\n",
            "Batch 13400 of 31915\n",
            "Batch 13500 of 31915\n",
            "Batch 13600 of 31915\n",
            "Batch 13700 of 31915\n",
            "Batch 13800 of 31915\n",
            "Batch 13900 of 31915\n",
            "Batch 14000 of 31915\n",
            "Batch 14100 of 31915\n",
            "Batch 14200 of 31915\n",
            "Batch 14300 of 31915\n",
            "Batch 14400 of 31915\n",
            "Batch 14500 of 31915\n",
            "Batch 14600 of 31915\n",
            "Batch 14700 of 31915\n",
            "Batch 14800 of 31915\n",
            "Batch 14900 of 31915\n",
            "Batch 15000 of 31915\n",
            "Batch 15100 of 31915\n",
            "Batch 15200 of 31915\n",
            "Batch 15300 of 31915\n",
            "Batch 15400 of 31915\n",
            "Batch 15500 of 31915\n",
            "Batch 15600 of 31915\n",
            "Batch 15700 of 31915\n",
            "Batch 15800 of 31915\n",
            "Batch 15900 of 31915\n",
            "Batch 16000 of 31915\n",
            "Batch 16100 of 31915\n",
            "Batch 16200 of 31915\n",
            "Batch 16300 of 31915\n",
            "Batch 16400 of 31915\n",
            "Batch 16500 of 31915\n",
            "Batch 16600 of 31915\n",
            "Batch 16700 of 31915\n",
            "Batch 16800 of 31915\n",
            "Batch 16900 of 31915\n",
            "Batch 17000 of 31915\n",
            "Batch 17100 of 31915\n",
            "Batch 17200 of 31915\n",
            "Batch 17300 of 31915\n",
            "Batch 17400 of 31915\n",
            "Batch 17500 of 31915\n",
            "Batch 17600 of 31915\n",
            "Batch 17700 of 31915\n",
            "Batch 17800 of 31915\n",
            "Batch 17900 of 31915\n",
            "Batch 18000 of 31915\n",
            "Batch 18100 of 31915\n",
            "Batch 18200 of 31915\n",
            "Batch 18300 of 31915\n",
            "Batch 18400 of 31915\n",
            "Batch 18500 of 31915\n",
            "Batch 18600 of 31915\n",
            "Batch 18700 of 31915\n",
            "Batch 18800 of 31915\n",
            "Batch 18900 of 31915\n",
            "Batch 19000 of 31915\n",
            "Batch 19100 of 31915\n",
            "Batch 19200 of 31915\n",
            "Batch 19300 of 31915\n",
            "Batch 19400 of 31915\n",
            "Batch 19500 of 31915\n",
            "Batch 19600 of 31915\n",
            "Batch 19700 of 31915\n",
            "Batch 19800 of 31915\n",
            "Batch 19900 of 31915\n",
            "Batch 20000 of 31915\n",
            "Batch 20100 of 31915\n",
            "Batch 20200 of 31915\n",
            "Batch 20300 of 31915\n",
            "Batch 20400 of 31915\n",
            "Batch 20500 of 31915\n",
            "Batch 20600 of 31915\n",
            "Batch 20700 of 31915\n",
            "Batch 20800 of 31915\n",
            "Batch 20900 of 31915\n",
            "Batch 21000 of 31915\n",
            "Batch 21100 of 31915\n",
            "Batch 21200 of 31915\n",
            "Batch 21300 of 31915\n",
            "Batch 21400 of 31915\n",
            "Batch 21500 of 31915\n",
            "Batch 21600 of 31915\n",
            "Batch 21700 of 31915\n",
            "Batch 21800 of 31915\n",
            "Batch 21900 of 31915\n",
            "Batch 22000 of 31915\n",
            "Batch 22100 of 31915\n",
            "Batch 22200 of 31915\n",
            "Batch 22300 of 31915\n",
            "Batch 22400 of 31915\n",
            "Batch 22500 of 31915\n",
            "Batch 22600 of 31915\n",
            "Batch 22700 of 31915\n",
            "Batch 22800 of 31915\n",
            "Batch 22900 of 31915\n",
            "Batch 23000 of 31915\n",
            "Batch 23100 of 31915\n",
            "Batch 23200 of 31915\n",
            "Batch 23300 of 31915\n",
            "Batch 23400 of 31915\n",
            "Batch 23500 of 31915\n",
            "Batch 23600 of 31915\n",
            "Batch 23700 of 31915\n",
            "Batch 23800 of 31915\n",
            "Batch 23900 of 31915\n",
            "Batch 24000 of 31915\n",
            "Batch 24100 of 31915\n",
            "Batch 24200 of 31915\n",
            "Batch 24300 of 31915\n",
            "Batch 24400 of 31915\n",
            "Batch 24500 of 31915\n",
            "Batch 24600 of 31915\n",
            "Batch 24700 of 31915\n",
            "Batch 24800 of 31915\n",
            "Batch 24900 of 31915\n",
            "Batch 25000 of 31915\n",
            "Batch 25100 of 31915\n",
            "Batch 25200 of 31915\n",
            "Batch 25300 of 31915\n",
            "Batch 25400 of 31915\n",
            "Batch 25500 of 31915\n",
            "Batch 25600 of 31915\n",
            "Batch 25700 of 31915\n",
            "Batch 25800 of 31915\n",
            "Batch 25900 of 31915\n",
            "Batch 26000 of 31915\n",
            "Batch 26100 of 31915\n",
            "Batch 26200 of 31915\n",
            "Batch 26300 of 31915\n",
            "Batch 26400 of 31915\n",
            "Batch 26500 of 31915\n",
            "Batch 26600 of 31915\n",
            "Batch 26700 of 31915\n",
            "Batch 26800 of 31915\n",
            "Batch 26900 of 31915\n",
            "Batch 27000 of 31915\n",
            "Batch 27100 of 31915\n",
            "Batch 27200 of 31915\n",
            "Batch 27300 of 31915\n",
            "Batch 27400 of 31915\n",
            "Batch 27500 of 31915\n",
            "Batch 27600 of 31915\n",
            "Batch 27700 of 31915\n",
            "Batch 27800 of 31915\n",
            "Batch 27900 of 31915\n",
            "Batch 28000 of 31915\n",
            "Batch 28100 of 31915\n",
            "Batch 28200 of 31915\n",
            "Batch 28300 of 31915\n",
            "Batch 28400 of 31915\n",
            "Batch 28500 of 31915\n",
            "Batch 28600 of 31915\n",
            "Batch 28700 of 31915\n",
            "Batch 28800 of 31915\n",
            "Batch 28900 of 31915\n",
            "Batch 29000 of 31915\n",
            "Batch 29100 of 31915\n",
            "Batch 29200 of 31915\n",
            "Batch 29300 of 31915\n",
            "Batch 29400 of 31915\n",
            "Batch 29500 of 31915\n",
            "Batch 29600 of 31915\n",
            "Batch 29700 of 31915\n",
            "Batch 29800 of 31915\n",
            "Batch 29900 of 31915\n",
            "Batch 30000 of 31915\n",
            "Batch 30100 of 31915\n",
            "Batch 30200 of 31915\n",
            "Batch 30300 of 31915\n",
            "Batch 30400 of 31915\n",
            "Batch 30500 of 31915\n",
            "Batch 30600 of 31915\n",
            "Batch 30700 of 31915\n",
            "Batch 30800 of 31915\n",
            "Batch 30900 of 31915\n",
            "Batch 31000 of 31915\n",
            "Batch 31100 of 31915\n",
            "Batch 31200 of 31915\n",
            "Batch 31300 of 31915\n",
            "Batch 31400 of 31915\n",
            "Batch 31500 of 31915\n",
            "Batch 31600 of 31915\n",
            "Batch 31700 of 31915\n",
            "Batch 31800 of 31915\n",
            "Batch 31900 of 31915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our predictions to Google Drive\n",
        "df_pred.to_csv(\"/content/gdrive/MyDrive/T5/assignment_test_predictions.csv\", index=None)"
      ],
      "metadata": {
        "id": "IvqrZltMgpkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here we will compute accuracy metrics on each toxicity class.\n",
        "Specifically we will compute:\n",
        "- Precision: Among all positive predictions, the fraction that were\n",
        "    correctly predicted.\n",
        "- Recall: Among all positives in the groud-truth, the fraction that were\n",
        "    correctly predicted.\n",
        "- F1-score: Harmonic mean of precision and recall.\n",
        "- Support: The number of positive instances in the ground-truth.\n",
        "- Rate: The fraction of positives in the ground truth.\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Categories that we will report metrics for\n",
        "categories = [\n",
        "    \"toxic\",\n",
        "    \"severe_toxic\",\n",
        "    \"obscene\",\n",
        "    \"threat\",\n",
        "    \"insult\",\n",
        "    \"identity_hate\"\n",
        "]\n",
        "\n",
        "# Loop over categories and print metrics\n",
        "for c in categories:\n",
        "  metrics = precision_recall_fscore_support(\n",
        "      df_pred[f\"label_{c}\"],\n",
        "      df_pred[f\"pred_{c}\"]\n",
        "  )\n",
        "  print(c)\n",
        "  print(f\"Precision: {metrics[0][1].round(3)}\")\n",
        "  print(f\"Recall: {metrics[1][1].round(3)}\")\n",
        "  print(f\"F1-score: {metrics[2][1].round(3)}\")\n",
        "  print(f\"Support: {metrics[3][1]}\")\n",
        "  label_category = f\"label_{c}\"\n",
        "  print(f\"Rate: {df_pred[label_category].mean().round(3)}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqK8LPZf1O7D",
        "outputId": "c85d430f-4f5e-453a-fadb-81ee72d4b098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxic\n",
            "Precision: 0.837\n",
            "Recall: 0.802\n",
            "F1-score: 0.819\n",
            "Support: 3079\n",
            "Rate: 0.096\n",
            "\n",
            "severe_toxic\n",
            "Precision: 0.938\n",
            "Recall: 0.044\n",
            "F1-score: 0.084\n",
            "Support: 343\n",
            "Rate: 0.011\n",
            "\n",
            "obscene\n",
            "Precision: 0.959\n",
            "Recall: 0.479\n",
            "F1-score: 0.638\n",
            "Support: 1701\n",
            "Rate: 0.053\n",
            "\n",
            "threat\n",
            "Precision: 0.692\n",
            "Recall: 0.189\n",
            "F1-score: 0.298\n",
            "Support: 95\n",
            "Rate: 0.003\n",
            "\n",
            "insult\n",
            "Precision: 0.875\n",
            "Recall: 0.432\n",
            "F1-score: 0.579\n",
            "Support: 1557\n",
            "Rate: 0.049\n",
            "\n",
            "identity_hate\n",
            "Precision: 0.671\n",
            "Recall: 0.221\n",
            "F1-score: 0.332\n",
            "Support: 249\n",
            "Rate: 0.008\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We achieve a reasonably good F1-score of 82% on the \"toxic\" class, with well-balanced precision and recall. For other classes, the F1-scores appear to depend on the rate of positive labels associated with the class. This is expected, since rare classes will generally be the most difficult to predict accurately. The categories \"severe_toxic\" and \"threat\" occur very infrequently and have the lowest F1-scores (only 1.1% and 0.3% of samples in the test set have these labels, respectively). F1-scores increase with in line with increasing frequency of occurence on the remaining categories."
      ],
      "metadata": {
        "id": "hIVriA0SnDut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Cross-Dataset Evaluation\n",
        "\n",
        "Now we will check [the different dataset](https://huggingface.co/datasets/OxAISH-AL-LLM/wiki_toxic) referenced in the assignment to measure our model's generalization capabilities."
      ],
      "metadata": {
        "id": "c2ufJe_5SA-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We uploaded this dataset to Google Drive. Load from there.\n",
        "df_wiki_toxic = pd.read_csv(\"/content/gdrive/MyDrive/T5/wiki_toxic_test.csv\")"
      ],
      "metadata": {
        "id": "tu2Dt2llSNC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first few rows\n",
        "df_wiki_toxic.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kTxeEZVhSaNN",
        "outputId": "a5d7aab1-3f53-43fa-8120-db25259ea55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 id                                       comment_text  label\n",
              "0  0001ea8717f6de06  Thank you for understanding. I think very high...      0\n",
              "1  000247e83dcc1211                   :Dear god this site is horrible.      0\n",
              "2  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0\n",
              "3  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0\n",
              "4  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a1011d9a-1d7e-4f5f-999e-6e769b7798c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>Thank you for understanding. I think very high...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1011d9a-1d7e-4f5f-999e-6e769b7798c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a1011d9a-1d7e-4f5f-999e-6e769b7798c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a1011d9a-1d7e-4f5f-999e-6e769b7798c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bd211a49-c808-48a4-8fe2-9354549d14a5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd211a49-c808-48a4-8fe2-9354549d14a5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bd211a49-c808-48a4-8fe2-9354549d14a5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the schema of this new dataset is a bit different than the toxic comment dataset that we worked with previously. Specifically, there is now only a single ground truth label indicating whether or not the comment is toxic. We will convert this table into a format compatible with our previous dataset so that we can use our existing code to produce predictions. Specifically, we will treat this \"label\" column as the \"toxic\" column in our previous dataset, then add dummy columns for all of the other toxic comment classes."
      ],
      "metadata": {
        "id": "C9fu5ekiUTgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the \"label\" column\n",
        "df_wiki_toxic = df_wiki_toxic.rename(columns={\"label\": \"toxic\"})\n",
        "# Add dummy columns for all other toxic classes\n",
        "df_wiki_toxic[\"severe_toxic\"] = 0\n",
        "df_wiki_toxic[\"obscene\"] = 0\n",
        "df_wiki_toxic[\"threat\"] = 0\n",
        "df_wiki_toxic[\"insult\"] = 0\n",
        "df_wiki_toxic[\"identity_hate\"] = 0"
      ],
      "metadata": {
        "id": "8O6eq2wySi8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will just run inference on a sample, since the original dataset is quite large\n",
        "# Save this sample to Google Drive so that we can load it as a ToxicityDataset\n",
        "df_wiki_toxic.sample(10000).to_csv(\"/content/gdrive/MyDrive/T5/wiki_toxic_test_new_schema.csv\", index=False)"
      ],
      "metadata": {
        "id": "_tqaKxXuS34N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset for testing\n",
        "test_dataset = ToxicityDataset(\"/content/gdrive/MyDrive/T5/wiki_toxic_test_new_schema.csv\", tokenizer)"
      ],
      "metadata": {
        "id": "kTueaswcTxPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "df_pred_wiki_toxic = generate_predictions(test_dataset, tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKfjgQaQTj3x",
        "outputId": "f2fcc093-682b-4cb4-b6ce-be745ed58ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 of 10000\n",
            "Batch 200 of 10000\n",
            "Batch 300 of 10000\n",
            "Batch 400 of 10000\n",
            "Batch 500 of 10000\n",
            "Batch 600 of 10000\n",
            "Batch 700 of 10000\n",
            "Batch 800 of 10000\n",
            "Batch 900 of 10000\n",
            "Batch 1000 of 10000\n",
            "Batch 1100 of 10000\n",
            "Batch 1200 of 10000\n",
            "Batch 1300 of 10000\n",
            "Batch 1400 of 10000\n",
            "Batch 1500 of 10000\n",
            "Batch 1600 of 10000\n",
            "Batch 1700 of 10000\n",
            "Batch 1800 of 10000\n",
            "Batch 1900 of 10000\n",
            "Batch 2000 of 10000\n",
            "Batch 2100 of 10000\n",
            "Batch 2200 of 10000\n",
            "Batch 2300 of 10000\n",
            "Batch 2400 of 10000\n",
            "Batch 2500 of 10000\n",
            "Batch 2600 of 10000\n",
            "Batch 2700 of 10000\n",
            "Batch 2800 of 10000\n",
            "Batch 2900 of 10000\n",
            "Batch 3000 of 10000\n",
            "Batch 3100 of 10000\n",
            "Batch 3200 of 10000\n",
            "Batch 3300 of 10000\n",
            "Batch 3400 of 10000\n",
            "Batch 3500 of 10000\n",
            "Batch 3600 of 10000\n",
            "Batch 3700 of 10000\n",
            "Batch 3800 of 10000\n",
            "Batch 3900 of 10000\n",
            "Batch 4000 of 10000\n",
            "Batch 4100 of 10000\n",
            "Batch 4200 of 10000\n",
            "Batch 4300 of 10000\n",
            "Batch 4400 of 10000\n",
            "Batch 4500 of 10000\n",
            "Batch 4600 of 10000\n",
            "Batch 4700 of 10000\n",
            "Batch 4800 of 10000\n",
            "Batch 4900 of 10000\n",
            "Batch 5000 of 10000\n",
            "Batch 5100 of 10000\n",
            "Batch 5200 of 10000\n",
            "Batch 5300 of 10000\n",
            "Batch 5400 of 10000\n",
            "Batch 5500 of 10000\n",
            "Batch 5600 of 10000\n",
            "Batch 5700 of 10000\n",
            "Batch 5800 of 10000\n",
            "Batch 5900 of 10000\n",
            "Batch 6000 of 10000\n",
            "Batch 6100 of 10000\n",
            "Batch 6200 of 10000\n",
            "Batch 6300 of 10000\n",
            "Batch 6400 of 10000\n",
            "Batch 6500 of 10000\n",
            "Batch 6600 of 10000\n",
            "Batch 6700 of 10000\n",
            "Batch 6800 of 10000\n",
            "Batch 6900 of 10000\n",
            "Batch 7000 of 10000\n",
            "Batch 7100 of 10000\n",
            "Batch 7200 of 10000\n",
            "Batch 7300 of 10000\n",
            "Batch 7400 of 10000\n",
            "Batch 7500 of 10000\n",
            "Batch 7600 of 10000\n",
            "Batch 7700 of 10000\n",
            "Batch 7800 of 10000\n",
            "Batch 7900 of 10000\n",
            "Batch 8000 of 10000\n",
            "Batch 8100 of 10000\n",
            "Batch 8200 of 10000\n",
            "Batch 8300 of 10000\n",
            "Batch 8400 of 10000\n",
            "Batch 8500 of 10000\n",
            "Batch 8600 of 10000\n",
            "Batch 8700 of 10000\n",
            "Batch 8800 of 10000\n",
            "Batch 8900 of 10000\n",
            "Batch 9000 of 10000\n",
            "Batch 9100 of 10000\n",
            "Batch 9200 of 10000\n",
            "Batch 9300 of 10000\n",
            "Batch 9400 of 10000\n",
            "Batch 9500 of 10000\n",
            "Batch 9600 of 10000\n",
            "Batch 9700 of 10000\n",
            "Batch 9800 of 10000\n",
            "Batch 9900 of 10000\n",
            "Batch 10000 of 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to Google Drive\n",
        "df_pred_wiki_toxic.to_csv(\"/content/gdrive/MyDrive/T5/wiki_toxic_test_predictions.csv\", index=None)"
      ],
      "metadata": {
        "id": "wszh01eRVUGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will evaluate the performance of our classifier on this dataset. We will only consider the \"toxic\" category since this is the only label contained in this dataset.\n"
      ],
      "metadata": {
        "id": "3fPRx-SxVjef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categories that we will report metrics for\n",
        "categories = [\"toxic\"]\n",
        "\n",
        "# Loop over categories and print metrics\n",
        "for c in categories:\n",
        "  metrics = precision_recall_fscore_support(\n",
        "      df_pred_wiki_toxic[f\"label_{c}\"],\n",
        "      df_pred_wiki_toxic[f\"pred_{c}\"]\n",
        "  )\n",
        "  print(c)\n",
        "  print(f\"Precision: {metrics[0][1].round(3)}\")\n",
        "  print(f\"Recall: {metrics[1][1].round(3)}\")\n",
        "  print(f\"F1-score: {metrics[2][1].round(3)}\")\n",
        "  print(f\"Support: {metrics[3][1]}\")\n",
        "  label_category = f\"label_{c}\"\n",
        "  print(f\"Rate: {df_pred_wiki_toxic[label_category].mean().round(3)}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbxSaJSxVerg",
        "outputId": "ea7c0dff-8c0a-49db-e076-ebde5cf18e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxic\n",
            "Precision: 0.569\n",
            "Recall: 0.827\n",
            "F1-score: 0.674\n",
            "Support: 973\n",
            "Rate: 0.097\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall F1-score is lower on this dataset, which is reasonable. It is possible that toxicity was evaluated according to somewhat different conventions in this dataset compared to the dataset that we trained on. Specifically, here we see that recall is still high but precision is now lower. This suggests that this dataset might label fewer comments as toxic than in the dataset that we trained on. That is, when our model predicts that a comment is toxic based on its training, that comment is less likely to be labeled as toxic in this dataset than in our previous test set. However, our model is still capable of accurately predicting comments that were labeled as toxic in this dataset."
      ],
      "metadata": {
        "id": "LMeVmmNjgFQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Training a Model from Scratch\n",
        "\n",
        "Now we will repeat the process of training and evaluating a model, but we will do it this time with no pre-training. To re-create the process of training a model from scratch, we will load the T5-small model then reinitialize its weights to random values. This will allow us to train a model with the same architecture as T5, but will undo the effects of any pre-training already performed."
      ],
      "metadata": {
        "id": "KbXggJmHUZww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the T5-small model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "A0C_U3mDT01z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_parameters(model):\n",
        "    \"\"\"\n",
        "    This function will recursively move through a model's\n",
        "    layers and reset the weights on any that can be reset.\n",
        "    \"\"\"\n",
        "    for layer in model.children():\n",
        "        if hasattr(layer, 'reset_parameters'):\n",
        "            layer.reset_parameters()\n",
        "        else:\n",
        "            reset_parameters(layer)"
      ],
      "metadata": {
        "id": "Hf3SgAjqcOeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the parameters of the entire T5 model\n",
        "reset_parameters(model)"
      ],
      "metadata": {
        "id": "jpPISR6DWo3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to the GPU\n",
        "model = model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "5sUs1GQ5WyS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's verify that the model now produces random output on a sentiment classification input that the pretrained model could handle sensibly."
      ],
      "metadata": {
        "id": "61jjP2srfMjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate a previous sentiment classification example\n",
        "input_ids = tokenizer(\n",
        "    \"sst2 sentence: The movie was too long to be interesting.\",\n",
        "    return_tensors=\"pt\"\n",
        ").input_ids\n",
        "output = model.generate(input_ids=input_ids.to(\"cuda\"))\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_uM5CftcWP2",
        "outputId": "5282fbc1-764d-457b-f404-a662f35e551b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad>happiness transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér transfér\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a dataset for our training data\n",
        "dataset = ToxicityDataset(\"/content/gdrive/MyDrive/T5/assignment_train.csv\", tokenizer)\n",
        "\n",
        "# Create a DataLoader for the training data\n",
        "collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "training_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    collate_fn=collator\n",
        ")"
      ],
      "metadata": {
        "id": "raYe2FDad8Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the optimizer that we will use for training the model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "ztfSRDWOgzS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here we implement the training loop for training our model.\n",
        "This is essentially equivalent to the training loop we used\n",
        "when fine-tuning the model. The only difference is that the model's\n",
        "initial weights are random, rather than the pre-trained T5 weights.\n",
        "\"\"\"\n",
        "\n",
        "# Get the number of batches, which we will print in status messages\n",
        "n_batches = dataset.__len__()\n",
        "\n",
        "# Loop over batches\n",
        "current_batch = 0\n",
        "total_loss = 0\n",
        "for batch in training_loader:\n",
        "\n",
        "    # Move the current batch to the GPU\n",
        "    batch = batch.to(\"cuda\")\n",
        "    # Evaluate the loss on the current batch and perform\n",
        "    # a single step of parameter optimization.\n",
        "    output = model(**batch)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    output.loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Give a status update every 100 iterations\n",
        "    total_loss += output.loss.item()\n",
        "    current_batch += 1\n",
        "    if current_batch%100 == 0:\n",
        "        status = \"Mean loss: {}. Batch {} of {}\".format(\n",
        "            total_loss/100,\n",
        "            current_batch,\n",
        "            n_batches\n",
        "        )\n",
        "        print(status)\n",
        "        total_loss = 0\n",
        "\n",
        "    # Save a checkpoint every 5000 iterations\n",
        "    if current_batch%5000 == 0:\n",
        "        model.save_pretrained(\"/content/gdrive/MyDrive/T5/t5_no_pretrain_checkpoint\")\n",
        "        print(\"Checkpointing model...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpzK2QUxgz4y",
        "outputId": "7753667a-39cd-4f06-8487-4d7e178c1812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean loss: 10.02941198348999. Batch 100 of 127656\n",
            "Mean loss: 9.795738954544067. Batch 200 of 127656\n",
            "Mean loss: 9.56429648399353. Batch 300 of 127656\n",
            "Mean loss: 9.289384927749634. Batch 400 of 127656\n",
            "Mean loss: 9.010039625167847. Batch 500 of 127656\n",
            "Mean loss: 8.74532172203064. Batch 600 of 127656\n",
            "Mean loss: 8.468506603240966. Batch 700 of 127656\n",
            "Mean loss: 8.13690592288971. Batch 800 of 127656\n",
            "Mean loss: 7.853899726867676. Batch 900 of 127656\n",
            "Mean loss: 7.536981410980225. Batch 1000 of 127656\n",
            "Mean loss: 7.196212720870972. Batch 1100 of 127656\n",
            "Mean loss: 6.862560253143311. Batch 1200 of 127656\n",
            "Mean loss: 6.544826788902283. Batch 1300 of 127656\n",
            "Mean loss: 6.167839441299439. Batch 1400 of 127656\n",
            "Mean loss: 5.625668239593506. Batch 1500 of 127656\n",
            "Mean loss: 5.227845764160156. Batch 1600 of 127656\n",
            "Mean loss: 4.759560990333557. Batch 1700 of 127656\n",
            "Mean loss: 4.555448789596557. Batch 1800 of 127656\n",
            "Mean loss: 3.9650766611099244. Batch 1900 of 127656\n",
            "Mean loss: 3.7731185245513914. Batch 2000 of 127656\n",
            "Mean loss: 3.3447605204582214. Batch 2100 of 127656\n",
            "Mean loss: 2.952406527996063. Batch 2200 of 127656\n",
            "Mean loss: 2.651377730369568. Batch 2300 of 127656\n",
            "Mean loss: 2.3087886667251585. Batch 2400 of 127656\n",
            "Mean loss: 2.1160520458221437. Batch 2500 of 127656\n",
            "Mean loss: 1.925292135477066. Batch 2600 of 127656\n",
            "Mean loss: 1.8180921852588654. Batch 2700 of 127656\n",
            "Mean loss: 1.7779984152317048. Batch 2800 of 127656\n",
            "Mean loss: 1.670516505241394. Batch 2900 of 127656\n",
            "Mean loss: 1.4733638429641724. Batch 3000 of 127656\n",
            "Mean loss: 1.4716039264202119. Batch 3100 of 127656\n",
            "Mean loss: 1.1696532398462296. Batch 3200 of 127656\n",
            "Mean loss: 1.3087766885757446. Batch 3300 of 127656\n",
            "Mean loss: 0.9490646517276764. Batch 3400 of 127656\n",
            "Mean loss: 1.0160217326879502. Batch 3500 of 127656\n",
            "Mean loss: 1.0616397869586944. Batch 3600 of 127656\n",
            "Mean loss: 1.0998703652620316. Batch 3700 of 127656\n",
            "Mean loss: 1.0034316271543502. Batch 3800 of 127656\n",
            "Mean loss: 0.7215882024168968. Batch 3900 of 127656\n",
            "Mean loss: 0.8355729022622108. Batch 4000 of 127656\n",
            "Mean loss: 0.6989532786607743. Batch 4100 of 127656\n",
            "Mean loss: 0.7014818507432937. Batch 4200 of 127656\n",
            "Mean loss: 0.5763005283474922. Batch 4300 of 127656\n",
            "Mean loss: 0.5109596917033196. Batch 4400 of 127656\n",
            "Mean loss: 0.6863579896092414. Batch 4500 of 127656\n",
            "Mean loss: 0.5799893011152745. Batch 4600 of 127656\n",
            "Mean loss: 0.44680193603038787. Batch 4700 of 127656\n",
            "Mean loss: 0.6945509931445122. Batch 4800 of 127656\n",
            "Mean loss: 0.6282598559558391. Batch 4900 of 127656\n",
            "Mean loss: 0.6155472950637341. Batch 5000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.35815643578767775. Batch 5100 of 127656\n",
            "Mean loss: 0.5804180416464806. Batch 5200 of 127656\n",
            "Mean loss: 0.5314567425847053. Batch 5300 of 127656\n",
            "Mean loss: 0.43928807839751244. Batch 5400 of 127656\n",
            "Mean loss: 0.3729773414880037. Batch 5500 of 127656\n",
            "Mean loss: 0.4961561466753483. Batch 5600 of 127656\n",
            "Mean loss: 0.5196190571039915. Batch 5700 of 127656\n",
            "Mean loss: 0.6078245930373669. Batch 5800 of 127656\n",
            "Mean loss: 0.2504266293346882. Batch 5900 of 127656\n",
            "Mean loss: 0.3777640423923731. Batch 6000 of 127656\n",
            "Mean loss: 0.374295042231679. Batch 6100 of 127656\n",
            "Mean loss: 0.4582338596135378. Batch 6200 of 127656\n",
            "Mean loss: 0.24777074202895163. Batch 6300 of 127656\n",
            "Mean loss: 0.497198963612318. Batch 6400 of 127656\n",
            "Mean loss: 0.3875436864793301. Batch 6500 of 127656\n",
            "Mean loss: 0.32059287998825314. Batch 6600 of 127656\n",
            "Mean loss: 0.5179422326385975. Batch 6700 of 127656\n",
            "Mean loss: 0.34073932353407144. Batch 6800 of 127656\n",
            "Mean loss: 0.3309718057513237. Batch 6900 of 127656\n",
            "Mean loss: 0.3541513184458017. Batch 7000 of 127656\n",
            "Mean loss: 0.39063634268939496. Batch 7100 of 127656\n",
            "Mean loss: 0.46788134559988975. Batch 7200 of 127656\n",
            "Mean loss: 0.2832165259495378. Batch 7300 of 127656\n",
            "Mean loss: 0.37536886766552924. Batch 7400 of 127656\n",
            "Mean loss: 0.29169678714126346. Batch 7500 of 127656\n",
            "Mean loss: 0.19188740093261003. Batch 7600 of 127656\n",
            "Mean loss: 0.4106620940938592. Batch 7700 of 127656\n",
            "Mean loss: 0.3429646276682615. Batch 7800 of 127656\n",
            "Mean loss: 0.334365485906601. Batch 7900 of 127656\n",
            "Mean loss: 0.27259465474635364. Batch 8000 of 127656\n",
            "Mean loss: 0.29984086986631153. Batch 8100 of 127656\n",
            "Mean loss: 0.35295993380248547. Batch 8200 of 127656\n",
            "Mean loss: 0.22504532504826785. Batch 8300 of 127656\n",
            "Mean loss: 0.32074620719999075. Batch 8400 of 127656\n",
            "Mean loss: 0.22824723802506924. Batch 8500 of 127656\n",
            "Mean loss: 0.20921582892537116. Batch 8600 of 127656\n",
            "Mean loss: 0.2957935335859656. Batch 8700 of 127656\n",
            "Mean loss: 0.17763871762901545. Batch 8800 of 127656\n",
            "Mean loss: 0.23185840530321003. Batch 8900 of 127656\n",
            "Mean loss: 0.29605669107288124. Batch 9000 of 127656\n",
            "Mean loss: 0.34653726045042277. Batch 9100 of 127656\n",
            "Mean loss: 0.26322703178972007. Batch 9200 of 127656\n",
            "Mean loss: 0.27974404640495776. Batch 9300 of 127656\n",
            "Mean loss: 0.232659303098917. Batch 9400 of 127656\n",
            "Mean loss: 0.24296995662152768. Batch 9500 of 127656\n",
            "Mean loss: 0.26680895160883666. Batch 9600 of 127656\n",
            "Mean loss: 0.2997807960212231. Batch 9700 of 127656\n",
            "Mean loss: 0.20785592690110208. Batch 9800 of 127656\n",
            "Mean loss: 0.1740852247737348. Batch 9900 of 127656\n",
            "Mean loss: 0.18618695698678495. Batch 10000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.25293630074709655. Batch 10100 of 127656\n",
            "Mean loss: 0.3802522145956755. Batch 10200 of 127656\n",
            "Mean loss: 0.2632175310328603. Batch 10300 of 127656\n",
            "Mean loss: 0.1792086217366159. Batch 10400 of 127656\n",
            "Mean loss: 0.2521484318748117. Batch 10500 of 127656\n",
            "Mean loss: 0.23517875634133817. Batch 10600 of 127656\n",
            "Mean loss: 0.19073000049218536. Batch 10700 of 127656\n",
            "Mean loss: 0.1450378713570535. Batch 10800 of 127656\n",
            "Mean loss: 0.2616112123243511. Batch 10900 of 127656\n",
            "Mean loss: 0.1984990944713354. Batch 11000 of 127656\n",
            "Mean loss: 0.19653685074299573. Batch 11100 of 127656\n",
            "Mean loss: 0.32073204349726436. Batch 11200 of 127656\n",
            "Mean loss: 0.2092471716925502. Batch 11300 of 127656\n",
            "Mean loss: 0.22227059047669173. Batch 11400 of 127656\n",
            "Mean loss: 0.2485911414399743. Batch 11500 of 127656\n",
            "Mean loss: 0.20606464445590972. Batch 11600 of 127656\n",
            "Mean loss: 0.21766023006290197. Batch 11700 of 127656\n",
            "Mean loss: 0.24336403673514723. Batch 11800 of 127656\n",
            "Mean loss: 0.21820862997323276. Batch 11900 of 127656\n",
            "Mean loss: 0.2752447113767266. Batch 12000 of 127656\n",
            "Mean loss: 0.18266495110467076. Batch 12100 of 127656\n",
            "Mean loss: 0.23955122677609325. Batch 12200 of 127656\n",
            "Mean loss: 0.22975144237279893. Batch 12300 of 127656\n",
            "Mean loss: 0.16412196895107628. Batch 12400 of 127656\n",
            "Mean loss: 0.1463217582926154. Batch 12500 of 127656\n",
            "Mean loss: 0.14450646352022886. Batch 12600 of 127656\n",
            "Mean loss: 0.14233050376176834. Batch 12700 of 127656\n",
            "Mean loss: 0.15908790683373808. Batch 12800 of 127656\n",
            "Mean loss: 0.11441293321549892. Batch 12900 of 127656\n",
            "Mean loss: 0.16760617349296808. Batch 13000 of 127656\n",
            "Mean loss: 0.1540987445227802. Batch 13100 of 127656\n",
            "Mean loss: 0.20265866827219725. Batch 13200 of 127656\n",
            "Mean loss: 0.21021454732865094. Batch 13300 of 127656\n",
            "Mean loss: 0.14311194952577352. Batch 13400 of 127656\n",
            "Mean loss: 0.12723474185913802. Batch 13500 of 127656\n",
            "Mean loss: 0.17885286854580046. Batch 13600 of 127656\n",
            "Mean loss: 0.1200473185814917. Batch 13700 of 127656\n",
            "Mean loss: 0.21919015679508447. Batch 13800 of 127656\n",
            "Mean loss: 0.19063948400318623. Batch 13900 of 127656\n",
            "Mean loss: 0.2438346964493394. Batch 14000 of 127656\n",
            "Mean loss: 0.14603139717131852. Batch 14100 of 127656\n",
            "Mean loss: 0.19667544381693006. Batch 14200 of 127656\n",
            "Mean loss: 0.1146456884406507. Batch 14300 of 127656\n",
            "Mean loss: 0.22939860578626395. Batch 14400 of 127656\n",
            "Mean loss: 0.18130709268152714. Batch 14500 of 127656\n",
            "Mean loss: 0.10159289184957743. Batch 14600 of 127656\n",
            "Mean loss: 0.20154455984011294. Batch 14700 of 127656\n",
            "Mean loss: 0.18171385187655686. Batch 14800 of 127656\n",
            "Mean loss: 0.18575009860098363. Batch 14900 of 127656\n",
            "Mean loss: 0.15142961828038096. Batch 15000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.2509366857819259. Batch 15100 of 127656\n",
            "Mean loss: 0.21380943659693002. Batch 15200 of 127656\n",
            "Mean loss: 0.14604076519608497. Batch 15300 of 127656\n",
            "Mean loss: 0.15179865147918462. Batch 15400 of 127656\n",
            "Mean loss: 0.14610999932512642. Batch 15500 of 127656\n",
            "Mean loss: 0.11359722789376975. Batch 15600 of 127656\n",
            "Mean loss: 0.21950768753886224. Batch 15700 of 127656\n",
            "Mean loss: 0.16501403369009496. Batch 15800 of 127656\n",
            "Mean loss: 0.12765021746978164. Batch 15900 of 127656\n",
            "Mean loss: 0.11739325584843754. Batch 16000 of 127656\n",
            "Mean loss: 0.17690196350216866. Batch 16100 of 127656\n",
            "Mean loss: 0.14207943059504033. Batch 16200 of 127656\n",
            "Mean loss: 0.14493570771068334. Batch 16300 of 127656\n",
            "Mean loss: 0.30149366211146117. Batch 16400 of 127656\n",
            "Mean loss: 0.1431831683218479. Batch 16500 of 127656\n",
            "Mean loss: 0.2097779220342636. Batch 16600 of 127656\n",
            "Mean loss: 0.136237634383142. Batch 16700 of 127656\n",
            "Mean loss: 0.18109182050451636. Batch 16800 of 127656\n",
            "Mean loss: 0.17994294788688422. Batch 16900 of 127656\n",
            "Mean loss: 0.17815776985138654. Batch 17000 of 127656\n",
            "Mean loss: 0.20040205555036664. Batch 17100 of 127656\n",
            "Mean loss: 0.36131836906075476. Batch 17200 of 127656\n",
            "Mean loss: 0.2918487588688731. Batch 17300 of 127656\n",
            "Mean loss: 0.15096832662820817. Batch 17400 of 127656\n",
            "Mean loss: 0.12422415440902114. Batch 17500 of 127656\n",
            "Mean loss: 0.2056354269385338. Batch 17600 of 127656\n",
            "Mean loss: 0.2306562528386712. Batch 17700 of 127656\n",
            "Mean loss: 0.1600869146361947. Batch 17800 of 127656\n",
            "Mean loss: 0.15413064714521169. Batch 17900 of 127656\n",
            "Mean loss: 0.2434535812959075. Batch 18000 of 127656\n",
            "Mean loss: 0.11278910167515278. Batch 18100 of 127656\n",
            "Mean loss: 0.21439024060964584. Batch 18200 of 127656\n",
            "Mean loss: 0.18568130649626255. Batch 18300 of 127656\n",
            "Mean loss: 0.15422189734876157. Batch 18400 of 127656\n",
            "Mean loss: 0.158610893599689. Batch 18500 of 127656\n",
            "Mean loss: 0.15581739865243435. Batch 18600 of 127656\n",
            "Mean loss: 0.2145619198307395. Batch 18700 of 127656\n",
            "Mean loss: 0.19808735903352498. Batch 18800 of 127656\n",
            "Mean loss: 0.11343765366822481. Batch 18900 of 127656\n",
            "Mean loss: 0.12485328990966081. Batch 19000 of 127656\n",
            "Mean loss: 0.19694405019283295. Batch 19100 of 127656\n",
            "Mean loss: 0.09902836535125971. Batch 19200 of 127656\n",
            "Mean loss: 0.20965351039543748. Batch 19300 of 127656\n",
            "Mean loss: 0.13349138209596276. Batch 19400 of 127656\n",
            "Mean loss: 0.17266817651689054. Batch 19500 of 127656\n",
            "Mean loss: 0.20286914840340614. Batch 19600 of 127656\n",
            "Mean loss: 0.17837432947009801. Batch 19700 of 127656\n",
            "Mean loss: 0.18211342688649892. Batch 19800 of 127656\n",
            "Mean loss: 0.2043049615621567. Batch 19900 of 127656\n",
            "Mean loss: 0.1724815885350108. Batch 20000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.15699602089822293. Batch 20100 of 127656\n",
            "Mean loss: 0.21539931882172822. Batch 20200 of 127656\n",
            "Mean loss: 0.09560442220419646. Batch 20300 of 127656\n",
            "Mean loss: 0.11409629007801414. Batch 20400 of 127656\n",
            "Mean loss: 0.1273955111950636. Batch 20500 of 127656\n",
            "Mean loss: 0.10059999912977219. Batch 20600 of 127656\n",
            "Mean loss: 0.21090748462826014. Batch 20700 of 127656\n",
            "Mean loss: 0.16876981899142265. Batch 20800 of 127656\n",
            "Mean loss: 0.1869365783408284. Batch 20900 of 127656\n",
            "Mean loss: 0.1522850925847888. Batch 21000 of 127656\n",
            "Mean loss: 0.11735398354008794. Batch 21100 of 127656\n",
            "Mean loss: 0.09747753396630288. Batch 21200 of 127656\n",
            "Mean loss: 0.10544867778196931. Batch 21300 of 127656\n",
            "Mean loss: 0.14818773003295063. Batch 21400 of 127656\n",
            "Mean loss: 0.12629131009802222. Batch 21500 of 127656\n",
            "Mean loss: 0.10762605257332325. Batch 21600 of 127656\n",
            "Mean loss: 0.14888087280094622. Batch 21700 of 127656\n",
            "Mean loss: 0.11211592610925436. Batch 21800 of 127656\n",
            "Mean loss: 0.22381283527240156. Batch 21900 of 127656\n",
            "Mean loss: 0.14958867575973273. Batch 22000 of 127656\n",
            "Mean loss: 0.10288415789604187. Batch 22100 of 127656\n",
            "Mean loss: 0.14360146798193454. Batch 22200 of 127656\n",
            "Mean loss: 0.18935559041798114. Batch 22300 of 127656\n",
            "Mean loss: 0.16299709137529134. Batch 22400 of 127656\n",
            "Mean loss: 0.12336018210276961. Batch 22500 of 127656\n",
            "Mean loss: 0.21402087731286884. Batch 22600 of 127656\n",
            "Mean loss: 0.09873445285484195. Batch 22700 of 127656\n",
            "Mean loss: 0.2018299554102123. Batch 22800 of 127656\n",
            "Mean loss: 0.21983392130583523. Batch 22900 of 127656\n",
            "Mean loss: 0.18487555958330631. Batch 23000 of 127656\n",
            "Mean loss: 0.08053532266989351. Batch 23100 of 127656\n",
            "Mean loss: 0.17588655412197113. Batch 23200 of 127656\n",
            "Mean loss: 0.11306136272847653. Batch 23300 of 127656\n",
            "Mean loss: 0.1112133626267314. Batch 23400 of 127656\n",
            "Mean loss: 0.14336355809122325. Batch 23500 of 127656\n",
            "Mean loss: 0.14029127944260836. Batch 23600 of 127656\n",
            "Mean loss: 0.20539678661152722. Batch 23700 of 127656\n",
            "Mean loss: 0.15652756087481975. Batch 23800 of 127656\n",
            "Mean loss: 0.12935766730457543. Batch 23900 of 127656\n",
            "Mean loss: 0.08474600788205862. Batch 24000 of 127656\n",
            "Mean loss: 0.18375653091818095. Batch 24100 of 127656\n",
            "Mean loss: 0.15013405971229077. Batch 24200 of 127656\n",
            "Mean loss: 0.10225308595225215. Batch 24300 of 127656\n",
            "Mean loss: 0.14195808550342917. Batch 24400 of 127656\n",
            "Mean loss: 0.10319319119676948. Batch 24500 of 127656\n",
            "Mean loss: 0.17412118626758455. Batch 24600 of 127656\n",
            "Mean loss: 0.1300335999019444. Batch 24700 of 127656\n",
            "Mean loss: 0.10146165592595935. Batch 24800 of 127656\n",
            "Mean loss: 0.14021440440788865. Batch 24900 of 127656\n",
            "Mean loss: 0.1376792372763157. Batch 25000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.08312306214123964. Batch 25100 of 127656\n",
            "Mean loss: 0.16768927911296486. Batch 25200 of 127656\n",
            "Mean loss: 0.15388971110805869. Batch 25300 of 127656\n",
            "Mean loss: 0.08803539648652077. Batch 25400 of 127656\n",
            "Mean loss: 0.06866112930700183. Batch 25500 of 127656\n",
            "Mean loss: 0.1310606862232089. Batch 25600 of 127656\n",
            "Mean loss: 0.21548671644181014. Batch 25700 of 127656\n",
            "Mean loss: 0.1026401985436678. Batch 25800 of 127656\n",
            "Mean loss: 0.08580993192270397. Batch 25900 of 127656\n",
            "Mean loss: 0.08398656513541937. Batch 26000 of 127656\n",
            "Mean loss: 0.22100353855639696. Batch 26100 of 127656\n",
            "Mean loss: 0.12415265787392854. Batch 26200 of 127656\n",
            "Mean loss: 0.15414996115490795. Batch 26300 of 127656\n",
            "Mean loss: 0.11205213073641061. Batch 26400 of 127656\n",
            "Mean loss: 0.10433546105399728. Batch 26500 of 127656\n",
            "Mean loss: 0.14203903786838054. Batch 26600 of 127656\n",
            "Mean loss: 0.1326224048063159. Batch 26700 of 127656\n",
            "Mean loss: 0.14577725043520331. Batch 26800 of 127656\n",
            "Mean loss: 0.08076079610735178. Batch 26900 of 127656\n",
            "Mean loss: 0.10997112888842821. Batch 27000 of 127656\n",
            "Mean loss: 0.11448382940143347. Batch 27100 of 127656\n",
            "Mean loss: 0.12452113319188357. Batch 27200 of 127656\n",
            "Mean loss: 0.11911619435995817. Batch 27300 of 127656\n",
            "Mean loss: 0.10758429914712905. Batch 27400 of 127656\n",
            "Mean loss: 0.06988407308235764. Batch 27500 of 127656\n",
            "Mean loss: 0.22239359721541405. Batch 27600 of 127656\n",
            "Mean loss: 0.13604493383318186. Batch 27700 of 127656\n",
            "Mean loss: 0.08526705188676714. Batch 27800 of 127656\n",
            "Mean loss: 0.14906185176223516. Batch 27900 of 127656\n",
            "Mean loss: 0.11215055825188756. Batch 28000 of 127656\n",
            "Mean loss: 0.19262542702257635. Batch 28100 of 127656\n",
            "Mean loss: 0.1446298937126994. Batch 28200 of 127656\n",
            "Mean loss: 0.07985935473814607. Batch 28300 of 127656\n",
            "Mean loss: 0.209409503005445. Batch 28400 of 127656\n",
            "Mean loss: 0.16455156579613686. Batch 28500 of 127656\n",
            "Mean loss: 0.21406491808593273. Batch 28600 of 127656\n",
            "Mean loss: 0.08019646782428026. Batch 28700 of 127656\n",
            "Mean loss: 0.12190877726301551. Batch 28800 of 127656\n",
            "Mean loss: 0.15537362691015005. Batch 28900 of 127656\n",
            "Mean loss: 0.12342297414317727. Batch 29000 of 127656\n",
            "Mean loss: 0.1600937613658607. Batch 29100 of 127656\n",
            "Mean loss: 0.16377191081643105. Batch 29200 of 127656\n",
            "Mean loss: 0.15717188831418752. Batch 29300 of 127656\n",
            "Mean loss: 0.1059993459098041. Batch 29400 of 127656\n",
            "Mean loss: 0.11495780885219574. Batch 29500 of 127656\n",
            "Mean loss: 0.0922827404178679. Batch 29600 of 127656\n",
            "Mean loss: 0.10639400705695153. Batch 29700 of 127656\n",
            "Mean loss: 0.12663726249709725. Batch 29800 of 127656\n",
            "Mean loss: 0.12493916714563966. Batch 29900 of 127656\n",
            "Mean loss: 0.13288235574960708. Batch 30000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.21462895303964616. Batch 30100 of 127656\n",
            "Mean loss: 0.14451551891863346. Batch 30200 of 127656\n",
            "Mean loss: 0.19229114435613157. Batch 30300 of 127656\n",
            "Mean loss: 0.0944027223624289. Batch 30400 of 127656\n",
            "Mean loss: 0.1584076408855617. Batch 30500 of 127656\n",
            "Mean loss: 0.16172146081924438. Batch 30600 of 127656\n",
            "Mean loss: 0.15143275935202838. Batch 30700 of 127656\n",
            "Mean loss: 0.058068099655210974. Batch 30800 of 127656\n",
            "Mean loss: 0.22558686457574367. Batch 30900 of 127656\n",
            "Mean loss: 0.08240220917388796. Batch 31000 of 127656\n",
            "Mean loss: 0.14415549170225858. Batch 31100 of 127656\n",
            "Mean loss: 0.11994700375944375. Batch 31200 of 127656\n",
            "Mean loss: 0.18407992463558912. Batch 31300 of 127656\n",
            "Mean loss: 0.1247016453370452. Batch 31400 of 127656\n",
            "Mean loss: 0.1894827582500875. Batch 31500 of 127656\n",
            "Mean loss: 0.18321413695812225. Batch 31600 of 127656\n",
            "Mean loss: 0.10226660434156656. Batch 31700 of 127656\n",
            "Mean loss: 0.14020862832665443. Batch 31800 of 127656\n",
            "Mean loss: 0.15009076230227947. Batch 31900 of 127656\n",
            "Mean loss: 0.2191910206899047. Batch 32000 of 127656\n",
            "Mean loss: 0.20221657879650592. Batch 32100 of 127656\n",
            "Mean loss: 0.1606430872157216. Batch 32200 of 127656\n",
            "Mean loss: 0.09341116340830923. Batch 32300 of 127656\n",
            "Mean loss: 0.1728346293605864. Batch 32400 of 127656\n",
            "Mean loss: 0.09216840106993913. Batch 32500 of 127656\n",
            "Mean loss: 0.18165235638618468. Batch 32600 of 127656\n",
            "Mean loss: 0.2084755408577621. Batch 32700 of 127656\n",
            "Mean loss: 0.14272989220917226. Batch 32800 of 127656\n",
            "Mean loss: 0.11689617209136487. Batch 32900 of 127656\n",
            "Mean loss: 0.1491747620329261. Batch 33000 of 127656\n",
            "Mean loss: 0.11525454483926297. Batch 33100 of 127656\n",
            "Mean loss: 0.20145497316494584. Batch 33200 of 127656\n",
            "Mean loss: 0.10252200476825238. Batch 33300 of 127656\n",
            "Mean loss: 0.15657753067091107. Batch 33400 of 127656\n",
            "Mean loss: 0.09615448577329516. Batch 33500 of 127656\n",
            "Mean loss: 0.13189370073378087. Batch 33600 of 127656\n",
            "Mean loss: 0.20159647051244975. Batch 33700 of 127656\n",
            "Mean loss: 0.1342929807305336. Batch 33800 of 127656\n",
            "Mean loss: 0.16653928196057677. Batch 33900 of 127656\n",
            "Mean loss: 0.11524854190647602. Batch 34000 of 127656\n",
            "Mean loss: 0.19425307461991906. Batch 34100 of 127656\n",
            "Mean loss: 0.15656301349401475. Batch 34200 of 127656\n",
            "Mean loss: 0.1592361678928137. Batch 34300 of 127656\n",
            "Mean loss: 0.19279825942590834. Batch 34400 of 127656\n",
            "Mean loss: 0.12159239972010255. Batch 34500 of 127656\n",
            "Mean loss: 0.10448838479816913. Batch 34600 of 127656\n",
            "Mean loss: 0.20474572584033013. Batch 34700 of 127656\n",
            "Mean loss: 0.18698446668684482. Batch 34800 of 127656\n",
            "Mean loss: 0.17694484677165748. Batch 34900 of 127656\n",
            "Mean loss: 0.15503737282007932. Batch 35000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.1601089785248041. Batch 35100 of 127656\n",
            "Mean loss: 0.12909278605133295. Batch 35200 of 127656\n",
            "Mean loss: 0.11084721375256777. Batch 35300 of 127656\n",
            "Mean loss: 0.1236838118173182. Batch 35400 of 127656\n",
            "Mean loss: 0.11872786214575172. Batch 35500 of 127656\n",
            "Mean loss: 0.11462788412347436. Batch 35600 of 127656\n",
            "Mean loss: 0.20220051599666478. Batch 35700 of 127656\n",
            "Mean loss: 0.16404645405709745. Batch 35800 of 127656\n",
            "Mean loss: 0.16419347871094941. Batch 35900 of 127656\n",
            "Mean loss: 0.14190317284315823. Batch 36000 of 127656\n",
            "Mean loss: 0.18805872693657874. Batch 36100 of 127656\n",
            "Mean loss: 0.10782714497298002. Batch 36200 of 127656\n",
            "Mean loss: 0.14891362372785807. Batch 36300 of 127656\n",
            "Mean loss: 0.17518385712057352. Batch 36400 of 127656\n",
            "Mean loss: 0.19319649588316679. Batch 36500 of 127656\n",
            "Mean loss: 0.13717726960778237. Batch 36600 of 127656\n",
            "Mean loss: 0.15154853891581296. Batch 36700 of 127656\n",
            "Mean loss: 0.15318574164062737. Batch 36800 of 127656\n",
            "Mean loss: 0.17641990963369608. Batch 36900 of 127656\n",
            "Mean loss: 0.11361616391688585. Batch 37000 of 127656\n",
            "Mean loss: 0.1702074697613716. Batch 37100 of 127656\n",
            "Mean loss: 0.1098606937378645. Batch 37200 of 127656\n",
            "Mean loss: 0.08305291322991253. Batch 37300 of 127656\n",
            "Mean loss: 0.10469017140567302. Batch 37400 of 127656\n",
            "Mean loss: 0.15349955655634404. Batch 37500 of 127656\n",
            "Mean loss: 0.2366534355469048. Batch 37600 of 127656\n",
            "Mean loss: 0.09094560047611594. Batch 37700 of 127656\n",
            "Mean loss: 0.13823549387976528. Batch 37800 of 127656\n",
            "Mean loss: 0.12052995912730693. Batch 37900 of 127656\n",
            "Mean loss: 0.19017904767766594. Batch 38000 of 127656\n",
            "Mean loss: 0.18400508146733047. Batch 38100 of 127656\n",
            "Mean loss: 0.13870368607342243. Batch 38200 of 127656\n",
            "Mean loss: 0.08584764895960689. Batch 38300 of 127656\n",
            "Mean loss: 0.15004386704415082. Batch 38400 of 127656\n",
            "Mean loss: 0.12943746168166398. Batch 38500 of 127656\n",
            "Mean loss: 0.1664350915327668. Batch 38600 of 127656\n",
            "Mean loss: 0.15906559467315673. Batch 38700 of 127656\n",
            "Mean loss: 0.14636694606393574. Batch 38800 of 127656\n",
            "Mean loss: 0.2380390852689743. Batch 38900 of 127656\n",
            "Mean loss: 0.21152844667434692. Batch 39000 of 127656\n",
            "Mean loss: 0.14300397209823132. Batch 39100 of 127656\n",
            "Mean loss: 0.16288724713027478. Batch 39200 of 127656\n",
            "Mean loss: 0.1568998731300235. Batch 39300 of 127656\n",
            "Mean loss: 0.13394267026335002. Batch 39400 of 127656\n",
            "Mean loss: 0.16709526788443327. Batch 39500 of 127656\n",
            "Mean loss: 0.11111476130783558. Batch 39600 of 127656\n",
            "Mean loss: 0.14144731532782318. Batch 39700 of 127656\n",
            "Mean loss: 0.07360617958009243. Batch 39800 of 127656\n",
            "Mean loss: 0.09975024916231633. Batch 39900 of 127656\n",
            "Mean loss: 0.1368971907161176. Batch 40000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.08795833352953196. Batch 40100 of 127656\n",
            "Mean loss: 0.19012304021045565. Batch 40200 of 127656\n",
            "Mean loss: 0.17518323494121432. Batch 40300 of 127656\n",
            "Mean loss: 0.2083338288590312. Batch 40400 of 127656\n",
            "Mean loss: 0.18899193666875364. Batch 40500 of 127656\n",
            "Mean loss: 0.11549755798652768. Batch 40600 of 127656\n",
            "Mean loss: 0.19099693398922682. Batch 40700 of 127656\n",
            "Mean loss: 0.1478266992792487. Batch 40800 of 127656\n",
            "Mean loss: 0.17172182995826005. Batch 40900 of 127656\n",
            "Mean loss: 0.10412542376667261. Batch 41000 of 127656\n",
            "Mean loss: 0.14177200049161912. Batch 41100 of 127656\n",
            "Mean loss: 0.14326525649055838. Batch 41200 of 127656\n",
            "Mean loss: 0.14654715664684773. Batch 41300 of 127656\n",
            "Mean loss: 0.13435580359771848. Batch 41400 of 127656\n",
            "Mean loss: 0.14515713470056654. Batch 41500 of 127656\n",
            "Mean loss: 0.14273726258426905. Batch 41600 of 127656\n",
            "Mean loss: 0.14161113617941737. Batch 41700 of 127656\n",
            "Mean loss: 0.13905552458018064. Batch 41800 of 127656\n",
            "Mean loss: 0.1791598903760314. Batch 41900 of 127656\n",
            "Mean loss: 0.12200525702908635. Batch 42000 of 127656\n",
            "Mean loss: 0.20725215511396528. Batch 42100 of 127656\n",
            "Mean loss: 0.09348283981904387. Batch 42200 of 127656\n",
            "Mean loss: 0.14205875154584646. Batch 42300 of 127656\n",
            "Mean loss: 0.1850286128744483. Batch 42400 of 127656\n",
            "Mean loss: 0.12715827060863374. Batch 42500 of 127656\n",
            "Mean loss: 0.19855045598000287. Batch 42600 of 127656\n",
            "Mean loss: 0.12224104084074497. Batch 42700 of 127656\n",
            "Mean loss: 0.06492517497390508. Batch 42800 of 127656\n",
            "Mean loss: 0.17887743651866914. Batch 42900 of 127656\n",
            "Mean loss: 0.16909083023667335. Batch 43000 of 127656\n",
            "Mean loss: 0.14005324970930816. Batch 43100 of 127656\n",
            "Mean loss: 0.1627565328963101. Batch 43200 of 127656\n",
            "Mean loss: 0.12291571609675885. Batch 43300 of 127656\n",
            "Mean loss: 0.17068834006786346. Batch 43400 of 127656\n",
            "Mean loss: 0.10521324962377548. Batch 43500 of 127656\n",
            "Mean loss: 0.166633539814502. Batch 43600 of 127656\n",
            "Mean loss: 0.12931901946663857. Batch 43700 of 127656\n",
            "Mean loss: 0.1745683495141566. Batch 43800 of 127656\n",
            "Mean loss: 0.10521113673225045. Batch 43900 of 127656\n",
            "Mean loss: 0.14063907980918885. Batch 44000 of 127656\n",
            "Mean loss: 0.15844564694911242. Batch 44100 of 127656\n",
            "Mean loss: 0.16442452020943166. Batch 44200 of 127656\n",
            "Mean loss: 0.10513356126844883. Batch 44300 of 127656\n",
            "Mean loss: 0.13183370182290674. Batch 44400 of 127656\n",
            "Mean loss: 0.18138009916990996. Batch 44500 of 127656\n",
            "Mean loss: 0.12299069439992309. Batch 44600 of 127656\n",
            "Mean loss: 0.03734703144058585. Batch 44700 of 127656\n",
            "Mean loss: 0.09564005887135864. Batch 44800 of 127656\n",
            "Mean loss: 0.138208392187953. Batch 44900 of 127656\n",
            "Mean loss: 0.18303693616762756. Batch 45000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.1349852596409619. Batch 45100 of 127656\n",
            "Mean loss: 0.10243600488640368. Batch 45200 of 127656\n",
            "Mean loss: 0.06062514310702682. Batch 45300 of 127656\n",
            "Mean loss: 0.0956223962828517. Batch 45400 of 127656\n",
            "Mean loss: 0.059727336382493376. Batch 45500 of 127656\n",
            "Mean loss: 0.14320106782019137. Batch 45600 of 127656\n",
            "Mean loss: 0.16175493201240898. Batch 45700 of 127656\n",
            "Mean loss: 0.07107487183995545. Batch 45800 of 127656\n",
            "Mean loss: 0.12017174248583615. Batch 45900 of 127656\n",
            "Mean loss: 0.11997899872250856. Batch 46000 of 127656\n",
            "Mean loss: 0.13434555753134192. Batch 46100 of 127656\n",
            "Mean loss: 0.09465531526133418. Batch 46200 of 127656\n",
            "Mean loss: 0.1541598807834089. Batch 46300 of 127656\n",
            "Mean loss: 0.11965419269166887. Batch 46400 of 127656\n",
            "Mean loss: 0.1473099159821868. Batch 46500 of 127656\n",
            "Mean loss: 0.13810853674076498. Batch 46600 of 127656\n",
            "Mean loss: 0.2384377701766789. Batch 46700 of 127656\n",
            "Mean loss: 0.11727623148821294. Batch 46800 of 127656\n",
            "Mean loss: 0.1753901977557689. Batch 46900 of 127656\n",
            "Mean loss: 0.10038114000111818. Batch 47000 of 127656\n",
            "Mean loss: 0.14171416931785644. Batch 47100 of 127656\n",
            "Mean loss: 0.1079506341740489. Batch 47200 of 127656\n",
            "Mean loss: 0.13282668370753525. Batch 47300 of 127656\n",
            "Mean loss: 0.09933055562898517. Batch 47400 of 127656\n",
            "Mean loss: 0.04644126375205815. Batch 47500 of 127656\n",
            "Mean loss: 0.18684045194648205. Batch 47600 of 127656\n",
            "Mean loss: 0.09422385063022375. Batch 47700 of 127656\n",
            "Mean loss: 0.18789252981543542. Batch 47800 of 127656\n",
            "Mean loss: 0.10075979197397827. Batch 47900 of 127656\n",
            "Mean loss: 0.10388423682190479. Batch 48000 of 127656\n",
            "Mean loss: 0.1057207403704524. Batch 48100 of 127656\n",
            "Mean loss: 0.09884390667080879. Batch 48200 of 127656\n",
            "Mean loss: 0.1027480692230165. Batch 48300 of 127656\n",
            "Mean loss: 0.17352180833928288. Batch 48400 of 127656\n",
            "Mean loss: 0.12371363204903901. Batch 48500 of 127656\n",
            "Mean loss: 0.16725260114297272. Batch 48600 of 127656\n",
            "Mean loss: 0.14110747312195598. Batch 48700 of 127656\n",
            "Mean loss: 0.15016592358238995. Batch 48800 of 127656\n",
            "Mean loss: 0.10621604958549141. Batch 48900 of 127656\n",
            "Mean loss: 0.07818276596255601. Batch 49000 of 127656\n",
            "Mean loss: 0.12400009394623339. Batch 49100 of 127656\n",
            "Mean loss: 0.06030311627313495. Batch 49200 of 127656\n",
            "Mean loss: 0.10309240974485874. Batch 49300 of 127656\n",
            "Mean loss: 0.11920857388526201. Batch 49400 of 127656\n",
            "Mean loss: 0.15784824267029762. Batch 49500 of 127656\n",
            "Mean loss: 0.12333554118871688. Batch 49600 of 127656\n",
            "Mean loss: 0.12677308302372695. Batch 49700 of 127656\n",
            "Mean loss: 0.08638007622212171. Batch 49800 of 127656\n",
            "Mean loss: 0.12657199589535595. Batch 49900 of 127656\n",
            "Mean loss: 0.06175854583270848. Batch 50000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.11723764146678149. Batch 50100 of 127656\n",
            "Mean loss: 0.142590107973665. Batch 50200 of 127656\n",
            "Mean loss: 0.12050087488256395. Batch 50300 of 127656\n",
            "Mean loss: 0.06845314905047417. Batch 50400 of 127656\n",
            "Mean loss: 0.1386787666240707. Batch 50500 of 127656\n",
            "Mean loss: 0.11408281201031059. Batch 50600 of 127656\n",
            "Mean loss: 0.1292534365132451. Batch 50700 of 127656\n",
            "Mean loss: 0.14863860885147007. Batch 50800 of 127656\n",
            "Mean loss: 0.12310386235360056. Batch 50900 of 127656\n",
            "Mean loss: 0.11113527473062276. Batch 51000 of 127656\n",
            "Mean loss: 0.09819623694755136. Batch 51100 of 127656\n",
            "Mean loss: 0.1392448936868459. Batch 51200 of 127656\n",
            "Mean loss: 0.10725265235640108. Batch 51300 of 127656\n",
            "Mean loss: 0.10723135710693896. Batch 51400 of 127656\n",
            "Mean loss: 0.09614188169129192. Batch 51500 of 127656\n",
            "Mean loss: 0.1851667363103479. Batch 51600 of 127656\n",
            "Mean loss: 0.14485585875809193. Batch 51700 of 127656\n",
            "Mean loss: 0.047039918648079035. Batch 51800 of 127656\n",
            "Mean loss: 0.09061417042277753. Batch 51900 of 127656\n",
            "Mean loss: 0.12904622831847518. Batch 52000 of 127656\n",
            "Mean loss: 0.08497958790510893. Batch 52100 of 127656\n",
            "Mean loss: 0.12204330443404615. Batch 52200 of 127656\n",
            "Mean loss: 0.07037083258386702. Batch 52300 of 127656\n",
            "Mean loss: 0.08272735222242772. Batch 52400 of 127656\n",
            "Mean loss: 0.13902420941274612. Batch 52500 of 127656\n",
            "Mean loss: 0.06804174335673452. Batch 52600 of 127656\n",
            "Mean loss: 0.11690342989750206. Batch 52700 of 127656\n",
            "Mean loss: 0.16702967839781194. Batch 52800 of 127656\n",
            "Mean loss: 0.1259125114325434. Batch 52900 of 127656\n",
            "Mean loss: 0.16484644336160273. Batch 53000 of 127656\n",
            "Mean loss: 0.15265715210232883. Batch 53100 of 127656\n",
            "Mean loss: 0.1128377337846905. Batch 53200 of 127656\n",
            "Mean loss: 0.07544408908579499. Batch 53300 of 127656\n",
            "Mean loss: 0.09804847078863531. Batch 53400 of 127656\n",
            "Mean loss: 0.13869160387665033. Batch 53500 of 127656\n",
            "Mean loss: 0.1310769440140575. Batch 53600 of 127656\n",
            "Mean loss: 0.09048812531866134. Batch 53700 of 127656\n",
            "Mean loss: 0.07540549616329371. Batch 53800 of 127656\n",
            "Mean loss: 0.14675773564726113. Batch 53900 of 127656\n",
            "Mean loss: 0.08418928619939833. Batch 54000 of 127656\n",
            "Mean loss: 0.10274376455228776. Batch 54100 of 127656\n",
            "Mean loss: 0.09659398459363729. Batch 54200 of 127656\n",
            "Mean loss: 0.12663602138403804. Batch 54300 of 127656\n",
            "Mean loss: 0.17383499208837747. Batch 54400 of 127656\n",
            "Mean loss: 0.09810968155506998. Batch 54500 of 127656\n",
            "Mean loss: 0.1246889273589477. Batch 54600 of 127656\n",
            "Mean loss: 0.10762040256056934. Batch 54700 of 127656\n",
            "Mean loss: 0.1287481529545039. Batch 54800 of 127656\n",
            "Mean loss: 0.0937029829621315. Batch 54900 of 127656\n",
            "Mean loss: 0.10087232718244195. Batch 55000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.10843347106128931. Batch 55100 of 127656\n",
            "Mean loss: 0.10454473907593638. Batch 55200 of 127656\n",
            "Mean loss: 0.13751098178327084. Batch 55300 of 127656\n",
            "Mean loss: 0.06954275716096163. Batch 55400 of 127656\n",
            "Mean loss: 0.07216266355942935. Batch 55500 of 127656\n",
            "Mean loss: 0.08682521763257682. Batch 55600 of 127656\n",
            "Mean loss: 0.14600438938941807. Batch 55700 of 127656\n",
            "Mean loss: 0.11252513023093343. Batch 55800 of 127656\n",
            "Mean loss: 0.13088878269307316. Batch 55900 of 127656\n",
            "Mean loss: 0.1363892414793372. Batch 56000 of 127656\n",
            "Mean loss: 0.1269989301310852. Batch 56100 of 127656\n",
            "Mean loss: 0.14870540854986758. Batch 56200 of 127656\n",
            "Mean loss: 0.15844310159794986. Batch 56300 of 127656\n",
            "Mean loss: 0.12659007957670837. Batch 56400 of 127656\n",
            "Mean loss: 0.17062733137980104. Batch 56500 of 127656\n",
            "Mean loss: 0.07959180030971766. Batch 56600 of 127656\n",
            "Mean loss: 0.1165893082600087. Batch 56700 of 127656\n",
            "Mean loss: 0.09014598262961954. Batch 56800 of 127656\n",
            "Mean loss: 0.09327339599374682. Batch 56900 of 127656\n",
            "Mean loss: 0.122990943165496. Batch 57000 of 127656\n",
            "Mean loss: 0.12099617612082511. Batch 57100 of 127656\n",
            "Mean loss: 0.10682540258392692. Batch 57200 of 127656\n",
            "Mean loss: 0.09286844129674136. Batch 57300 of 127656\n",
            "Mean loss: 0.06626995612867176. Batch 57400 of 127656\n",
            "Mean loss: 0.1427437216276303. Batch 57500 of 127656\n",
            "Mean loss: 0.09444361008703708. Batch 57600 of 127656\n",
            "Mean loss: 0.12398080726154148. Batch 57700 of 127656\n",
            "Mean loss: 0.10085846648551523. Batch 57800 of 127656\n",
            "Mean loss: 0.060857781041413546. Batch 57900 of 127656\n",
            "Mean loss: 0.08164388953708113. Batch 58000 of 127656\n",
            "Mean loss: 0.12421688158996404. Batch 58100 of 127656\n",
            "Mean loss: 0.06417974792886526. Batch 58200 of 127656\n",
            "Mean loss: 0.1105490841763094. Batch 58300 of 127656\n",
            "Mean loss: 0.061768895741552116. Batch 58400 of 127656\n",
            "Mean loss: 0.054917298923246566. Batch 58500 of 127656\n",
            "Mean loss: 0.12514512794557958. Batch 58600 of 127656\n",
            "Mean loss: 0.03822926517110318. Batch 58700 of 127656\n",
            "Mean loss: 0.09926568165887147. Batch 58800 of 127656\n",
            "Mean loss: 0.12097325349692255. Batch 58900 of 127656\n",
            "Mean loss: 0.12106229265220464. Batch 59000 of 127656\n",
            "Mean loss: 0.12090991209726781. Batch 59100 of 127656\n",
            "Mean loss: 0.1438462157221511. Batch 59200 of 127656\n",
            "Mean loss: 0.12988973166793585. Batch 59300 of 127656\n",
            "Mean loss: 0.06866267254110425. Batch 59400 of 127656\n",
            "Mean loss: 0.09848088492173701. Batch 59500 of 127656\n",
            "Mean loss: 0.11072663893923164. Batch 59600 of 127656\n",
            "Mean loss: 0.1269424412259832. Batch 59700 of 127656\n",
            "Mean loss: 0.1333835472073406. Batch 59800 of 127656\n",
            "Mean loss: 0.10942101586144418. Batch 59900 of 127656\n",
            "Mean loss: 0.07878466058056802. Batch 60000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.0979452625894919. Batch 60100 of 127656\n",
            "Mean loss: 0.09269021729473025. Batch 60200 of 127656\n",
            "Mean loss: 0.042261765669099984. Batch 60300 of 127656\n",
            "Mean loss: 0.14977967394981534. Batch 60400 of 127656\n",
            "Mean loss: 0.09071598566602916. Batch 60500 of 127656\n",
            "Mean loss: 0.09790513606276363. Batch 60600 of 127656\n",
            "Mean loss: 0.08780719073023648. Batch 60700 of 127656\n",
            "Mean loss: 0.10771318212617188. Batch 60800 of 127656\n",
            "Mean loss: 0.12538247598335148. Batch 60900 of 127656\n",
            "Mean loss: 0.13507983521092684. Batch 61000 of 127656\n",
            "Mean loss: 0.11590214749798179. Batch 61100 of 127656\n",
            "Mean loss: 0.08244378698989749. Batch 61200 of 127656\n",
            "Mean loss: 0.06636665115132928. Batch 61300 of 127656\n",
            "Mean loss: 0.04490222775842995. Batch 61400 of 127656\n",
            "Mean loss: 0.09397864523343742. Batch 61500 of 127656\n",
            "Mean loss: 0.08535214964300394. Batch 61600 of 127656\n",
            "Mean loss: 0.18057113302405925. Batch 61700 of 127656\n",
            "Mean loss: 0.07572653822135181. Batch 61800 of 127656\n",
            "Mean loss: 0.08551711686886847. Batch 61900 of 127656\n",
            "Mean loss: 0.16899942588526756. Batch 62000 of 127656\n",
            "Mean loss: 0.06901424893643707. Batch 62100 of 127656\n",
            "Mean loss: 0.052293056696653364. Batch 62200 of 127656\n",
            "Mean loss: 0.09084153804928065. Batch 62300 of 127656\n",
            "Mean loss: 0.0680626188358292. Batch 62400 of 127656\n",
            "Mean loss: 0.07169154858216643. Batch 62500 of 127656\n",
            "Mean loss: 0.08601438336074352. Batch 62600 of 127656\n",
            "Mean loss: 0.11717008039355278. Batch 62700 of 127656\n",
            "Mean loss: 0.1636660550069064. Batch 62800 of 127656\n",
            "Mean loss: 0.11211996332742274. Batch 62900 of 127656\n",
            "Mean loss: 0.08739137639757245. Batch 63000 of 127656\n",
            "Mean loss: 0.08470579667948186. Batch 63100 of 127656\n",
            "Mean loss: 0.11228036814834923. Batch 63200 of 127656\n",
            "Mean loss: 0.06635242232587188. Batch 63300 of 127656\n",
            "Mean loss: 0.0758195549948141. Batch 63400 of 127656\n",
            "Mean loss: 0.07783912484068424. Batch 63500 of 127656\n",
            "Mean loss: 0.10649388760793954. Batch 63600 of 127656\n",
            "Mean loss: 0.11314121779520064. Batch 63700 of 127656\n",
            "Mean loss: 0.10273099702317268. Batch 63800 of 127656\n",
            "Mean loss: 0.147231932785362. Batch 63900 of 127656\n",
            "Mean loss: 0.12587783536408095. Batch 64000 of 127656\n",
            "Mean loss: 0.1447550874296576. Batch 64100 of 127656\n",
            "Mean loss: 0.10160020200535655. Batch 64200 of 127656\n",
            "Mean loss: 0.055829302784986795. Batch 64300 of 127656\n",
            "Mean loss: 0.07390876565594226. Batch 64400 of 127656\n",
            "Mean loss: 0.12491233476903289. Batch 64500 of 127656\n",
            "Mean loss: 0.12265608062967658. Batch 64600 of 127656\n",
            "Mean loss: 0.08604941269848496. Batch 64700 of 127656\n",
            "Mean loss: 0.06977990149287507. Batch 64800 of 127656\n",
            "Mean loss: 0.10498514340026305. Batch 64900 of 127656\n",
            "Mean loss: 0.04409621765371412. Batch 65000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.07762829642510041. Batch 65100 of 127656\n",
            "Mean loss: 0.13167126572690904. Batch 65200 of 127656\n",
            "Mean loss: 0.10721398303285241. Batch 65300 of 127656\n",
            "Mean loss: 0.08524746041744947. Batch 65400 of 127656\n",
            "Mean loss: 0.13493658191058786. Batch 65500 of 127656\n",
            "Mean loss: 0.1428269775933586. Batch 65600 of 127656\n",
            "Mean loss: 0.062414711038582024. Batch 65700 of 127656\n",
            "Mean loss: 0.07333456716965885. Batch 65800 of 127656\n",
            "Mean loss: 0.13157836333382875. Batch 65900 of 127656\n",
            "Mean loss: 0.09076315029989929. Batch 66000 of 127656\n",
            "Mean loss: 0.07141423443797976. Batch 66100 of 127656\n",
            "Mean loss: 0.057780476317275314. Batch 66200 of 127656\n",
            "Mean loss: 0.0695554813160561. Batch 66300 of 127656\n",
            "Mean loss: 0.06360796081833542. Batch 66400 of 127656\n",
            "Mean loss: 0.08520922594936564. Batch 66500 of 127656\n",
            "Mean loss: 0.10182233866304159. Batch 66600 of 127656\n",
            "Mean loss: 0.12885257793590427. Batch 66700 of 127656\n",
            "Mean loss: 0.05178073280490935. Batch 66800 of 127656\n",
            "Mean loss: 0.06843263757182286. Batch 66900 of 127656\n",
            "Mean loss: 0.10061509469058365. Batch 67000 of 127656\n",
            "Mean loss: 0.0697865386516787. Batch 67100 of 127656\n",
            "Mean loss: 0.10749859602190553. Batch 67200 of 127656\n",
            "Mean loss: 0.11847752748290077. Batch 67300 of 127656\n",
            "Mean loss: 0.11794775566086173. Batch 67400 of 127656\n",
            "Mean loss: 0.11041567753069102. Batch 67500 of 127656\n",
            "Mean loss: 0.0886623390368186. Batch 67600 of 127656\n",
            "Mean loss: 0.06932538139401004. Batch 67700 of 127656\n",
            "Mean loss: 0.12243775446433575. Batch 67800 of 127656\n",
            "Mean loss: 0.098706495820079. Batch 67900 of 127656\n",
            "Mean loss: 0.06789337346563115. Batch 68000 of 127656\n",
            "Mean loss: 0.0902820805949159. Batch 68100 of 127656\n",
            "Mean loss: 0.06874911557883025. Batch 68200 of 127656\n",
            "Mean loss: 0.06496929835295305. Batch 68300 of 127656\n",
            "Mean loss: 0.09708795699523762. Batch 68400 of 127656\n",
            "Mean loss: 0.10548981967847794. Batch 68500 of 127656\n",
            "Mean loss: 0.08957277859095485. Batch 68600 of 127656\n",
            "Mean loss: 0.03440437026089058. Batch 68700 of 127656\n",
            "Mean loss: 0.09311062280554325. Batch 68800 of 127656\n",
            "Mean loss: 0.11684435832779855. Batch 68900 of 127656\n",
            "Mean loss: 0.05913941854145378. Batch 69000 of 127656\n",
            "Mean loss: 0.09252424352336674. Batch 69100 of 127656\n",
            "Mean loss: 0.09166568238753825. Batch 69200 of 127656\n",
            "Mean loss: 0.0809034539759159. Batch 69300 of 127656\n",
            "Mean loss: 0.0839280259073712. Batch 69400 of 127656\n",
            "Mean loss: 0.09663807750679552. Batch 69500 of 127656\n",
            "Mean loss: 0.0765610062982887. Batch 69600 of 127656\n",
            "Mean loss: 0.14615343287121504. Batch 69700 of 127656\n",
            "Mean loss: 0.048317389774601906. Batch 69800 of 127656\n",
            "Mean loss: 0.05903641921002418. Batch 69900 of 127656\n",
            "Mean loss: 0.0520589525741525. Batch 70000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.16571602352429182. Batch 70100 of 127656\n",
            "Mean loss: 0.10055457794573158. Batch 70200 of 127656\n",
            "Mean loss: 0.123282492114231. Batch 70300 of 127656\n",
            "Mean loss: 0.08600755691993982. Batch 70400 of 127656\n",
            "Mean loss: 0.0883701226580888. Batch 70500 of 127656\n",
            "Mean loss: 0.15293644121382385. Batch 70600 of 127656\n",
            "Mean loss: 0.05289651183178648. Batch 70700 of 127656\n",
            "Mean loss: 0.060802500729914756. Batch 70800 of 127656\n",
            "Mean loss: 0.13683512568240985. Batch 70900 of 127656\n",
            "Mean loss: 0.09838712609838694. Batch 71000 of 127656\n",
            "Mean loss: 0.07839826903771609. Batch 71100 of 127656\n",
            "Mean loss: 0.08224406809546053. Batch 71200 of 127656\n",
            "Mean loss: 0.18235348157584666. Batch 71300 of 127656\n",
            "Mean loss: 0.07153701356146484. Batch 71400 of 127656\n",
            "Mean loss: 0.11961313671665266. Batch 71500 of 127656\n",
            "Mean loss: 0.11467280192766338. Batch 71600 of 127656\n",
            "Mean loss: 0.09553025346715004. Batch 71700 of 127656\n",
            "Mean loss: 0.11907757091568782. Batch 71800 of 127656\n",
            "Mean loss: 0.07620796784292906. Batch 71900 of 127656\n",
            "Mean loss: 0.03736698223277926. Batch 72000 of 127656\n",
            "Mean loss: 0.14746996894478798. Batch 72100 of 127656\n",
            "Mean loss: 0.11946463253814726. Batch 72200 of 127656\n",
            "Mean loss: 0.11351198982913047. Batch 72300 of 127656\n",
            "Mean loss: 0.08510864183306693. Batch 72400 of 127656\n",
            "Mean loss: 0.06684228926897048. Batch 72500 of 127656\n",
            "Mean loss: 0.07274026079569013. Batch 72600 of 127656\n",
            "Mean loss: 0.09348872353788465. Batch 72700 of 127656\n",
            "Mean loss: 0.10261952998582274. Batch 72800 of 127656\n",
            "Mean loss: 0.12017430228181183. Batch 72900 of 127656\n",
            "Mean loss: 0.057378962102811785. Batch 73000 of 127656\n",
            "Mean loss: 0.12937371775275097. Batch 73100 of 127656\n",
            "Mean loss: 0.16790422103367747. Batch 73200 of 127656\n",
            "Mean loss: 0.07801973586203531. Batch 73300 of 127656\n",
            "Mean loss: 0.07232079265872017. Batch 73400 of 127656\n",
            "Mean loss: 0.07238148741191253. Batch 73500 of 127656\n",
            "Mean loss: 0.07968900348525494. Batch 73600 of 127656\n",
            "Mean loss: 0.08068287880858406. Batch 73700 of 127656\n",
            "Mean loss: 0.06304962298134342. Batch 73800 of 127656\n",
            "Mean loss: 0.09647759405197576. Batch 73900 of 127656\n",
            "Mean loss: 0.1109422951284796. Batch 74000 of 127656\n",
            "Mean loss: 0.07368892708560452. Batch 74100 of 127656\n",
            "Mean loss: 0.0606976648978889. Batch 74200 of 127656\n",
            "Mean loss: 0.10497217744588852. Batch 74300 of 127656\n",
            "Mean loss: 0.061227609957568346. Batch 74400 of 127656\n",
            "Mean loss: 0.1435276546003297. Batch 74500 of 127656\n",
            "Mean loss: 0.11232137084007263. Batch 74600 of 127656\n",
            "Mean loss: 0.07018035128479823. Batch 74700 of 127656\n",
            "Mean loss: 0.040514920689165596. Batch 74800 of 127656\n",
            "Mean loss: 0.12188469487708062. Batch 74900 of 127656\n",
            "Mean loss: 0.10075069594196975. Batch 75000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.09840641010086984. Batch 75100 of 127656\n",
            "Mean loss: 0.10386965029872954. Batch 75200 of 127656\n",
            "Mean loss: 0.08493059978820383. Batch 75300 of 127656\n",
            "Mean loss: 0.07703746516257524. Batch 75400 of 127656\n",
            "Mean loss: 0.10302180868806317. Batch 75500 of 127656\n",
            "Mean loss: 0.07837145567405969. Batch 75600 of 127656\n",
            "Mean loss: 0.04871392959728837. Batch 75700 of 127656\n",
            "Mean loss: 0.13368037558626383. Batch 75800 of 127656\n",
            "Mean loss: 0.07454098732909188. Batch 75900 of 127656\n",
            "Mean loss: 0.0909857404208742. Batch 76000 of 127656\n",
            "Mean loss: 0.056531956361141056. Batch 76100 of 127656\n",
            "Mean loss: 0.08977698421571403. Batch 76200 of 127656\n",
            "Mean loss: 0.06509481468936429. Batch 76300 of 127656\n",
            "Mean loss: 0.1004587004892528. Batch 76400 of 127656\n",
            "Mean loss: 0.08678651138441637. Batch 76500 of 127656\n",
            "Mean loss: 0.07906484144739806. Batch 76600 of 127656\n",
            "Mean loss: 0.08793454648926854. Batch 76700 of 127656\n",
            "Mean loss: 0.09035337968496605. Batch 76800 of 127656\n",
            "Mean loss: 0.15584322807611897. Batch 76900 of 127656\n",
            "Mean loss: 0.10433262436650693. Batch 77000 of 127656\n",
            "Mean loss: 0.06683583779260516. Batch 77100 of 127656\n",
            "Mean loss: 0.05326471502892673. Batch 77200 of 127656\n",
            "Mean loss: 0.12152222408680245. Batch 77300 of 127656\n",
            "Mean loss: 0.11208689772989601. Batch 77400 of 127656\n",
            "Mean loss: 0.0579354976140894. Batch 77500 of 127656\n",
            "Mean loss: 0.115344652866479. Batch 77600 of 127656\n",
            "Mean loss: 0.11613865646999329. Batch 77700 of 127656\n",
            "Mean loss: 0.13177489915629848. Batch 77800 of 127656\n",
            "Mean loss: 0.06945959992008284. Batch 77900 of 127656\n",
            "Mean loss: 0.06340634866151959. Batch 78000 of 127656\n",
            "Mean loss: 0.09218235600506887. Batch 78100 of 127656\n",
            "Mean loss: 0.12683646130142734. Batch 78200 of 127656\n",
            "Mean loss: 0.0737753421231173. Batch 78300 of 127656\n",
            "Mean loss: 0.08429879626259208. Batch 78400 of 127656\n",
            "Mean loss: 0.0659418726223521. Batch 78500 of 127656\n",
            "Mean loss: 0.09730353667400778. Batch 78600 of 127656\n",
            "Mean loss: 0.09653778860345483. Batch 78700 of 127656\n",
            "Mean loss: 0.1352953786822036. Batch 78800 of 127656\n",
            "Mean loss: 0.09110719824209809. Batch 78900 of 127656\n",
            "Mean loss: 0.13266979228472336. Batch 79000 of 127656\n",
            "Mean loss: 0.11752442290773615. Batch 79100 of 127656\n",
            "Mean loss: 0.05379315821453929. Batch 79200 of 127656\n",
            "Mean loss: 0.029064909184817224. Batch 79300 of 127656\n",
            "Mean loss: 0.0586630281386897. Batch 79400 of 127656\n",
            "Mean loss: 0.0474200618150644. Batch 79500 of 127656\n",
            "Mean loss: 0.11071504564024508. Batch 79600 of 127656\n",
            "Mean loss: 0.09264397346880288. Batch 79700 of 127656\n",
            "Mean loss: 0.05999236963223666. Batch 79800 of 127656\n",
            "Mean loss: 0.19080806970829145. Batch 79900 of 127656\n",
            "Mean loss: 0.07503180640982464. Batch 80000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.10600485610077158. Batch 80100 of 127656\n",
            "Mean loss: 0.09930417985888199. Batch 80200 of 127656\n",
            "Mean loss: 0.11274336038157344. Batch 80300 of 127656\n",
            "Mean loss: 0.10892812735168263. Batch 80400 of 127656\n",
            "Mean loss: 0.17760275424225255. Batch 80500 of 127656\n",
            "Mean loss: 0.09286051502684131. Batch 80600 of 127656\n",
            "Mean loss: 0.05279199710348621. Batch 80700 of 127656\n",
            "Mean loss: 0.09955013992730528. Batch 80800 of 127656\n",
            "Mean loss: 0.09179472770309076. Batch 80900 of 127656\n",
            "Mean loss: 0.11841007124632597. Batch 81000 of 127656\n",
            "Mean loss: 0.13117180010071025. Batch 81100 of 127656\n",
            "Mean loss: 0.06343922554515302. Batch 81200 of 127656\n",
            "Mean loss: 0.06847716711694375. Batch 81300 of 127656\n",
            "Mean loss: 0.05998833403689787. Batch 81400 of 127656\n",
            "Mean loss: 0.10239131502108649. Batch 81500 of 127656\n",
            "Mean loss: 0.14864904311252758. Batch 81600 of 127656\n",
            "Mean loss: 0.05844548319000751. Batch 81700 of 127656\n",
            "Mean loss: 0.09322227276628837. Batch 81800 of 127656\n",
            "Mean loss: 0.13351770498091356. Batch 81900 of 127656\n",
            "Mean loss: 0.04953642793931067. Batch 82000 of 127656\n",
            "Mean loss: 0.10177095963386819. Batch 82100 of 127656\n",
            "Mean loss: 0.09506030125543476. Batch 82200 of 127656\n",
            "Mean loss: 0.05558159195119515. Batch 82300 of 127656\n",
            "Mean loss: 0.04623761779395864. Batch 82400 of 127656\n",
            "Mean loss: 0.07195061274105682. Batch 82500 of 127656\n",
            "Mean loss: 0.05871478231623769. Batch 82600 of 127656\n",
            "Mean loss: 0.11787538665346801. Batch 82700 of 127656\n",
            "Mean loss: 0.07071540144272148. Batch 82800 of 127656\n",
            "Mean loss: 0.14281826096121222. Batch 82900 of 127656\n",
            "Mean loss: 0.1358827454596758. Batch 83000 of 127656\n",
            "Mean loss: 0.10552801861893385. Batch 83100 of 127656\n",
            "Mean loss: 0.07512945284601301. Batch 83200 of 127656\n",
            "Mean loss: 0.12205411589471624. Batch 83300 of 127656\n",
            "Mean loss: 0.1238878638902679. Batch 83400 of 127656\n",
            "Mean loss: 0.07298424344044178. Batch 83500 of 127656\n",
            "Mean loss: 0.08962919848039747. Batch 83600 of 127656\n",
            "Mean loss: 0.09966797415167093. Batch 83700 of 127656\n",
            "Mean loss: 0.08955330212134868. Batch 83800 of 127656\n",
            "Mean loss: 0.10590033336542547. Batch 83900 of 127656\n",
            "Mean loss: 0.06445930835092441. Batch 84000 of 127656\n",
            "Mean loss: 0.10971003851154819. Batch 84100 of 127656\n",
            "Mean loss: 0.08749567253515124. Batch 84200 of 127656\n",
            "Mean loss: 0.08495086070150137. Batch 84300 of 127656\n",
            "Mean loss: 0.059220253087114545. Batch 84400 of 127656\n",
            "Mean loss: 0.08915640728548169. Batch 84500 of 127656\n",
            "Mean loss: 0.03245868297293782. Batch 84600 of 127656\n",
            "Mean loss: 0.0572917330218479. Batch 84700 of 127656\n",
            "Mean loss: 0.09006476966431365. Batch 84800 of 127656\n",
            "Mean loss: 0.09983176174107938. Batch 84900 of 127656\n",
            "Mean loss: 0.07408067123498768. Batch 85000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.08538677796255797. Batch 85100 of 127656\n",
            "Mean loss: 0.0501103998045437. Batch 85200 of 127656\n",
            "Mean loss: 0.029177216221578418. Batch 85300 of 127656\n",
            "Mean loss: 0.11162548805819825. Batch 85400 of 127656\n",
            "Mean loss: 0.09509378694463522. Batch 85500 of 127656\n",
            "Mean loss: 0.13190023556118832. Batch 85600 of 127656\n",
            "Mean loss: 0.08530256755882874. Batch 85700 of 127656\n",
            "Mean loss: 0.09796044087968767. Batch 85800 of 127656\n",
            "Mean loss: 0.14534249126445503. Batch 85900 of 127656\n",
            "Mean loss: 0.10084255240857601. Batch 86000 of 127656\n",
            "Mean loss: 0.07624164698412642. Batch 86100 of 127656\n",
            "Mean loss: 0.07394454212859274. Batch 86200 of 127656\n",
            "Mean loss: 0.14731803262373433. Batch 86300 of 127656\n",
            "Mean loss: 0.11868609916651622. Batch 86400 of 127656\n",
            "Mean loss: 0.07479239643085749. Batch 86500 of 127656\n",
            "Mean loss: 0.08743674959288911. Batch 86600 of 127656\n",
            "Mean loss: 0.043666824586689475. Batch 86700 of 127656\n",
            "Mean loss: 0.1342532641813159. Batch 86800 of 127656\n",
            "Mean loss: 0.12772040859796108. Batch 86900 of 127656\n",
            "Mean loss: 0.11278537495061755. Batch 87000 of 127656\n",
            "Mean loss: 0.0827905952814035. Batch 87100 of 127656\n",
            "Mean loss: 0.045872925778385254. Batch 87200 of 127656\n",
            "Mean loss: 0.12034903550054878. Batch 87300 of 127656\n",
            "Mean loss: 0.10225684210192412. Batch 87400 of 127656\n",
            "Mean loss: 0.08825275504263118. Batch 87500 of 127656\n",
            "Mean loss: 0.10941146360011772. Batch 87600 of 127656\n",
            "Mean loss: 0.08074268799042329. Batch 87700 of 127656\n",
            "Mean loss: 0.10949663006700576. Batch 87800 of 127656\n",
            "Mean loss: 0.09103208052227274. Batch 87900 of 127656\n",
            "Mean loss: 0.134089735976886. Batch 88000 of 127656\n",
            "Mean loss: 0.10621631996007636. Batch 88100 of 127656\n",
            "Mean loss: 0.05199042703490704. Batch 88200 of 127656\n",
            "Mean loss: 0.14299253433709963. Batch 88300 of 127656\n",
            "Mean loss: 0.12039880402386188. Batch 88400 of 127656\n",
            "Mean loss: 0.13112618206534535. Batch 88500 of 127656\n",
            "Mean loss: 0.07986803660169244. Batch 88600 of 127656\n",
            "Mean loss: 0.06065482029924169. Batch 88700 of 127656\n",
            "Mean loss: 0.10735856841318309. Batch 88800 of 127656\n",
            "Mean loss: 0.10594621859723702. Batch 88900 of 127656\n",
            "Mean loss: 0.06280638015363366. Batch 89000 of 127656\n",
            "Mean loss: 0.09945770125836134. Batch 89100 of 127656\n",
            "Mean loss: 0.050494681724812834. Batch 89200 of 127656\n",
            "Mean loss: 0.10981696904404088. Batch 89300 of 127656\n",
            "Mean loss: 0.07550444101216272. Batch 89400 of 127656\n",
            "Mean loss: 0.10611782514955849. Batch 89500 of 127656\n",
            "Mean loss: 0.06727937352610752. Batch 89600 of 127656\n",
            "Mean loss: 0.0944720559869893. Batch 89700 of 127656\n",
            "Mean loss: 0.05497554434230551. Batch 89800 of 127656\n",
            "Mean loss: 0.0941707391035743. Batch 89900 of 127656\n",
            "Mean loss: 0.11485058065038174. Batch 90000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.10499325727811083. Batch 90100 of 127656\n",
            "Mean loss: 0.11918512731092051. Batch 90200 of 127656\n",
            "Mean loss: 0.07237934514181689. Batch 90300 of 127656\n",
            "Mean loss: 0.09705571158323437. Batch 90400 of 127656\n",
            "Mean loss: 0.10251507912762463. Batch 90500 of 127656\n",
            "Mean loss: 0.05269690872170031. Batch 90600 of 127656\n",
            "Mean loss: 0.13567691924283282. Batch 90700 of 127656\n",
            "Mean loss: 0.04980952061712742. Batch 90800 of 127656\n",
            "Mean loss: 0.0878847614908591. Batch 90900 of 127656\n",
            "Mean loss: 0.07687684843083843. Batch 91000 of 127656\n",
            "Mean loss: 0.12774366514524446. Batch 91100 of 127656\n",
            "Mean loss: 0.08669432301539928. Batch 91200 of 127656\n",
            "Mean loss: 0.1289556703181006. Batch 91300 of 127656\n",
            "Mean loss: 0.047803797002416104. Batch 91400 of 127656\n",
            "Mean loss: 0.07111193246673793. Batch 91500 of 127656\n",
            "Mean loss: 0.0984456881065853. Batch 91600 of 127656\n",
            "Mean loss: 0.12614810889353975. Batch 91700 of 127656\n",
            "Mean loss: 0.05671467437641695. Batch 91800 of 127656\n",
            "Mean loss: 0.1114501471654512. Batch 91900 of 127656\n",
            "Mean loss: 0.1129942295141518. Batch 92000 of 127656\n",
            "Mean loss: 0.10314494106918573. Batch 92100 of 127656\n",
            "Mean loss: 0.12226991587551311. Batch 92200 of 127656\n",
            "Mean loss: 0.11051146508194507. Batch 92300 of 127656\n",
            "Mean loss: 0.11448345008306206. Batch 92400 of 127656\n",
            "Mean loss: 0.06407913448289036. Batch 92500 of 127656\n",
            "Mean loss: 0.10070956217241474. Batch 92600 of 127656\n",
            "Mean loss: 0.07399034511996433. Batch 92700 of 127656\n",
            "Mean loss: 0.10206833919044583. Batch 92800 of 127656\n",
            "Mean loss: 0.09212275806814432. Batch 92900 of 127656\n",
            "Mean loss: 0.1070062207058072. Batch 93000 of 127656\n",
            "Mean loss: 0.09674774617655203. Batch 93100 of 127656\n",
            "Mean loss: 0.05686245762044564. Batch 93200 of 127656\n",
            "Mean loss: 0.05515248536830768. Batch 93300 of 127656\n",
            "Mean loss: 0.044151913817040624. Batch 93400 of 127656\n",
            "Mean loss: 0.07197812869446352. Batch 93500 of 127656\n",
            "Mean loss: 0.08243349271360785. Batch 93600 of 127656\n",
            "Mean loss: 0.04772513955365867. Batch 93700 of 127656\n",
            "Mean loss: 0.10427639989880845. Batch 93800 of 127656\n",
            "Mean loss: 0.07965971131809056. Batch 93900 of 127656\n",
            "Mean loss: 0.08698055677348748. Batch 94000 of 127656\n",
            "Mean loss: 0.06776885108323767. Batch 94100 of 127656\n",
            "Mean loss: 0.05357334494590759. Batch 94200 of 127656\n",
            "Mean loss: 0.12058436827734113. Batch 94300 of 127656\n",
            "Mean loss: 0.05767426933627576. Batch 94400 of 127656\n",
            "Mean loss: 0.10759950256207958. Batch 94500 of 127656\n",
            "Mean loss: 0.0646796934097074. Batch 94600 of 127656\n",
            "Mean loss: 0.09945785601856187. Batch 94700 of 127656\n",
            "Mean loss: 0.07181254474213347. Batch 94800 of 127656\n",
            "Mean loss: 0.09865525657543912. Batch 94900 of 127656\n",
            "Mean loss: 0.07544364857953041. Batch 95000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.0670746586099267. Batch 95100 of 127656\n",
            "Mean loss: 0.08356642302591354. Batch 95200 of 127656\n",
            "Mean loss: 0.06315681795589626. Batch 95300 of 127656\n",
            "Mean loss: 0.0790781164332293. Batch 95400 of 127656\n",
            "Mean loss: 0.07790568308904768. Batch 95500 of 127656\n",
            "Mean loss: 0.09149503298569471. Batch 95600 of 127656\n",
            "Mean loss: 0.07953113622497768. Batch 95700 of 127656\n",
            "Mean loss: 0.08052927484735846. Batch 95800 of 127656\n",
            "Mean loss: 0.12755122043425218. Batch 95900 of 127656\n",
            "Mean loss: 0.11435668029123917. Batch 96000 of 127656\n",
            "Mean loss: 0.09243781120982021. Batch 96100 of 127656\n",
            "Mean loss: 0.08618958218488842. Batch 96200 of 127656\n",
            "Mean loss: 0.06309196185320616. Batch 96300 of 127656\n",
            "Mean loss: 0.09121998464688659. Batch 96400 of 127656\n",
            "Mean loss: 0.04980844517704099. Batch 96500 of 127656\n",
            "Mean loss: 0.09154975500656291. Batch 96600 of 127656\n",
            "Mean loss: 0.06662092345068231. Batch 96700 of 127656\n",
            "Mean loss: 0.12678306827321648. Batch 96800 of 127656\n",
            "Mean loss: 0.07668469081167131. Batch 96900 of 127656\n",
            "Mean loss: 0.07032656945288181. Batch 97000 of 127656\n",
            "Mean loss: 0.0717359150853008. Batch 97100 of 127656\n",
            "Mean loss: 0.0995985366916284. Batch 97200 of 127656\n",
            "Mean loss: 0.1277178778522648. Batch 97300 of 127656\n",
            "Mean loss: 0.08478632288053632. Batch 97400 of 127656\n",
            "Mean loss: 0.09025369846727699. Batch 97500 of 127656\n",
            "Mean loss: 0.08706697449553757. Batch 97600 of 127656\n",
            "Mean loss: 0.12845545125426724. Batch 97700 of 127656\n",
            "Mean loss: 0.11314713290892542. Batch 97800 of 127656\n",
            "Mean loss: 0.0849736053077504. Batch 97900 of 127656\n",
            "Mean loss: 0.12715159769635648. Batch 98000 of 127656\n",
            "Mean loss: 0.09196859627962112. Batch 98100 of 127656\n",
            "Mean loss: 0.03067083595553413. Batch 98200 of 127656\n",
            "Mean loss: 0.07655735610751435. Batch 98300 of 127656\n",
            "Mean loss: 0.1286027837311849. Batch 98400 of 127656\n",
            "Mean loss: 0.09712228383636103. Batch 98500 of 127656\n",
            "Mean loss: 0.06414720101514831. Batch 98600 of 127656\n",
            "Mean loss: 0.10281767105218023. Batch 98700 of 127656\n",
            "Mean loss: 0.08825244683539495. Batch 98800 of 127656\n",
            "Mean loss: 0.08747937321197241. Batch 98900 of 127656\n",
            "Mean loss: 0.10333740955218673. Batch 99000 of 127656\n",
            "Mean loss: 0.09125865347450599. Batch 99100 of 127656\n",
            "Mean loss: 0.0869833729043603. Batch 99200 of 127656\n",
            "Mean loss: 0.0719913486461155. Batch 99300 of 127656\n",
            "Mean loss: 0.10831174765946344. Batch 99400 of 127656\n",
            "Mean loss: 0.0760885749827139. Batch 99500 of 127656\n",
            "Mean loss: 0.07271538942819461. Batch 99600 of 127656\n",
            "Mean loss: 0.056392237863037736. Batch 99700 of 127656\n",
            "Mean loss: 0.09571939694229513. Batch 99800 of 127656\n",
            "Mean loss: 0.13008073577424512. Batch 99900 of 127656\n",
            "Mean loss: 0.057950911908410487. Batch 100000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.12388667046558112. Batch 100100 of 127656\n",
            "Mean loss: 0.05720081079285592. Batch 100200 of 127656\n",
            "Mean loss: 0.06661625790176913. Batch 100300 of 127656\n",
            "Mean loss: 0.10873649788554758. Batch 100400 of 127656\n",
            "Mean loss: 0.10447144623845815. Batch 100500 of 127656\n",
            "Mean loss: 0.07021156673552469. Batch 100600 of 127656\n",
            "Mean loss: 0.052263092999346554. Batch 100700 of 127656\n",
            "Mean loss: 0.09286221357062459. Batch 100800 of 127656\n",
            "Mean loss: 0.06881706100190059. Batch 100900 of 127656\n",
            "Mean loss: 0.11745153513504192. Batch 101000 of 127656\n",
            "Mean loss: 0.08576496639754623. Batch 101100 of 127656\n",
            "Mean loss: 0.09430729642510415. Batch 101200 of 127656\n",
            "Mean loss: 0.09484231312293559. Batch 101300 of 127656\n",
            "Mean loss: 0.10459877390181646. Batch 101400 of 127656\n",
            "Mean loss: 0.09185154776321725. Batch 101500 of 127656\n",
            "Mean loss: 0.04756724340608343. Batch 101600 of 127656\n",
            "Mean loss: 0.09682997237658128. Batch 101700 of 127656\n",
            "Mean loss: 0.0531136599346064. Batch 101800 of 127656\n",
            "Mean loss: 0.13090550672844983. Batch 101900 of 127656\n",
            "Mean loss: 0.04136860288446769. Batch 102000 of 127656\n",
            "Mean loss: 0.06542168133193627. Batch 102100 of 127656\n",
            "Mean loss: 0.07646077456185595. Batch 102200 of 127656\n",
            "Mean loss: 0.057196249200496825. Batch 102300 of 127656\n",
            "Mean loss: 0.06479781522299163. Batch 102400 of 127656\n",
            "Mean loss: 0.05621642042300664. Batch 102500 of 127656\n",
            "Mean loss: 0.06539522863691673. Batch 102600 of 127656\n",
            "Mean loss: 0.06987953408621252. Batch 102700 of 127656\n",
            "Mean loss: 0.04461216481868178. Batch 102800 of 127656\n",
            "Mean loss: 0.07714849181706086. Batch 102900 of 127656\n",
            "Mean loss: 0.09313714870135299. Batch 103000 of 127656\n",
            "Mean loss: 0.07203895025886595. Batch 103100 of 127656\n",
            "Mean loss: 0.09451259311754256. Batch 103200 of 127656\n",
            "Mean loss: 0.08156107454560697. Batch 103300 of 127656\n",
            "Mean loss: 0.023970220948103815. Batch 103400 of 127656\n",
            "Mean loss: 0.09647212100448087. Batch 103500 of 127656\n",
            "Mean loss: 0.09111923516727984. Batch 103600 of 127656\n",
            "Mean loss: 0.05354860215447843. Batch 103700 of 127656\n",
            "Mean loss: 0.07460858904756605. Batch 103800 of 127656\n",
            "Mean loss: 0.08730414607562124. Batch 103900 of 127656\n",
            "Mean loss: 0.08293168827192858. Batch 104000 of 127656\n",
            "Mean loss: 0.06899377703666687. Batch 104100 of 127656\n",
            "Mean loss: 0.1224159001163207. Batch 104200 of 127656\n",
            "Mean loss: 0.1500617009657435. Batch 104300 of 127656\n",
            "Mean loss: 0.060238748929696155. Batch 104400 of 127656\n",
            "Mean loss: 0.07346890244167298. Batch 104500 of 127656\n",
            "Mean loss: 0.13485897996928542. Batch 104600 of 127656\n",
            "Mean loss: 0.08561845435760915. Batch 104700 of 127656\n",
            "Mean loss: 0.06911600542021916. Batch 104800 of 127656\n",
            "Mean loss: 0.05851215398870409. Batch 104900 of 127656\n",
            "Mean loss: 0.11821287175407633. Batch 105000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.0769443981233053. Batch 105100 of 127656\n",
            "Mean loss: 0.07008938824059442. Batch 105200 of 127656\n",
            "Mean loss: 0.07693435990950093. Batch 105300 of 127656\n",
            "Mean loss: 0.07843796238070354. Batch 105400 of 127656\n",
            "Mean loss: 0.08853571075713262. Batch 105500 of 127656\n",
            "Mean loss: 0.08846812322968617. Batch 105600 of 127656\n",
            "Mean loss: 0.10567327300086618. Batch 105700 of 127656\n",
            "Mean loss: 0.09036903085187077. Batch 105800 of 127656\n",
            "Mean loss: 0.09945115884533152. Batch 105900 of 127656\n",
            "Mean loss: 0.08391243706224487. Batch 106000 of 127656\n",
            "Mean loss: 0.11092192367184907. Batch 106100 of 127656\n",
            "Mean loss: 0.04883111952571198. Batch 106200 of 127656\n",
            "Mean loss: 0.09925151897361502. Batch 106300 of 127656\n",
            "Mean loss: 0.05433401090791449. Batch 106400 of 127656\n",
            "Mean loss: 0.062440941506065425. Batch 106500 of 127656\n",
            "Mean loss: 0.06832479434786365. Batch 106600 of 127656\n",
            "Mean loss: 0.11934004439506679. Batch 106700 of 127656\n",
            "Mean loss: 0.0898060480877757. Batch 106800 of 127656\n",
            "Mean loss: 0.0544861932983622. Batch 106900 of 127656\n",
            "Mean loss: 0.052557075426448135. Batch 107000 of 127656\n",
            "Mean loss: 0.08227603423874826. Batch 107100 of 127656\n",
            "Mean loss: 0.049536024287808685. Batch 107200 of 127656\n",
            "Mean loss: 0.0548390441457741. Batch 107300 of 127656\n",
            "Mean loss: 0.06449340643826872. Batch 107400 of 127656\n",
            "Mean loss: 0.06857348497374914. Batch 107500 of 127656\n",
            "Mean loss: 0.062497302057454365. Batch 107600 of 127656\n",
            "Mean loss: 0.09414184768451378. Batch 107700 of 127656\n",
            "Mean loss: 0.06634265902335755. Batch 107800 of 127656\n",
            "Mean loss: 0.1021362631150987. Batch 107900 of 127656\n",
            "Mean loss: 0.06809791337465868. Batch 108000 of 127656\n",
            "Mean loss: 0.10025977928191424. Batch 108100 of 127656\n",
            "Mean loss: 0.0600555793242529. Batch 108200 of 127656\n",
            "Mean loss: 0.1394194933678955. Batch 108300 of 127656\n",
            "Mean loss: 0.06080881665693596. Batch 108400 of 127656\n",
            "Mean loss: 0.0936506074294448. Batch 108500 of 127656\n",
            "Mean loss: 0.03188434132374823. Batch 108600 of 127656\n",
            "Mean loss: 0.11784700295422226. Batch 108700 of 127656\n",
            "Mean loss: 0.07622724523302167. Batch 108800 of 127656\n",
            "Mean loss: 0.11105561655247584. Batch 108900 of 127656\n",
            "Mean loss: 0.033669417342171076. Batch 109000 of 127656\n",
            "Mean loss: 0.07544502160511911. Batch 109100 of 127656\n",
            "Mean loss: 0.030314265680499376. Batch 109200 of 127656\n",
            "Mean loss: 0.0881751552876085. Batch 109300 of 127656\n",
            "Mean loss: 0.15961223612772302. Batch 109400 of 127656\n",
            "Mean loss: 0.09288665288127959. Batch 109500 of 127656\n",
            "Mean loss: 0.06264688851777464. Batch 109600 of 127656\n",
            "Mean loss: 0.07328511412255466. Batch 109700 of 127656\n",
            "Mean loss: 0.07807703077560291. Batch 109800 of 127656\n",
            "Mean loss: 0.059649053036700934. Batch 109900 of 127656\n",
            "Mean loss: 0.14722895759157836. Batch 110000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.11181242709280922. Batch 110100 of 127656\n",
            "Mean loss: 0.07024103720672428. Batch 110200 of 127656\n",
            "Mean loss: 0.0913299700547941. Batch 110300 of 127656\n",
            "Mean loss: 0.06858927847468294. Batch 110400 of 127656\n",
            "Mean loss: 0.07433647615252995. Batch 110500 of 127656\n",
            "Mean loss: 0.09017437335802242. Batch 110600 of 127656\n",
            "Mean loss: 0.09490494003053755. Batch 110700 of 127656\n",
            "Mean loss: 0.10704637909773737. Batch 110800 of 127656\n",
            "Mean loss: 0.039698592838831245. Batch 110900 of 127656\n",
            "Mean loss: 0.10086656128522009. Batch 111000 of 127656\n",
            "Mean loss: 0.12012504742015154. Batch 111100 of 127656\n",
            "Mean loss: 0.09182721549645066. Batch 111200 of 127656\n",
            "Mean loss: 0.08907246361719444. Batch 111300 of 127656\n",
            "Mean loss: 0.09734520324040204. Batch 111400 of 127656\n",
            "Mean loss: 0.15602385538630187. Batch 111500 of 127656\n",
            "Mean loss: 0.08232194246258587. Batch 111600 of 127656\n",
            "Mean loss: 0.07085151034872979. Batch 111700 of 127656\n",
            "Mean loss: 0.06356202063150704. Batch 111800 of 127656\n",
            "Mean loss: 0.11025864524533972. Batch 111900 of 127656\n",
            "Mean loss: 0.06826328387949615. Batch 112000 of 127656\n",
            "Mean loss: 0.09198245218023658. Batch 112100 of 127656\n",
            "Mean loss: 0.0813931672112085. Batch 112200 of 127656\n",
            "Mean loss: 0.061776759079657496. Batch 112300 of 127656\n",
            "Mean loss: 0.14925539383664727. Batch 112400 of 127656\n",
            "Mean loss: 0.09149248759262264. Batch 112500 of 127656\n",
            "Mean loss: 0.07126630199607462. Batch 112600 of 127656\n",
            "Mean loss: 0.09011018115561456. Batch 112700 of 127656\n",
            "Mean loss: 0.11988869578344748. Batch 112800 of 127656\n",
            "Mean loss: 0.1323629746073857. Batch 112900 of 127656\n",
            "Mean loss: 0.08569037029519677. Batch 113000 of 127656\n",
            "Mean loss: 0.041772241359576585. Batch 113100 of 127656\n",
            "Mean loss: 0.09970191293163225. Batch 113200 of 127656\n",
            "Mean loss: 0.0656396024953574. Batch 113300 of 127656\n",
            "Mean loss: 0.10799002529121936. Batch 113400 of 127656\n",
            "Mean loss: 0.1085369108384475. Batch 113500 of 127656\n",
            "Mean loss: 0.09021260481793433. Batch 113600 of 127656\n",
            "Mean loss: 0.10009501765016467. Batch 113700 of 127656\n",
            "Mean loss: 0.10529154488118365. Batch 113800 of 127656\n",
            "Mean loss: 0.1164303900883533. Batch 113900 of 127656\n",
            "Mean loss: 0.06816218743100762. Batch 114000 of 127656\n",
            "Mean loss: 0.09798490568529815. Batch 114100 of 127656\n",
            "Mean loss: 0.04352833826327696. Batch 114200 of 127656\n",
            "Mean loss: 0.11144817896420137. Batch 114300 of 127656\n",
            "Mean loss: 0.05677846170496195. Batch 114400 of 127656\n",
            "Mean loss: 0.09984022504882888. Batch 114500 of 127656\n",
            "Mean loss: 0.07259456526720896. Batch 114600 of 127656\n",
            "Mean loss: 0.06564518194179982. Batch 114700 of 127656\n",
            "Mean loss: 0.0720172797050327. Batch 114800 of 127656\n",
            "Mean loss: 0.08999504024861381. Batch 114900 of 127656\n",
            "Mean loss: 0.06541324459016323. Batch 115000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.09826146107632666. Batch 115100 of 127656\n",
            "Mean loss: 0.08168802390340715. Batch 115200 of 127656\n",
            "Mean loss: 0.0871600131620653. Batch 115300 of 127656\n",
            "Mean loss: 0.09517915835836903. Batch 115400 of 127656\n",
            "Mean loss: 0.11648977123200893. Batch 115500 of 127656\n",
            "Mean loss: 0.16920681057032197. Batch 115600 of 127656\n",
            "Mean loss: 0.13358059774385767. Batch 115700 of 127656\n",
            "Mean loss: 0.11186910582473501. Batch 115800 of 127656\n",
            "Mean loss: 0.057988109793514014. Batch 115900 of 127656\n",
            "Mean loss: 0.08001582371769472. Batch 116000 of 127656\n",
            "Mean loss: 0.07725491966120898. Batch 116100 of 127656\n",
            "Mean loss: 0.09711306589422747. Batch 116200 of 127656\n",
            "Mean loss: 0.05990234964992851. Batch 116300 of 127656\n",
            "Mean loss: 0.08515153985703364. Batch 116400 of 127656\n",
            "Mean loss: 0.10028491269797087. Batch 116500 of 127656\n",
            "Mean loss: 0.09555974632501602. Batch 116600 of 127656\n",
            "Mean loss: 0.08597281496273354. Batch 116700 of 127656\n",
            "Mean loss: 0.07184568220749497. Batch 116800 of 127656\n",
            "Mean loss: 0.07840682509588077. Batch 116900 of 127656\n",
            "Mean loss: 0.06748030518414452. Batch 117000 of 127656\n",
            "Mean loss: 0.09175407870439813. Batch 117100 of 127656\n",
            "Mean loss: 0.10567389639094472. Batch 117200 of 127656\n",
            "Mean loss: 0.07039277764037251. Batch 117300 of 127656\n",
            "Mean loss: 0.08531076490413397. Batch 117400 of 127656\n",
            "Mean loss: 0.08540442786645144. Batch 117500 of 127656\n",
            "Mean loss: 0.07700352444779128. Batch 117600 of 127656\n",
            "Mean loss: 0.11648248739074915. Batch 117700 of 127656\n",
            "Mean loss: 0.07993698600912466. Batch 117800 of 127656\n",
            "Mean loss: 0.05367582820123062. Batch 117900 of 127656\n",
            "Mean loss: 0.09616333351237699. Batch 118000 of 127656\n",
            "Mean loss: 0.08472244524396956. Batch 118100 of 127656\n",
            "Mean loss: 0.09312498796498403. Batch 118200 of 127656\n",
            "Mean loss: 0.07793493725592271. Batch 118300 of 127656\n",
            "Mean loss: 0.08142652852926403. Batch 118400 of 127656\n",
            "Mean loss: 0.11519373569870367. Batch 118500 of 127656\n",
            "Mean loss: 0.14011056227958762. Batch 118600 of 127656\n",
            "Mean loss: 0.10320982834557071. Batch 118700 of 127656\n",
            "Mean loss: 0.11036130148218945. Batch 118800 of 127656\n",
            "Mean loss: 0.0860502135695424. Batch 118900 of 127656\n",
            "Mean loss: 0.09568426802754403. Batch 119000 of 127656\n",
            "Mean loss: 0.056623281135689464. Batch 119100 of 127656\n",
            "Mean loss: 0.07008622190915048. Batch 119200 of 127656\n",
            "Mean loss: 0.07389915970619768. Batch 119300 of 127656\n",
            "Mean loss: 0.08374404545640574. Batch 119400 of 127656\n",
            "Mean loss: 0.06977134005166591. Batch 119500 of 127656\n",
            "Mean loss: 0.08027878657216206. Batch 119600 of 127656\n",
            "Mean loss: 0.0997880692454055. Batch 119700 of 127656\n",
            "Mean loss: 0.12114804555196315. Batch 119800 of 127656\n",
            "Mean loss: 0.06934725687373429. Batch 119900 of 127656\n",
            "Mean loss: 0.04833613691385835. Batch 120000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.07468746961094438. Batch 120100 of 127656\n",
            "Mean loss: 0.0938271072448697. Batch 120200 of 127656\n",
            "Mean loss: 0.07913285184418782. Batch 120300 of 127656\n",
            "Mean loss: 0.08377105213934556. Batch 120400 of 127656\n",
            "Mean loss: 0.11000322356354446. Batch 120500 of 127656\n",
            "Mean loss: 0.08914524192456157. Batch 120600 of 127656\n",
            "Mean loss: 0.0961837406666018. Batch 120700 of 127656\n",
            "Mean loss: 0.1015205493173562. Batch 120800 of 127656\n",
            "Mean loss: 0.1044957936683204. Batch 120900 of 127656\n",
            "Mean loss: 0.04292882025358267. Batch 121000 of 127656\n",
            "Mean loss: 0.1073053329566028. Batch 121100 of 127656\n",
            "Mean loss: 0.07871751213213429. Batch 121200 of 127656\n",
            "Mean loss: 0.1136067155271303. Batch 121300 of 127656\n",
            "Mean loss: 0.09406046509044245. Batch 121400 of 127656\n",
            "Mean loss: 0.1210582599870395. Batch 121500 of 127656\n",
            "Mean loss: 0.11088796156924217. Batch 121600 of 127656\n",
            "Mean loss: 0.08039083786774427. Batch 121700 of 127656\n",
            "Mean loss: 0.12024567057145759. Batch 121800 of 127656\n",
            "Mean loss: 0.06346105287084355. Batch 121900 of 127656\n",
            "Mean loss: 0.10413036913378164. Batch 122000 of 127656\n",
            "Mean loss: 0.11147953676059842. Batch 122100 of 127656\n",
            "Mean loss: 0.029982712893979625. Batch 122200 of 127656\n",
            "Mean loss: 0.04886526386835612. Batch 122300 of 127656\n",
            "Mean loss: 0.059420848039444536. Batch 122400 of 127656\n",
            "Mean loss: 0.08287226367159746. Batch 122500 of 127656\n",
            "Mean loss: 0.07839907484012656. Batch 122600 of 127656\n",
            "Mean loss: 0.11534182954696007. Batch 122700 of 127656\n",
            "Mean loss: 0.06939118098001927. Batch 122800 of 127656\n",
            "Mean loss: 0.09071384262526408. Batch 122900 of 127656\n",
            "Mean loss: 0.06807534234714695. Batch 123000 of 127656\n",
            "Mean loss: 0.07696321053546853. Batch 123100 of 127656\n",
            "Mean loss: 0.06291615737834945. Batch 123200 of 127656\n",
            "Mean loss: 0.16226215322036297. Batch 123300 of 127656\n",
            "Mean loss: 0.07616638816194608. Batch 123400 of 127656\n",
            "Mean loss: 0.13104290681891143. Batch 123500 of 127656\n",
            "Mean loss: 0.07848205606802366. Batch 123600 of 127656\n",
            "Mean loss: 0.0939761226286646. Batch 123700 of 127656\n",
            "Mean loss: 0.062153456240193916. Batch 123800 of 127656\n",
            "Mean loss: 0.10136762138805352. Batch 123900 of 127656\n",
            "Mean loss: 0.05074763256823644. Batch 124000 of 127656\n",
            "Mean loss: 0.10725970156956464. Batch 124100 of 127656\n",
            "Mean loss: 0.054621700139250606. Batch 124200 of 127656\n",
            "Mean loss: 0.06691527926013804. Batch 124300 of 127656\n",
            "Mean loss: 0.09243390936637297. Batch 124400 of 127656\n",
            "Mean loss: 0.062289122159127144. Batch 124500 of 127656\n",
            "Mean loss: 0.09627677353098989. Batch 124600 of 127656\n",
            "Mean loss: 0.0504507345170714. Batch 124700 of 127656\n",
            "Mean loss: 0.07310436764499173. Batch 124800 of 127656\n",
            "Mean loss: 0.07695129194296896. Batch 124900 of 127656\n",
            "Mean loss: 0.1083423516759649. Batch 125000 of 127656\n",
            "Checkpointing model...\n",
            "Mean loss: 0.09423835596302524. Batch 125100 of 127656\n",
            "Mean loss: 0.048041764132212846. Batch 125200 of 127656\n",
            "Mean loss: 0.029508858760818837. Batch 125300 of 127656\n",
            "Mean loss: 0.08751307123806328. Batch 125400 of 127656\n",
            "Mean loss: 0.13260128336492927. Batch 125500 of 127656\n",
            "Mean loss: 0.09342093590064905. Batch 125600 of 127656\n",
            "Mean loss: 0.05195172650157474. Batch 125700 of 127656\n",
            "Mean loss: 0.1252434497408103. Batch 125800 of 127656\n",
            "Mean loss: 0.06897559158969671. Batch 125900 of 127656\n",
            "Mean loss: 0.05888352933106944. Batch 126000 of 127656\n",
            "Mean loss: 0.07513967256993055. Batch 126100 of 127656\n",
            "Mean loss: 0.1345840676384978. Batch 126200 of 127656\n",
            "Mean loss: 0.07455258736852556. Batch 126300 of 127656\n",
            "Mean loss: 0.040114213316701355. Batch 126400 of 127656\n",
            "Mean loss: 0.07369251183350571. Batch 126500 of 127656\n",
            "Mean loss: 0.12258897274266928. Batch 126600 of 127656\n",
            "Mean loss: 0.078670990181854. Batch 126700 of 127656\n",
            "Mean loss: 0.10314208259340375. Batch 126800 of 127656\n",
            "Mean loss: 0.07239189499174245. Batch 126900 of 127656\n",
            "Mean loss: 0.11218287569237873. Batch 127000 of 127656\n",
            "Mean loss: 0.08743540244060569. Batch 127100 of 127656\n",
            "Mean loss: 0.08049694612738677. Batch 127200 of 127656\n",
            "Mean loss: 0.11492507426883095. Batch 127300 of 127656\n",
            "Mean loss: 0.10810610028565862. Batch 127400 of 127656\n",
            "Mean loss: 0.08019664361258037. Batch 127500 of 127656\n",
            "Mean loss: 0.06338147217640654. Batch 127600 of 127656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will evaluate this newly trained model on the first test set that we evaluated our fine-tuned model on."
      ],
      "metadata": {
        "id": "UPs9bEjflNJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test dataset\n",
        "test_dataset = ToxicityDataset(\"/content/gdrive/MyDrive/T5/assignment_test.csv\", tokenizer)"
      ],
      "metadata": {
        "id": "9hVi_Qbug8DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions with the new model\n",
        "model = model.to(\"cuda\")\n",
        "df_pred = generate_predictions(test_dataset, tokenizer, model)"
      ],
      "metadata": {
        "id": "sC5MRCijhs89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7fd01b-51b7-4bcb-dcde-73f76aa4dd82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 100 of 31915\n",
            "Batch 200 of 31915\n",
            "Batch 300 of 31915\n",
            "Batch 400 of 31915\n",
            "Batch 500 of 31915\n",
            "Batch 600 of 31915\n",
            "Batch 700 of 31915\n",
            "Batch 800 of 31915\n",
            "Batch 900 of 31915\n",
            "Batch 1000 of 31915\n",
            "Batch 1100 of 31915\n",
            "Batch 1200 of 31915\n",
            "Batch 1300 of 31915\n",
            "Batch 1400 of 31915\n",
            "Batch 1500 of 31915\n",
            "Batch 1600 of 31915\n",
            "Batch 1700 of 31915\n",
            "Batch 1800 of 31915\n",
            "Batch 1900 of 31915\n",
            "Batch 2000 of 31915\n",
            "Batch 2100 of 31915\n",
            "Batch 2200 of 31915\n",
            "Batch 2300 of 31915\n",
            "Batch 2400 of 31915\n",
            "Batch 2500 of 31915\n",
            "Batch 2600 of 31915\n",
            "Batch 2700 of 31915\n",
            "Batch 2800 of 31915\n",
            "Batch 2900 of 31915\n",
            "Batch 3000 of 31915\n",
            "Batch 3100 of 31915\n",
            "Batch 3200 of 31915\n",
            "Batch 3300 of 31915\n",
            "Batch 3400 of 31915\n",
            "Batch 3500 of 31915\n",
            "Batch 3600 of 31915\n",
            "Batch 3700 of 31915\n",
            "Batch 3800 of 31915\n",
            "Batch 3900 of 31915\n",
            "Batch 4000 of 31915\n",
            "Batch 4100 of 31915\n",
            "Batch 4200 of 31915\n",
            "Batch 4300 of 31915\n",
            "Batch 4400 of 31915\n",
            "Batch 4500 of 31915\n",
            "Batch 4600 of 31915\n",
            "Batch 4700 of 31915\n",
            "Batch 4800 of 31915\n",
            "Batch 4900 of 31915\n",
            "Batch 5000 of 31915\n",
            "Batch 5100 of 31915\n",
            "Batch 5200 of 31915\n",
            "Batch 5300 of 31915\n",
            "Batch 5400 of 31915\n",
            "Batch 5500 of 31915\n",
            "Batch 5600 of 31915\n",
            "Batch 5700 of 31915\n",
            "Batch 5800 of 31915\n",
            "Batch 5900 of 31915\n",
            "Batch 6000 of 31915\n",
            "Batch 6100 of 31915\n",
            "Batch 6200 of 31915\n",
            "Batch 6300 of 31915\n",
            "Batch 6400 of 31915\n",
            "Batch 6500 of 31915\n",
            "Batch 6600 of 31915\n",
            "Batch 6700 of 31915\n",
            "Batch 6800 of 31915\n",
            "Batch 6900 of 31915\n",
            "Batch 7000 of 31915\n",
            "Batch 7100 of 31915\n",
            "Batch 7200 of 31915\n",
            "Batch 7300 of 31915\n",
            "Batch 7400 of 31915\n",
            "Batch 7500 of 31915\n",
            "Batch 7600 of 31915\n",
            "Batch 7700 of 31915\n",
            "Batch 7800 of 31915\n",
            "Batch 7900 of 31915\n",
            "Batch 8000 of 31915\n",
            "Batch 8100 of 31915\n",
            "Batch 8200 of 31915\n",
            "Batch 8300 of 31915\n",
            "Batch 8400 of 31915\n",
            "Batch 8500 of 31915\n",
            "Batch 8600 of 31915\n",
            "Batch 8700 of 31915\n",
            "Batch 8800 of 31915\n",
            "Batch 8900 of 31915\n",
            "Batch 9000 of 31915\n",
            "Batch 9100 of 31915\n",
            "Batch 9200 of 31915\n",
            "Batch 9300 of 31915\n",
            "Batch 9400 of 31915\n",
            "Batch 9500 of 31915\n",
            "Batch 9600 of 31915\n",
            "Batch 9700 of 31915\n",
            "Batch 9800 of 31915\n",
            "Batch 9900 of 31915\n",
            "Batch 10000 of 31915\n",
            "Batch 10100 of 31915\n",
            "Batch 10200 of 31915\n",
            "Batch 10300 of 31915\n",
            "Batch 10400 of 31915\n",
            "Batch 10500 of 31915\n",
            "Batch 10600 of 31915\n",
            "Batch 10700 of 31915\n",
            "Batch 10800 of 31915\n",
            "Batch 10900 of 31915\n",
            "Batch 11000 of 31915\n",
            "Batch 11100 of 31915\n",
            "Batch 11200 of 31915\n",
            "Batch 11300 of 31915\n",
            "Batch 11400 of 31915\n",
            "Batch 11500 of 31915\n",
            "Batch 11600 of 31915\n",
            "Batch 11700 of 31915\n",
            "Batch 11800 of 31915\n",
            "Batch 11900 of 31915\n",
            "Batch 12000 of 31915\n",
            "Batch 12100 of 31915\n",
            "Batch 12200 of 31915\n",
            "Batch 12300 of 31915\n",
            "Batch 12400 of 31915\n",
            "Batch 12500 of 31915\n",
            "Batch 12600 of 31915\n",
            "Batch 12700 of 31915\n",
            "Batch 12800 of 31915\n",
            "Batch 12900 of 31915\n",
            "Batch 13000 of 31915\n",
            "Batch 13100 of 31915\n",
            "Batch 13200 of 31915\n",
            "Batch 13300 of 31915\n",
            "Batch 13400 of 31915\n",
            "Batch 13500 of 31915\n",
            "Batch 13600 of 31915\n",
            "Batch 13700 of 31915\n",
            "Batch 13800 of 31915\n",
            "Batch 13900 of 31915\n",
            "Batch 14000 of 31915\n",
            "Batch 14100 of 31915\n",
            "Batch 14200 of 31915\n",
            "Batch 14300 of 31915\n",
            "Batch 14400 of 31915\n",
            "Batch 14500 of 31915\n",
            "Batch 14600 of 31915\n",
            "Batch 14700 of 31915\n",
            "Batch 14800 of 31915\n",
            "Batch 14900 of 31915\n",
            "Batch 15000 of 31915\n",
            "Batch 15100 of 31915\n",
            "Batch 15200 of 31915\n",
            "Batch 15300 of 31915\n",
            "Batch 15400 of 31915\n",
            "Batch 15500 of 31915\n",
            "Batch 15600 of 31915\n",
            "Batch 15700 of 31915\n",
            "Batch 15800 of 31915\n",
            "Batch 15900 of 31915\n",
            "Batch 16000 of 31915\n",
            "Batch 16100 of 31915\n",
            "Batch 16200 of 31915\n",
            "Batch 16300 of 31915\n",
            "Batch 16400 of 31915\n",
            "Batch 16500 of 31915\n",
            "Batch 16600 of 31915\n",
            "Batch 16700 of 31915\n",
            "Batch 16800 of 31915\n",
            "Batch 16900 of 31915\n",
            "Batch 17000 of 31915\n",
            "Batch 17100 of 31915\n",
            "Batch 17200 of 31915\n",
            "Batch 17300 of 31915\n",
            "Batch 17400 of 31915\n",
            "Batch 17500 of 31915\n",
            "Batch 17600 of 31915\n",
            "Batch 17700 of 31915\n",
            "Batch 17800 of 31915\n",
            "Batch 17900 of 31915\n",
            "Batch 18000 of 31915\n",
            "Batch 18100 of 31915\n",
            "Batch 18200 of 31915\n",
            "Batch 18300 of 31915\n",
            "Batch 18400 of 31915\n",
            "Batch 18500 of 31915\n",
            "Batch 18600 of 31915\n",
            "Batch 18700 of 31915\n",
            "Batch 18800 of 31915\n",
            "Batch 18900 of 31915\n",
            "Batch 19000 of 31915\n",
            "Batch 19100 of 31915\n",
            "Batch 19200 of 31915\n",
            "Batch 19300 of 31915\n",
            "Batch 19400 of 31915\n",
            "Batch 19500 of 31915\n",
            "Batch 19600 of 31915\n",
            "Batch 19700 of 31915\n",
            "Batch 19800 of 31915\n",
            "Batch 19900 of 31915\n",
            "Batch 20000 of 31915\n",
            "Batch 20100 of 31915\n",
            "Batch 20200 of 31915\n",
            "Batch 20300 of 31915\n",
            "Batch 20400 of 31915\n",
            "Batch 20500 of 31915\n",
            "Batch 20600 of 31915\n",
            "Batch 20700 of 31915\n",
            "Batch 20800 of 31915\n",
            "Batch 20900 of 31915\n",
            "Batch 21000 of 31915\n",
            "Batch 21100 of 31915\n",
            "Batch 21200 of 31915\n",
            "Batch 21300 of 31915\n",
            "Batch 21400 of 31915\n",
            "Batch 21500 of 31915\n",
            "Batch 21600 of 31915\n",
            "Batch 21700 of 31915\n",
            "Batch 21800 of 31915\n",
            "Batch 21900 of 31915\n",
            "Batch 22000 of 31915\n",
            "Batch 22100 of 31915\n",
            "Batch 22200 of 31915\n",
            "Batch 22300 of 31915\n",
            "Batch 22400 of 31915\n",
            "Batch 22500 of 31915\n",
            "Batch 22600 of 31915\n",
            "Batch 22700 of 31915\n",
            "Batch 22800 of 31915\n",
            "Batch 22900 of 31915\n",
            "Batch 23000 of 31915\n",
            "Batch 23100 of 31915\n",
            "Batch 23200 of 31915\n",
            "Batch 23300 of 31915\n",
            "Batch 23400 of 31915\n",
            "Batch 23500 of 31915\n",
            "Batch 23600 of 31915\n",
            "Batch 23700 of 31915\n",
            "Batch 23800 of 31915\n",
            "Batch 23900 of 31915\n",
            "Batch 24000 of 31915\n",
            "Batch 24100 of 31915\n",
            "Batch 24200 of 31915\n",
            "Batch 24300 of 31915\n",
            "Batch 24400 of 31915\n",
            "Batch 24500 of 31915\n",
            "Batch 24600 of 31915\n",
            "Batch 24700 of 31915\n",
            "Batch 24800 of 31915\n",
            "Batch 24900 of 31915\n",
            "Batch 25000 of 31915\n",
            "Batch 25100 of 31915\n",
            "Batch 25200 of 31915\n",
            "Batch 25300 of 31915\n",
            "Batch 25400 of 31915\n",
            "Batch 25500 of 31915\n",
            "Batch 25600 of 31915\n",
            "Batch 25700 of 31915\n",
            "Batch 25800 of 31915\n",
            "Batch 25900 of 31915\n",
            "Batch 26000 of 31915\n",
            "Batch 26100 of 31915\n",
            "Batch 26200 of 31915\n",
            "Batch 26300 of 31915\n",
            "Batch 26400 of 31915\n",
            "Batch 26500 of 31915\n",
            "Batch 26600 of 31915\n",
            "Batch 26700 of 31915\n",
            "Batch 26800 of 31915\n",
            "Batch 26900 of 31915\n",
            "Batch 27000 of 31915\n",
            "Batch 27100 of 31915\n",
            "Batch 27200 of 31915\n",
            "Batch 27300 of 31915\n",
            "Batch 27400 of 31915\n",
            "Batch 27500 of 31915\n",
            "Batch 27600 of 31915\n",
            "Batch 27700 of 31915\n",
            "Batch 27800 of 31915\n",
            "Batch 27900 of 31915\n",
            "Batch 28000 of 31915\n",
            "Batch 28100 of 31915\n",
            "Batch 28200 of 31915\n",
            "Batch 28300 of 31915\n",
            "Batch 28400 of 31915\n",
            "Batch 28500 of 31915\n",
            "Batch 28600 of 31915\n",
            "Batch 28700 of 31915\n",
            "Batch 28800 of 31915\n",
            "Batch 28900 of 31915\n",
            "Batch 29000 of 31915\n",
            "Batch 29100 of 31915\n",
            "Batch 29200 of 31915\n",
            "Batch 29300 of 31915\n",
            "Batch 29400 of 31915\n",
            "Batch 29500 of 31915\n",
            "Batch 29600 of 31915\n",
            "Batch 29700 of 31915\n",
            "Batch 29800 of 31915\n",
            "Batch 29900 of 31915\n",
            "Batch 30000 of 31915\n",
            "Batch 30100 of 31915\n",
            "Batch 30200 of 31915\n",
            "Batch 30300 of 31915\n",
            "Batch 30400 of 31915\n",
            "Batch 30500 of 31915\n",
            "Batch 30600 of 31915\n",
            "Batch 30700 of 31915\n",
            "Batch 30800 of 31915\n",
            "Batch 30900 of 31915\n",
            "Batch 31000 of 31915\n",
            "Batch 31100 of 31915\n",
            "Batch 31200 of 31915\n",
            "Batch 31300 of 31915\n",
            "Batch 31400 of 31915\n",
            "Batch 31500 of 31915\n",
            "Batch 31600 of 31915\n",
            "Batch 31700 of 31915\n",
            "Batch 31800 of 31915\n",
            "Batch 31900 of 31915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions to Google Drive\n",
        "df_pred.to_csv(\"/content/gdrive/MyDrive/T5/assignment_test_predictions_no_pretrain.csv\", index=None)"
      ],
      "metadata": {
        "id": "dLZY4SJ4hyn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute accuracy metrics on each toxicity class\n",
        "\n",
        "# Categories that we will report metrics for\n",
        "categories = [\n",
        "    \"toxic\",\n",
        "    \"severe_toxic\",\n",
        "    \"obscene\",\n",
        "    \"threat\",\n",
        "    \"insult\",\n",
        "    \"identity_hate\"\n",
        "]\n",
        "\n",
        "# Loop over categories and print metrics\n",
        "for c in categories:\n",
        "  metrics = precision_recall_fscore_support(\n",
        "      df_pred[f\"label_{c}\"],\n",
        "      df_pred[f\"pred_{c}\"]\n",
        "  )\n",
        "  print(c)\n",
        "  print(f\"Precision: {metrics[0][1].round(3)}\")\n",
        "  print(f\"Recall: {metrics[1][1].round(3)}\")\n",
        "  print(f\"F1-score: {metrics[2][1].round(3)}\")\n",
        "  print(f\"Support: {metrics[3][1]}\")\n",
        "  label_category = f\"label_{c}\"\n",
        "  print(f\"Rate: {df_pred[label_category].mean().round(3)}\")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZNBc2iWyTSO",
        "outputId": "c0dcb8a2-6b64-4dd6-eb35-da19d0ac6177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "toxic\n",
            "Precision: 0.693\n",
            "Recall: 0.807\n",
            "F1-score: 0.746\n",
            "Support: 3079\n",
            "Rate: 0.096\n",
            "\n",
            "severe_toxic\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-score: 0.0\n",
            "Support: 343\n",
            "Rate: 0.011\n",
            "\n",
            "obscene\n",
            "Precision: 0.805\n",
            "Recall: 0.664\n",
            "F1-score: 0.727\n",
            "Support: 1701\n",
            "Rate: 0.053\n",
            "\n",
            "threat\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-score: 0.0\n",
            "Support: 95\n",
            "Rate: 0.003\n",
            "\n",
            "insult\n",
            "Precision: 0.688\n",
            "Recall: 0.62\n",
            "F1-score: 0.652\n",
            "Support: 1557\n",
            "Rate: 0.049\n",
            "\n",
            "identity_hate\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-score: 0.0\n",
            "Support: 249\n",
            "Rate: 0.008\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 scores on this model are surprisingly not bad given the lack of pretraining. A side-by-side comparison between this and the previous model is in the table below:\n",
        "\n",
        "| Class | F1 with pretraining | F1 with no pretraining |\n",
        "| :--- | :--- | :--- |\n",
        "| toxic | 0.819 | 0.746 |\n",
        "| severe_toxic | 0.084 | 0.0 |\n",
        "| obscene | 0.638 | 0.727 |\n",
        "| threat | 0.298 | 0.0 |\n",
        "| insult | 0.579 | 0.652 |\n",
        "| identity_hate | 0.332 | 0.0 |\n",
        "\n",
        "The most common class \"toxic\" has a lower F1-score, but F1-scores are actually higher on the next two most common classes. The three remaining classes have an F1-score of zero because the model never predicts these classes. This is likely an area where pretraining helped, since the first model was able to learn how to attempt to predict these classes after exposure to few instances in the training set.\n",
        "\n",
        "We can also see that training was far less efficient, in the sense that training losses decreased much more slowly when training a model from scratch versus fine-tuning on top of a pretrained model. For example, when fine-tuning the pretrained model, mean training loss was less than 0.1 after 100 iterations and was less than 0.06 after 200 iterations. When training from scratch, a loss less than 0.1 was not observed until after 19,200 iterations and a loss less than 0.06 was not observed until after 45,500 iterations."
      ],
      "metadata": {
        "id": "a9xX1wW_YTvR"
      }
    }
  ]
}